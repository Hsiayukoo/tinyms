{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affecting-convenience",
   "metadata": {},
   "source": [
    "# TinyMS LeNet5 Tutorial\n",
    "\n",
    "In this tutorial, constructing a LeNet5 model, downloading dataset, training, starting the server and making predictions of the model using TinyMS API will be demonstrated. \n",
    "\n",
    "## Prerequisite\n",
    " - Ubuntu: `18.04`\n",
    " - Python: `3.7.x`\n",
    " - Flask: `1.1.2`\n",
    " - MindSpore: `CPU-1.1.1`\n",
    " - TinyMS: `0.1.0`\n",
    " - numpy: `1.17.5`\n",
    " - opencv-python: `4.5.1.48`\n",
    " - Pillow: `8.1.0`\n",
    " - pip: `21.0.1`\n",
    " - requests: `2.18.4`\n",
    " \n",
    "## Introduction\n",
    "\n",
    "TinyMS is a high-level API which is designed for amateur of deep learning. It minimizes the number of actions of users required to construct, train, evaluate and serve a model. TinyMS also provides tutorials and documentations for developers. \n",
    "\n",
    "This tutorial consists of six parts, `constructing the model`, `downloading dataset`, `training`, `define servable json`, `starting server` and `making predictions` in which the server will be run in a sub process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11711:140661951510336,MainProcess):2021-03-17-11:47:37.600.288 [mindspore/ops/operations/array_ops.py:2302] WARN_DEPRECATED: The usage of Pack is deprecated. Please use Stack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 'ControlDepend' is deprecated from version 1.1 and will be removed in a future version, use 'Depend' instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tinyms as ts\n",
    "import tinyms.optimizers as opt\n",
    "\n",
    "from PIL import Image\n",
    "from tinyms import context\n",
    "from tinyms.data import MnistDataset, download_dataset\n",
    "from tinyms.vision import mnist_transform\n",
    "from tinyms.model import Model, lenet5\n",
    "from tinyms.serving import start_server, predict, list_servables, shutdown, server_started\n",
    "from tinyms.metrics import Accuracy\n",
    "from tinyms.losses import SoftmaxCrossEntropyWithLogits\n",
    "from tinyms.callbacks import ModelCheckpoint, CheckpointConfig, LossMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-medline",
   "metadata": {},
   "source": [
    "### 1. Construct the model\n",
    "\n",
    "TinyMS encapsulates init and construct of the LeNet5 model, the line of the code is reduced to construct the LeNet5 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "matched-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network\n",
    "net = lenet5(class_num=10)\n",
    "model = Model(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-activation",
   "metadata": {},
   "source": [
    "### 2. Download dataset\n",
    "\n",
    "The MNIST dataset will be downloaded if `mnist` folder didn't exist at the root. If `mnist` folder already exists, this step will not be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "laden-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** Downloading the MNIST dataset **************\n",
      "[████████████████████████████████████████████████████████████████████████████████████████████████████] 100.00%\n",
      "============== /root/mnist/train/train-images-idx3-ubyte.gz is already ==============\n",
      "[████████████████████████████████████████████████████████████████████████████████████████████████████] 100.00%\n",
      "============== /root/mnist/train/train-labels-idx1-ubyte.gz is already ==============\n",
      "[████████████████████████████████████████████████████████████████████████████████████████████████████] 100.00%\n",
      "============== /root/mnist/test/t10k-images-idx3-ubyte.gz is already ==============\n",
      "[████████████████████████████████████████████████████████████████████████████████████████████████████] 100.00%\n",
      "============== /root/mnist/test/t10k-labels-idx1-ubyte.gz is already ==============\n",
      "************Download complete*************\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "mnist_path = '/root/mnist'\n",
    "if not os.path.exists(mnist_path):\n",
    "    ts.data.download_dataset('mnist', '/root')\n",
    "    print('************Download complete*************')\n",
    "else:\n",
    "    print('************Dataset already exists.**************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-bradford",
   "metadata": {},
   "source": [
    "### 3. Train the model & evaluation\n",
    "\n",
    "The dataset for both training and evaluation will be defined here, and the parameters for training also set in this block. A trained ckpt file will be saved to `/etc/tinyms/serving/lenet5` folder for later use, meanwhile the evaluation will be performed and the `Accuracy` can be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bearing-showcase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenet5 ckpt folder already exists\n",
      "************************Start training*************************\n",
      "epoch: 1 step: 1, loss is 2.3025925\n",
      "epoch: 1 step: 2, loss is 2.302628\n",
      "epoch: 1 step: 3, loss is 2.302569\n",
      "epoch: 1 step: 4, loss is 2.3028355\n",
      "epoch: 1 step: 5, loss is 2.3022976\n",
      "epoch: 1 step: 6, loss is 2.3023846\n",
      "epoch: 1 step: 7, loss is 2.30307\n",
      "epoch: 1 step: 8, loss is 2.3017108\n",
      "epoch: 1 step: 9, loss is 2.3026295\n",
      "epoch: 1 step: 10, loss is 2.3037372\n",
      "epoch: 1 step: 11, loss is 2.3028166\n",
      "epoch: 1 step: 12, loss is 2.3027778\n",
      "epoch: 1 step: 13, loss is 2.3010583\n",
      "epoch: 1 step: 14, loss is 2.303954\n",
      "epoch: 1 step: 15, loss is 2.3019102\n",
      "epoch: 1 step: 16, loss is 2.301164\n",
      "epoch: 1 step: 17, loss is 2.3062768\n",
      "epoch: 1 step: 18, loss is 2.301557\n",
      "epoch: 1 step: 19, loss is 2.3004386\n",
      "epoch: 1 step: 20, loss is 2.300065\n",
      "epoch: 1 step: 21, loss is 2.301045\n",
      "epoch: 1 step: 22, loss is 2.3027081\n",
      "epoch: 1 step: 23, loss is 2.3062322\n",
      "epoch: 1 step: 24, loss is 2.3001027\n",
      "epoch: 1 step: 25, loss is 2.3017058\n",
      "epoch: 1 step: 26, loss is 2.3018801\n",
      "epoch: 1 step: 27, loss is 2.30416\n",
      "epoch: 1 step: 28, loss is 2.2999156\n",
      "epoch: 1 step: 29, loss is 2.3013325\n",
      "epoch: 1 step: 30, loss is 2.2989993\n",
      "epoch: 1 step: 31, loss is 2.3041267\n",
      "epoch: 1 step: 32, loss is 2.3010557\n",
      "epoch: 1 step: 33, loss is 2.2962193\n",
      "epoch: 1 step: 34, loss is 2.312958\n",
      "epoch: 1 step: 35, loss is 2.3111095\n",
      "epoch: 1 step: 36, loss is 2.3020442\n",
      "epoch: 1 step: 37, loss is 2.2943702\n",
      "epoch: 1 step: 38, loss is 2.3041015\n",
      "epoch: 1 step: 39, loss is 2.3075478\n",
      "epoch: 1 step: 40, loss is 2.2905786\n",
      "epoch: 1 step: 41, loss is 2.300455\n",
      "epoch: 1 step: 42, loss is 2.2950828\n",
      "epoch: 1 step: 43, loss is 2.2993307\n",
      "epoch: 1 step: 44, loss is 2.294104\n",
      "epoch: 1 step: 45, loss is 2.2881217\n",
      "epoch: 1 step: 46, loss is 2.3087137\n",
      "epoch: 1 step: 47, loss is 2.2950304\n",
      "epoch: 1 step: 48, loss is 2.2978497\n",
      "epoch: 1 step: 49, loss is 2.2991943\n",
      "epoch: 1 step: 50, loss is 2.2881558\n",
      "epoch: 1 step: 51, loss is 2.302539\n",
      "epoch: 1 step: 52, loss is 2.282765\n",
      "epoch: 1 step: 53, loss is 2.2945118\n",
      "epoch: 1 step: 54, loss is 2.295129\n",
      "epoch: 1 step: 55, loss is 2.3078282\n",
      "epoch: 1 step: 56, loss is 2.2985866\n",
      "epoch: 1 step: 57, loss is 2.3148928\n",
      "epoch: 1 step: 58, loss is 2.2863145\n",
      "epoch: 1 step: 59, loss is 2.2910194\n",
      "epoch: 1 step: 60, loss is 2.3003948\n",
      "epoch: 1 step: 61, loss is 2.3105457\n",
      "epoch: 1 step: 62, loss is 2.2986438\n",
      "epoch: 1 step: 63, loss is 2.2936795\n",
      "epoch: 1 step: 64, loss is 2.2852337\n",
      "epoch: 1 step: 65, loss is 2.304579\n",
      "epoch: 1 step: 66, loss is 2.3264723\n",
      "epoch: 1 step: 67, loss is 2.2813342\n",
      "epoch: 1 step: 68, loss is 2.300323\n",
      "epoch: 1 step: 69, loss is 2.302331\n",
      "epoch: 1 step: 70, loss is 2.2894514\n",
      "epoch: 1 step: 71, loss is 2.3198693\n",
      "epoch: 1 step: 72, loss is 2.2760782\n",
      "epoch: 1 step: 73, loss is 2.2934256\n",
      "epoch: 1 step: 74, loss is 2.29375\n",
      "epoch: 1 step: 75, loss is 2.3068194\n",
      "epoch: 1 step: 76, loss is 2.318593\n",
      "epoch: 1 step: 77, loss is 2.281145\n",
      "epoch: 1 step: 78, loss is 2.3031085\n",
      "epoch: 1 step: 79, loss is 2.322241\n",
      "epoch: 1 step: 80, loss is 2.3089755\n",
      "epoch: 1 step: 81, loss is 2.3007019\n",
      "epoch: 1 step: 82, loss is 2.2862449\n",
      "epoch: 1 step: 83, loss is 2.301054\n",
      "epoch: 1 step: 84, loss is 2.317668\n",
      "epoch: 1 step: 85, loss is 2.2769003\n",
      "epoch: 1 step: 86, loss is 2.308834\n",
      "epoch: 1 step: 87, loss is 2.329025\n",
      "epoch: 1 step: 88, loss is 2.311669\n",
      "epoch: 1 step: 89, loss is 2.3053658\n",
      "epoch: 1 step: 90, loss is 2.2743483\n",
      "epoch: 1 step: 91, loss is 2.2775712\n",
      "epoch: 1 step: 92, loss is 2.312112\n",
      "epoch: 1 step: 93, loss is 2.3056035\n",
      "epoch: 1 step: 94, loss is 2.3108902\n",
      "epoch: 1 step: 95, loss is 2.311024\n",
      "epoch: 1 step: 96, loss is 2.302003\n",
      "epoch: 1 step: 97, loss is 2.293661\n",
      "epoch: 1 step: 98, loss is 2.3139658\n",
      "epoch: 1 step: 99, loss is 2.2696743\n",
      "epoch: 1 step: 100, loss is 2.26388\n",
      "epoch: 1 step: 101, loss is 2.3175728\n",
      "epoch: 1 step: 102, loss is 2.3187582\n",
      "epoch: 1 step: 103, loss is 2.2946131\n",
      "epoch: 1 step: 104, loss is 2.2763288\n",
      "epoch: 1 step: 105, loss is 2.283209\n",
      "epoch: 1 step: 106, loss is 2.3230772\n",
      "epoch: 1 step: 107, loss is 2.3291934\n",
      "epoch: 1 step: 108, loss is 2.3041773\n",
      "epoch: 1 step: 109, loss is 2.2889385\n",
      "epoch: 1 step: 110, loss is 2.3204575\n",
      "epoch: 1 step: 111, loss is 2.2860816\n",
      "epoch: 1 step: 112, loss is 2.2963889\n",
      "epoch: 1 step: 113, loss is 2.3220527\n",
      "epoch: 1 step: 114, loss is 2.3097045\n",
      "epoch: 1 step: 115, loss is 2.2815824\n",
      "epoch: 1 step: 116, loss is 2.3152204\n",
      "epoch: 1 step: 117, loss is 2.3156934\n",
      "epoch: 1 step: 118, loss is 2.3019452\n",
      "epoch: 1 step: 119, loss is 2.275647\n",
      "epoch: 1 step: 120, loss is 2.307088\n",
      "epoch: 1 step: 121, loss is 2.286316\n",
      "epoch: 1 step: 122, loss is 2.2995481\n",
      "epoch: 1 step: 123, loss is 2.2723403\n",
      "epoch: 1 step: 124, loss is 2.315249\n",
      "epoch: 1 step: 125, loss is 2.304728\n",
      "epoch: 1 step: 126, loss is 2.30101\n",
      "epoch: 1 step: 127, loss is 2.3072064\n",
      "epoch: 1 step: 128, loss is 2.3239374\n",
      "epoch: 1 step: 129, loss is 2.3088558\n",
      "epoch: 1 step: 130, loss is 2.317924\n",
      "epoch: 1 step: 131, loss is 2.311959\n",
      "epoch: 1 step: 132, loss is 2.277222\n",
      "epoch: 1 step: 133, loss is 2.3209481\n",
      "epoch: 1 step: 134, loss is 2.3198469\n",
      "epoch: 1 step: 135, loss is 2.3047874\n",
      "epoch: 1 step: 136, loss is 2.288644\n",
      "epoch: 1 step: 137, loss is 2.289152\n",
      "epoch: 1 step: 138, loss is 2.3212476\n",
      "epoch: 1 step: 139, loss is 2.3037813\n",
      "epoch: 1 step: 140, loss is 2.313692\n",
      "epoch: 1 step: 141, loss is 2.2858038\n",
      "epoch: 1 step: 142, loss is 2.2887576\n",
      "epoch: 1 step: 143, loss is 2.2948864\n",
      "epoch: 1 step: 144, loss is 2.298587\n",
      "epoch: 1 step: 145, loss is 2.3143356\n",
      "epoch: 1 step: 146, loss is 2.3220842\n",
      "epoch: 1 step: 147, loss is 2.3026924\n",
      "epoch: 1 step: 148, loss is 2.2909462\n",
      "epoch: 1 step: 149, loss is 2.3069599\n",
      "epoch: 1 step: 150, loss is 2.2986279\n",
      "epoch: 1 step: 151, loss is 2.321187\n",
      "epoch: 1 step: 152, loss is 2.2985294\n",
      "epoch: 1 step: 153, loss is 2.3084629\n",
      "epoch: 1 step: 154, loss is 2.3027365\n",
      "epoch: 1 step: 155, loss is 2.2988563\n",
      "epoch: 1 step: 156, loss is 2.3112605\n",
      "epoch: 1 step: 157, loss is 2.277202\n",
      "epoch: 1 step: 158, loss is 2.3198113\n",
      "epoch: 1 step: 159, loss is 2.3000815\n",
      "epoch: 1 step: 160, loss is 2.312337\n",
      "epoch: 1 step: 161, loss is 2.3025608\n",
      "epoch: 1 step: 162, loss is 2.2727795\n",
      "epoch: 1 step: 163, loss is 2.3022833\n",
      "epoch: 1 step: 164, loss is 2.2926078\n",
      "epoch: 1 step: 165, loss is 2.2967944\n",
      "epoch: 1 step: 166, loss is 2.30587\n",
      "epoch: 1 step: 167, loss is 2.3023052\n",
      "epoch: 1 step: 168, loss is 2.2757912\n",
      "epoch: 1 step: 169, loss is 2.3016891\n",
      "epoch: 1 step: 170, loss is 2.3392026\n",
      "epoch: 1 step: 171, loss is 2.298823\n",
      "epoch: 1 step: 172, loss is 2.3091266\n",
      "epoch: 1 step: 173, loss is 2.3116248\n",
      "epoch: 1 step: 174, loss is 2.3106143\n",
      "epoch: 1 step: 175, loss is 2.3058963\n",
      "epoch: 1 step: 176, loss is 2.292945\n",
      "epoch: 1 step: 177, loss is 2.2994983\n",
      "epoch: 1 step: 178, loss is 2.313004\n",
      "epoch: 1 step: 179, loss is 2.3073416\n",
      "epoch: 1 step: 180, loss is 2.3082716\n",
      "epoch: 1 step: 181, loss is 2.289501\n",
      "epoch: 1 step: 182, loss is 2.3273985\n",
      "epoch: 1 step: 183, loss is 2.2646265\n",
      "epoch: 1 step: 184, loss is 2.2884614\n",
      "epoch: 1 step: 185, loss is 2.3099933\n",
      "epoch: 1 step: 186, loss is 2.3096051\n",
      "epoch: 1 step: 187, loss is 2.3091443\n",
      "epoch: 1 step: 188, loss is 2.3022375\n",
      "epoch: 1 step: 189, loss is 2.29481\n",
      "epoch: 1 step: 190, loss is 2.3030565\n",
      "epoch: 1 step: 191, loss is 2.3034828\n",
      "epoch: 1 step: 192, loss is 2.3087583\n",
      "epoch: 1 step: 193, loss is 2.3057241\n",
      "epoch: 1 step: 194, loss is 2.3273704\n",
      "epoch: 1 step: 195, loss is 2.2924612\n",
      "epoch: 1 step: 196, loss is 2.3109927\n",
      "epoch: 1 step: 197, loss is 2.288087\n",
      "epoch: 1 step: 198, loss is 2.2834065\n",
      "epoch: 1 step: 199, loss is 2.3135045\n",
      "epoch: 1 step: 200, loss is 2.2949724\n",
      "epoch: 1 step: 201, loss is 2.3012755\n",
      "epoch: 1 step: 202, loss is 2.308724\n",
      "epoch: 1 step: 203, loss is 2.291255\n",
      "epoch: 1 step: 204, loss is 2.3035784\n",
      "epoch: 1 step: 205, loss is 2.303659\n",
      "epoch: 1 step: 206, loss is 2.302518\n",
      "epoch: 1 step: 207, loss is 2.2889411\n",
      "epoch: 1 step: 208, loss is 2.3062203\n",
      "epoch: 1 step: 209, loss is 2.310485\n",
      "epoch: 1 step: 210, loss is 2.2869928\n",
      "epoch: 1 step: 211, loss is 2.3068702\n",
      "epoch: 1 step: 212, loss is 2.3264148\n",
      "epoch: 1 step: 213, loss is 2.2970998\n",
      "epoch: 1 step: 214, loss is 2.2984319\n",
      "epoch: 1 step: 215, loss is 2.3168652\n",
      "epoch: 1 step: 216, loss is 2.3166277\n",
      "epoch: 1 step: 217, loss is 2.2893674\n",
      "epoch: 1 step: 218, loss is 2.2898598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 219, loss is 2.28851\n",
      "epoch: 1 step: 220, loss is 2.3005345\n",
      "epoch: 1 step: 221, loss is 2.2895362\n",
      "epoch: 1 step: 222, loss is 2.304596\n",
      "epoch: 1 step: 223, loss is 2.3010707\n",
      "epoch: 1 step: 224, loss is 2.313743\n",
      "epoch: 1 step: 225, loss is 2.2963011\n",
      "epoch: 1 step: 226, loss is 2.3023293\n",
      "epoch: 1 step: 227, loss is 2.3180149\n",
      "epoch: 1 step: 228, loss is 2.291154\n",
      "epoch: 1 step: 229, loss is 2.299264\n",
      "epoch: 1 step: 230, loss is 2.2969563\n",
      "epoch: 1 step: 231, loss is 2.3065684\n",
      "epoch: 1 step: 232, loss is 2.2866714\n",
      "epoch: 1 step: 233, loss is 2.301034\n",
      "epoch: 1 step: 234, loss is 2.2969818\n",
      "epoch: 1 step: 235, loss is 2.3231783\n",
      "epoch: 1 step: 236, loss is 2.2968955\n",
      "epoch: 1 step: 237, loss is 2.3250144\n",
      "epoch: 1 step: 238, loss is 2.2995288\n",
      "epoch: 1 step: 239, loss is 2.2931275\n",
      "epoch: 1 step: 240, loss is 2.2980835\n",
      "epoch: 1 step: 241, loss is 2.3036187\n",
      "epoch: 1 step: 242, loss is 2.312397\n",
      "epoch: 1 step: 243, loss is 2.2856166\n",
      "epoch: 1 step: 244, loss is 2.2674377\n",
      "epoch: 1 step: 245, loss is 2.2846406\n",
      "epoch: 1 step: 246, loss is 2.3104434\n",
      "epoch: 1 step: 247, loss is 2.305111\n",
      "epoch: 1 step: 248, loss is 2.2977893\n",
      "epoch: 1 step: 249, loss is 2.318994\n",
      "epoch: 1 step: 250, loss is 2.2814841\n",
      "epoch: 1 step: 251, loss is 2.3040285\n",
      "epoch: 1 step: 252, loss is 2.2913392\n",
      "epoch: 1 step: 253, loss is 2.306001\n",
      "epoch: 1 step: 254, loss is 2.2893713\n",
      "epoch: 1 step: 255, loss is 2.3133907\n",
      "epoch: 1 step: 256, loss is 2.3119595\n",
      "epoch: 1 step: 257, loss is 2.320523\n",
      "epoch: 1 step: 258, loss is 2.2984118\n",
      "epoch: 1 step: 259, loss is 2.2962968\n",
      "epoch: 1 step: 260, loss is 2.2883978\n",
      "epoch: 1 step: 261, loss is 2.3005557\n",
      "epoch: 1 step: 262, loss is 2.3042254\n",
      "epoch: 1 step: 263, loss is 2.2954543\n",
      "epoch: 1 step: 264, loss is 2.2968366\n",
      "epoch: 1 step: 265, loss is 2.3118799\n",
      "epoch: 1 step: 266, loss is 2.325193\n",
      "epoch: 1 step: 267, loss is 2.2912347\n",
      "epoch: 1 step: 268, loss is 2.3085601\n",
      "epoch: 1 step: 269, loss is 2.306054\n",
      "epoch: 1 step: 270, loss is 2.299895\n",
      "epoch: 1 step: 271, loss is 2.3065422\n",
      "epoch: 1 step: 272, loss is 2.287075\n",
      "epoch: 1 step: 273, loss is 2.3017416\n",
      "epoch: 1 step: 274, loss is 2.2942984\n",
      "epoch: 1 step: 275, loss is 2.2880964\n",
      "epoch: 1 step: 276, loss is 2.3252068\n",
      "epoch: 1 step: 277, loss is 2.3041105\n",
      "epoch: 1 step: 278, loss is 2.2816837\n",
      "epoch: 1 step: 279, loss is 2.3057425\n",
      "epoch: 1 step: 280, loss is 2.3013558\n",
      "epoch: 1 step: 281, loss is 2.2938855\n",
      "epoch: 1 step: 282, loss is 2.3028653\n",
      "epoch: 1 step: 283, loss is 2.2958624\n",
      "epoch: 1 step: 284, loss is 2.3201418\n",
      "epoch: 1 step: 285, loss is 2.2728653\n",
      "epoch: 1 step: 286, loss is 2.3125772\n",
      "epoch: 1 step: 287, loss is 2.302091\n",
      "epoch: 1 step: 288, loss is 2.2903755\n",
      "epoch: 1 step: 289, loss is 2.3037212\n",
      "epoch: 1 step: 290, loss is 2.2908385\n",
      "epoch: 1 step: 291, loss is 2.2963796\n",
      "epoch: 1 step: 292, loss is 2.2909303\n",
      "epoch: 1 step: 293, loss is 2.2899792\n",
      "epoch: 1 step: 294, loss is 2.3039653\n",
      "epoch: 1 step: 295, loss is 2.3073566\n",
      "epoch: 1 step: 296, loss is 2.3193161\n",
      "epoch: 1 step: 297, loss is 2.3116543\n",
      "epoch: 1 step: 298, loss is 2.2844677\n",
      "epoch: 1 step: 299, loss is 2.295266\n",
      "epoch: 1 step: 300, loss is 2.3177302\n",
      "epoch: 1 step: 301, loss is 2.292933\n",
      "epoch: 1 step: 302, loss is 2.2978213\n",
      "epoch: 1 step: 303, loss is 2.3027146\n",
      "epoch: 1 step: 304, loss is 2.28616\n",
      "epoch: 1 step: 305, loss is 2.308208\n",
      "epoch: 1 step: 306, loss is 2.298022\n",
      "epoch: 1 step: 307, loss is 2.279496\n",
      "epoch: 1 step: 308, loss is 2.2685542\n",
      "epoch: 1 step: 309, loss is 2.3260202\n",
      "epoch: 1 step: 310, loss is 2.3181562\n",
      "epoch: 1 step: 311, loss is 2.30706\n",
      "epoch: 1 step: 312, loss is 2.3188431\n",
      "epoch: 1 step: 313, loss is 2.3088243\n",
      "epoch: 1 step: 314, loss is 2.2959557\n",
      "epoch: 1 step: 315, loss is 2.3189814\n",
      "epoch: 1 step: 316, loss is 2.2852616\n",
      "epoch: 1 step: 317, loss is 2.302534\n",
      "epoch: 1 step: 318, loss is 2.2390807\n",
      "epoch: 1 step: 319, loss is 2.3095276\n",
      "epoch: 1 step: 320, loss is 2.300695\n",
      "epoch: 1 step: 321, loss is 2.3237827\n",
      "epoch: 1 step: 322, loss is 2.3241918\n",
      "epoch: 1 step: 323, loss is 2.3297062\n",
      "epoch: 1 step: 324, loss is 2.2775295\n",
      "epoch: 1 step: 325, loss is 2.2752051\n",
      "epoch: 1 step: 326, loss is 2.2962155\n",
      "epoch: 1 step: 327, loss is 2.2974877\n",
      "epoch: 1 step: 328, loss is 2.2778158\n",
      "epoch: 1 step: 329, loss is 2.293205\n",
      "epoch: 1 step: 330, loss is 2.3205838\n",
      "epoch: 1 step: 331, loss is 2.2921536\n",
      "epoch: 1 step: 332, loss is 2.3052616\n",
      "epoch: 1 step: 333, loss is 2.300161\n",
      "epoch: 1 step: 334, loss is 2.289725\n",
      "epoch: 1 step: 335, loss is 2.2922206\n",
      "epoch: 1 step: 336, loss is 2.2992678\n",
      "epoch: 1 step: 337, loss is 2.3061635\n",
      "epoch: 1 step: 338, loss is 2.2975662\n",
      "epoch: 1 step: 339, loss is 2.3177361\n",
      "epoch: 1 step: 340, loss is 2.322895\n",
      "epoch: 1 step: 341, loss is 2.2994354\n",
      "epoch: 1 step: 342, loss is 2.30864\n",
      "epoch: 1 step: 343, loss is 2.3160346\n",
      "epoch: 1 step: 344, loss is 2.3300114\n",
      "epoch: 1 step: 345, loss is 2.293943\n",
      "epoch: 1 step: 346, loss is 2.3184724\n",
      "epoch: 1 step: 347, loss is 2.3033657\n",
      "epoch: 1 step: 348, loss is 2.3014266\n",
      "epoch: 1 step: 349, loss is 2.2990208\n",
      "epoch: 1 step: 350, loss is 2.297546\n",
      "epoch: 1 step: 351, loss is 2.310288\n",
      "epoch: 1 step: 352, loss is 2.2943048\n",
      "epoch: 1 step: 353, loss is 2.316012\n",
      "epoch: 1 step: 354, loss is 2.316694\n",
      "epoch: 1 step: 355, loss is 2.2928731\n",
      "epoch: 1 step: 356, loss is 2.3210626\n",
      "epoch: 1 step: 357, loss is 2.3145013\n",
      "epoch: 1 step: 358, loss is 2.288215\n",
      "epoch: 1 step: 359, loss is 2.2961667\n",
      "epoch: 1 step: 360, loss is 2.3056862\n",
      "epoch: 1 step: 361, loss is 2.2834969\n",
      "epoch: 1 step: 362, loss is 2.2855427\n",
      "epoch: 1 step: 363, loss is 2.2773364\n",
      "epoch: 1 step: 364, loss is 2.298187\n",
      "epoch: 1 step: 365, loss is 2.296772\n",
      "epoch: 1 step: 366, loss is 2.316983\n",
      "epoch: 1 step: 367, loss is 2.3087718\n",
      "epoch: 1 step: 368, loss is 2.2710617\n",
      "epoch: 1 step: 369, loss is 2.328654\n",
      "epoch: 1 step: 370, loss is 2.3184226\n",
      "epoch: 1 step: 371, loss is 2.288569\n",
      "epoch: 1 step: 372, loss is 2.3225935\n",
      "epoch: 1 step: 373, loss is 2.3074136\n",
      "epoch: 1 step: 374, loss is 2.299733\n",
      "epoch: 1 step: 375, loss is 2.306505\n",
      "epoch: 1 step: 376, loss is 2.3021972\n",
      "epoch: 1 step: 377, loss is 2.2915196\n",
      "epoch: 1 step: 378, loss is 2.3093977\n",
      "epoch: 1 step: 379, loss is 2.3017168\n",
      "epoch: 1 step: 380, loss is 2.3166397\n",
      "epoch: 1 step: 381, loss is 2.2976289\n",
      "epoch: 1 step: 382, loss is 2.2931619\n",
      "epoch: 1 step: 383, loss is 2.2960942\n",
      "epoch: 1 step: 384, loss is 2.2948577\n",
      "epoch: 1 step: 385, loss is 2.303109\n",
      "epoch: 1 step: 386, loss is 2.3149922\n",
      "epoch: 1 step: 387, loss is 2.2992282\n",
      "epoch: 1 step: 388, loss is 2.2923286\n",
      "epoch: 1 step: 389, loss is 2.2770908\n",
      "epoch: 1 step: 390, loss is 2.2894073\n",
      "epoch: 1 step: 391, loss is 2.319722\n",
      "epoch: 1 step: 392, loss is 2.301103\n",
      "epoch: 1 step: 393, loss is 2.3073385\n",
      "epoch: 1 step: 394, loss is 2.2884126\n",
      "epoch: 1 step: 395, loss is 2.2921648\n",
      "epoch: 1 step: 396, loss is 2.3036191\n",
      "epoch: 1 step: 397, loss is 2.2928207\n",
      "epoch: 1 step: 398, loss is 2.3257978\n",
      "epoch: 1 step: 399, loss is 2.3096745\n",
      "epoch: 1 step: 400, loss is 2.3092782\n",
      "epoch: 1 step: 401, loss is 2.3087008\n",
      "epoch: 1 step: 402, loss is 2.3002722\n",
      "epoch: 1 step: 403, loss is 2.308939\n",
      "epoch: 1 step: 404, loss is 2.3067837\n",
      "epoch: 1 step: 405, loss is 2.2892885\n",
      "epoch: 1 step: 406, loss is 2.31734\n",
      "epoch: 1 step: 407, loss is 2.3001769\n",
      "epoch: 1 step: 408, loss is 2.28884\n",
      "epoch: 1 step: 409, loss is 2.3127444\n",
      "epoch: 1 step: 410, loss is 2.314405\n",
      "epoch: 1 step: 411, loss is 2.2878754\n",
      "epoch: 1 step: 412, loss is 2.3033638\n",
      "epoch: 1 step: 413, loss is 2.2967188\n",
      "epoch: 1 step: 414, loss is 2.2920828\n",
      "epoch: 1 step: 415, loss is 2.2969294\n",
      "epoch: 1 step: 416, loss is 2.329027\n",
      "epoch: 1 step: 417, loss is 2.2914755\n",
      "epoch: 1 step: 418, loss is 2.2883985\n",
      "epoch: 1 step: 419, loss is 2.3014631\n",
      "epoch: 1 step: 420, loss is 2.2817664\n",
      "epoch: 1 step: 421, loss is 2.3004935\n",
      "epoch: 1 step: 422, loss is 2.303535\n",
      "epoch: 1 step: 423, loss is 2.2958736\n",
      "epoch: 1 step: 424, loss is 2.311399\n",
      "epoch: 1 step: 425, loss is 2.3115888\n",
      "epoch: 1 step: 426, loss is 2.3103516\n",
      "epoch: 1 step: 427, loss is 2.3064244\n",
      "epoch: 1 step: 428, loss is 2.3133566\n",
      "epoch: 1 step: 429, loss is 2.286579\n",
      "epoch: 1 step: 430, loss is 2.2812693\n",
      "epoch: 1 step: 431, loss is 2.2916975\n",
      "epoch: 1 step: 432, loss is 2.3013473\n",
      "epoch: 1 step: 433, loss is 2.2927933\n",
      "epoch: 1 step: 434, loss is 2.309328\n",
      "epoch: 1 step: 435, loss is 2.3022668\n",
      "epoch: 1 step: 436, loss is 2.2969809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 437, loss is 2.3145993\n",
      "epoch: 1 step: 438, loss is 2.2950926\n",
      "epoch: 1 step: 439, loss is 2.308606\n",
      "epoch: 1 step: 440, loss is 2.3115516\n",
      "epoch: 1 step: 441, loss is 2.3252168\n",
      "epoch: 1 step: 442, loss is 2.2952497\n",
      "epoch: 1 step: 443, loss is 2.2942939\n",
      "epoch: 1 step: 444, loss is 2.296428\n",
      "epoch: 1 step: 445, loss is 2.301\n",
      "epoch: 1 step: 446, loss is 2.2752087\n",
      "epoch: 1 step: 447, loss is 2.2788482\n",
      "epoch: 1 step: 448, loss is 2.3031545\n",
      "epoch: 1 step: 449, loss is 2.306693\n",
      "epoch: 1 step: 450, loss is 2.2956605\n",
      "epoch: 1 step: 451, loss is 2.3294692\n",
      "epoch: 1 step: 452, loss is 2.3027165\n",
      "epoch: 1 step: 453, loss is 2.3114824\n",
      "epoch: 1 step: 454, loss is 2.2863753\n",
      "epoch: 1 step: 455, loss is 2.2939892\n",
      "epoch: 1 step: 456, loss is 2.3136656\n",
      "epoch: 1 step: 457, loss is 2.332915\n",
      "epoch: 1 step: 458, loss is 2.2972908\n",
      "epoch: 1 step: 459, loss is 2.3041227\n",
      "epoch: 1 step: 460, loss is 2.3156846\n",
      "epoch: 1 step: 461, loss is 2.3226008\n",
      "epoch: 1 step: 462, loss is 2.2813306\n",
      "epoch: 1 step: 463, loss is 2.3030803\n",
      "epoch: 1 step: 464, loss is 2.2757027\n",
      "epoch: 1 step: 465, loss is 2.3156679\n",
      "epoch: 1 step: 466, loss is 2.289082\n",
      "epoch: 1 step: 467, loss is 2.3079813\n",
      "epoch: 1 step: 468, loss is 2.3019853\n",
      "epoch: 1 step: 469, loss is 2.3152215\n",
      "epoch: 1 step: 470, loss is 2.289119\n",
      "epoch: 1 step: 471, loss is 2.2985668\n",
      "epoch: 1 step: 472, loss is 2.3010254\n",
      "epoch: 1 step: 473, loss is 2.3109558\n",
      "epoch: 1 step: 474, loss is 2.3102598\n",
      "epoch: 1 step: 475, loss is 2.3190007\n",
      "epoch: 1 step: 476, loss is 2.2876716\n",
      "epoch: 1 step: 477, loss is 2.292525\n",
      "epoch: 1 step: 478, loss is 2.3020537\n",
      "epoch: 1 step: 479, loss is 2.3144362\n",
      "epoch: 1 step: 480, loss is 2.3038278\n",
      "epoch: 1 step: 481, loss is 2.2765787\n",
      "epoch: 1 step: 482, loss is 2.3050065\n",
      "epoch: 1 step: 483, loss is 2.3194652\n",
      "epoch: 1 step: 484, loss is 2.274889\n",
      "epoch: 1 step: 485, loss is 2.3085074\n",
      "epoch: 1 step: 486, loss is 2.3006065\n",
      "epoch: 1 step: 487, loss is 2.2892199\n",
      "epoch: 1 step: 488, loss is 2.3110707\n",
      "epoch: 1 step: 489, loss is 2.2910566\n",
      "epoch: 1 step: 490, loss is 2.3063712\n",
      "epoch: 1 step: 491, loss is 2.3223968\n",
      "epoch: 1 step: 492, loss is 2.3057003\n",
      "epoch: 1 step: 493, loss is 2.3019588\n",
      "epoch: 1 step: 494, loss is 2.2927125\n",
      "epoch: 1 step: 495, loss is 2.3096526\n",
      "epoch: 1 step: 496, loss is 2.301056\n",
      "epoch: 1 step: 497, loss is 2.2988203\n",
      "epoch: 1 step: 498, loss is 2.2845788\n",
      "epoch: 1 step: 499, loss is 2.3027244\n",
      "epoch: 1 step: 500, loss is 2.2789028\n",
      "epoch: 1 step: 501, loss is 2.2869084\n",
      "epoch: 1 step: 502, loss is 2.2967236\n",
      "epoch: 1 step: 503, loss is 2.2962391\n",
      "epoch: 1 step: 504, loss is 2.3222008\n",
      "epoch: 1 step: 505, loss is 2.320635\n",
      "epoch: 1 step: 506, loss is 2.3029158\n",
      "epoch: 1 step: 507, loss is 2.3215039\n",
      "epoch: 1 step: 508, loss is 2.2810879\n",
      "epoch: 1 step: 509, loss is 2.3106673\n",
      "epoch: 1 step: 510, loss is 2.307964\n",
      "epoch: 1 step: 511, loss is 2.296606\n",
      "epoch: 1 step: 512, loss is 2.2951171\n",
      "epoch: 1 step: 513, loss is 2.3012047\n",
      "epoch: 1 step: 514, loss is 2.3093913\n",
      "epoch: 1 step: 515, loss is 2.3011558\n",
      "epoch: 1 step: 516, loss is 2.2989924\n",
      "epoch: 1 step: 517, loss is 2.3035715\n",
      "epoch: 1 step: 518, loss is 2.282877\n",
      "epoch: 1 step: 519, loss is 2.2973971\n",
      "epoch: 1 step: 520, loss is 2.2682533\n",
      "epoch: 1 step: 521, loss is 2.313599\n",
      "epoch: 1 step: 522, loss is 2.3040128\n",
      "epoch: 1 step: 523, loss is 2.2736752\n",
      "epoch: 1 step: 524, loss is 2.2973585\n",
      "epoch: 1 step: 525, loss is 2.3133152\n",
      "epoch: 1 step: 526, loss is 2.2809641\n",
      "epoch: 1 step: 527, loss is 2.2779276\n",
      "epoch: 1 step: 528, loss is 2.2943566\n",
      "epoch: 1 step: 529, loss is 2.3105042\n",
      "epoch: 1 step: 530, loss is 2.3060637\n",
      "epoch: 1 step: 531, loss is 2.319663\n",
      "epoch: 1 step: 532, loss is 2.3150003\n",
      "epoch: 1 step: 533, loss is 2.3130155\n",
      "epoch: 1 step: 534, loss is 2.288615\n",
      "epoch: 1 step: 535, loss is 2.289403\n",
      "epoch: 1 step: 536, loss is 2.2832966\n",
      "epoch: 1 step: 537, loss is 2.3148556\n",
      "epoch: 1 step: 538, loss is 2.287791\n",
      "epoch: 1 step: 539, loss is 2.3002827\n",
      "epoch: 1 step: 540, loss is 2.2816036\n",
      "epoch: 1 step: 541, loss is 2.3084803\n",
      "epoch: 1 step: 542, loss is 2.307243\n",
      "epoch: 1 step: 543, loss is 2.2778895\n",
      "epoch: 1 step: 544, loss is 2.2869332\n",
      "epoch: 1 step: 545, loss is 2.2932367\n",
      "epoch: 1 step: 546, loss is 2.3206754\n",
      "epoch: 1 step: 547, loss is 2.3091438\n",
      "epoch: 1 step: 548, loss is 2.3265512\n",
      "epoch: 1 step: 549, loss is 2.323424\n",
      "epoch: 1 step: 550, loss is 2.3001273\n",
      "epoch: 1 step: 551, loss is 2.2940302\n",
      "epoch: 1 step: 552, loss is 2.3136816\n",
      "epoch: 1 step: 553, loss is 2.316302\n",
      "epoch: 1 step: 554, loss is 2.292569\n",
      "epoch: 1 step: 555, loss is 2.3154252\n",
      "epoch: 1 step: 556, loss is 2.328352\n",
      "epoch: 1 step: 557, loss is 2.2932153\n",
      "epoch: 1 step: 558, loss is 2.2921135\n",
      "epoch: 1 step: 559, loss is 2.3006954\n",
      "epoch: 1 step: 560, loss is 2.3123336\n",
      "epoch: 1 step: 561, loss is 2.2931616\n",
      "epoch: 1 step: 562, loss is 2.3131585\n",
      "epoch: 1 step: 563, loss is 2.305871\n",
      "epoch: 1 step: 564, loss is 2.3145332\n",
      "epoch: 1 step: 565, loss is 2.329773\n",
      "epoch: 1 step: 566, loss is 2.2873905\n",
      "epoch: 1 step: 567, loss is 2.3137257\n",
      "epoch: 1 step: 568, loss is 2.3202243\n",
      "epoch: 1 step: 569, loss is 2.3011332\n",
      "epoch: 1 step: 570, loss is 2.293823\n",
      "epoch: 1 step: 571, loss is 2.2900517\n",
      "epoch: 1 step: 572, loss is 2.290487\n",
      "epoch: 1 step: 573, loss is 2.3053277\n",
      "epoch: 1 step: 574, loss is 2.3042114\n",
      "epoch: 1 step: 575, loss is 2.289379\n",
      "epoch: 1 step: 576, loss is 2.3234544\n",
      "epoch: 1 step: 577, loss is 2.3099082\n",
      "epoch: 1 step: 578, loss is 2.2964838\n",
      "epoch: 1 step: 579, loss is 2.307637\n",
      "epoch: 1 step: 580, loss is 2.273672\n",
      "epoch: 1 step: 581, loss is 2.2998378\n",
      "epoch: 1 step: 582, loss is 2.3050895\n",
      "epoch: 1 step: 583, loss is 2.315682\n",
      "epoch: 1 step: 584, loss is 2.2892554\n",
      "epoch: 1 step: 585, loss is 2.3045485\n",
      "epoch: 1 step: 586, loss is 2.315457\n",
      "epoch: 1 step: 587, loss is 2.2909138\n",
      "epoch: 1 step: 588, loss is 2.3016748\n",
      "epoch: 1 step: 589, loss is 2.3109906\n",
      "epoch: 1 step: 590, loss is 2.281244\n",
      "epoch: 1 step: 591, loss is 2.317811\n",
      "epoch: 1 step: 592, loss is 2.2943132\n",
      "epoch: 1 step: 593, loss is 2.2946103\n",
      "epoch: 1 step: 594, loss is 2.2963185\n",
      "epoch: 1 step: 595, loss is 2.3080678\n",
      "epoch: 1 step: 596, loss is 2.2944338\n",
      "epoch: 1 step: 597, loss is 2.296767\n",
      "epoch: 1 step: 598, loss is 2.3051655\n",
      "epoch: 1 step: 599, loss is 2.2934492\n",
      "epoch: 1 step: 600, loss is 2.2933972\n",
      "epoch: 1 step: 601, loss is 2.3006642\n",
      "epoch: 1 step: 602, loss is 2.3083375\n",
      "epoch: 1 step: 603, loss is 2.3234012\n",
      "epoch: 1 step: 604, loss is 2.29773\n",
      "epoch: 1 step: 605, loss is 2.3093076\n",
      "epoch: 1 step: 606, loss is 2.2906282\n",
      "epoch: 1 step: 607, loss is 2.319785\n",
      "epoch: 1 step: 608, loss is 2.299419\n",
      "epoch: 1 step: 609, loss is 2.2908318\n",
      "epoch: 1 step: 610, loss is 2.3138158\n",
      "epoch: 1 step: 611, loss is 2.2998571\n",
      "epoch: 1 step: 612, loss is 2.30804\n",
      "epoch: 1 step: 613, loss is 2.2893558\n",
      "epoch: 1 step: 614, loss is 2.3177118\n",
      "epoch: 1 step: 615, loss is 2.2897608\n",
      "epoch: 1 step: 616, loss is 2.3037498\n",
      "epoch: 1 step: 617, loss is 2.2919605\n",
      "epoch: 1 step: 618, loss is 2.2974615\n",
      "epoch: 1 step: 619, loss is 2.3113513\n",
      "epoch: 1 step: 620, loss is 2.293627\n",
      "epoch: 1 step: 621, loss is 2.3012867\n",
      "epoch: 1 step: 622, loss is 2.304394\n",
      "epoch: 1 step: 623, loss is 2.2912834\n",
      "epoch: 1 step: 624, loss is 2.3095298\n",
      "epoch: 1 step: 625, loss is 2.2902918\n",
      "epoch: 1 step: 626, loss is 2.310888\n",
      "epoch: 1 step: 627, loss is 2.2958944\n",
      "epoch: 1 step: 628, loss is 2.3061802\n",
      "epoch: 1 step: 629, loss is 2.3106954\n",
      "epoch: 1 step: 630, loss is 2.2872605\n",
      "epoch: 1 step: 631, loss is 2.311028\n",
      "epoch: 1 step: 632, loss is 2.305148\n",
      "epoch: 1 step: 633, loss is 2.303318\n",
      "epoch: 1 step: 634, loss is 2.3074007\n",
      "epoch: 1 step: 635, loss is 2.2989275\n",
      "epoch: 1 step: 636, loss is 2.3041816\n",
      "epoch: 1 step: 637, loss is 2.3045962\n",
      "epoch: 1 step: 638, loss is 2.3216822\n",
      "epoch: 1 step: 639, loss is 2.3139756\n",
      "epoch: 1 step: 640, loss is 2.288162\n",
      "epoch: 1 step: 641, loss is 2.3075368\n",
      "epoch: 1 step: 642, loss is 2.287914\n",
      "epoch: 1 step: 643, loss is 2.3034594\n",
      "epoch: 1 step: 644, loss is 2.3036377\n",
      "epoch: 1 step: 645, loss is 2.29202\n",
      "epoch: 1 step: 646, loss is 2.3083818\n",
      "epoch: 1 step: 647, loss is 2.2811332\n",
      "epoch: 1 step: 648, loss is 2.2940273\n",
      "epoch: 1 step: 649, loss is 2.271212\n",
      "epoch: 1 step: 650, loss is 2.2910943\n",
      "epoch: 1 step: 651, loss is 2.3000746\n",
      "epoch: 1 step: 652, loss is 2.3021822\n",
      "epoch: 1 step: 653, loss is 2.2925012\n",
      "epoch: 1 step: 654, loss is 2.3011878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 655, loss is 2.3102322\n",
      "epoch: 1 step: 656, loss is 2.2913764\n",
      "epoch: 1 step: 657, loss is 2.295873\n",
      "epoch: 1 step: 658, loss is 2.307621\n",
      "epoch: 1 step: 659, loss is 2.2850494\n",
      "epoch: 1 step: 660, loss is 2.297321\n",
      "epoch: 1 step: 661, loss is 2.28621\n",
      "epoch: 1 step: 662, loss is 2.313583\n",
      "epoch: 1 step: 663, loss is 2.2948415\n",
      "epoch: 1 step: 664, loss is 2.293938\n",
      "epoch: 1 step: 665, loss is 2.315714\n",
      "epoch: 1 step: 666, loss is 2.3073995\n",
      "epoch: 1 step: 667, loss is 2.332832\n",
      "epoch: 1 step: 668, loss is 2.2968216\n",
      "epoch: 1 step: 669, loss is 2.2948122\n",
      "epoch: 1 step: 670, loss is 2.3037162\n",
      "epoch: 1 step: 671, loss is 2.3017802\n",
      "epoch: 1 step: 672, loss is 2.2979\n",
      "epoch: 1 step: 673, loss is 2.3237748\n",
      "epoch: 1 step: 674, loss is 2.315301\n",
      "epoch: 1 step: 675, loss is 2.29055\n",
      "epoch: 1 step: 676, loss is 2.3049011\n",
      "epoch: 1 step: 677, loss is 2.3190386\n",
      "epoch: 1 step: 678, loss is 2.3030374\n",
      "epoch: 1 step: 679, loss is 2.3275535\n",
      "epoch: 1 step: 680, loss is 2.2975557\n",
      "epoch: 1 step: 681, loss is 2.322021\n",
      "epoch: 1 step: 682, loss is 2.330593\n",
      "epoch: 1 step: 683, loss is 2.283628\n",
      "epoch: 1 step: 684, loss is 2.3069916\n",
      "epoch: 1 step: 685, loss is 2.3051171\n",
      "epoch: 1 step: 686, loss is 2.3169146\n",
      "epoch: 1 step: 687, loss is 2.3123875\n",
      "epoch: 1 step: 688, loss is 2.3038833\n",
      "epoch: 1 step: 689, loss is 2.3001525\n",
      "epoch: 1 step: 690, loss is 2.2971184\n",
      "epoch: 1 step: 691, loss is 2.3168573\n",
      "epoch: 1 step: 692, loss is 2.3155293\n",
      "epoch: 1 step: 693, loss is 2.318733\n",
      "epoch: 1 step: 694, loss is 2.3147197\n",
      "epoch: 1 step: 695, loss is 2.2955406\n",
      "epoch: 1 step: 696, loss is 2.3151016\n",
      "epoch: 1 step: 697, loss is 2.3095846\n",
      "epoch: 1 step: 698, loss is 2.3115995\n",
      "epoch: 1 step: 699, loss is 2.2911062\n",
      "epoch: 1 step: 700, loss is 2.307134\n",
      "epoch: 1 step: 701, loss is 2.2972178\n",
      "epoch: 1 step: 702, loss is 2.3008478\n",
      "epoch: 1 step: 703, loss is 2.303612\n",
      "epoch: 1 step: 704, loss is 2.3049378\n",
      "epoch: 1 step: 705, loss is 2.30499\n",
      "epoch: 1 step: 706, loss is 2.2950044\n",
      "epoch: 1 step: 707, loss is 2.2988966\n",
      "epoch: 1 step: 708, loss is 2.3097436\n",
      "epoch: 1 step: 709, loss is 2.3001184\n",
      "epoch: 1 step: 710, loss is 2.305894\n",
      "epoch: 1 step: 711, loss is 2.2934623\n",
      "epoch: 1 step: 712, loss is 2.3162377\n",
      "epoch: 1 step: 713, loss is 2.312164\n",
      "epoch: 1 step: 714, loss is 2.2881353\n",
      "epoch: 1 step: 715, loss is 2.2775884\n",
      "epoch: 1 step: 716, loss is 2.3074195\n",
      "epoch: 1 step: 717, loss is 2.299676\n",
      "epoch: 1 step: 718, loss is 2.3063743\n",
      "epoch: 1 step: 719, loss is 2.3086648\n",
      "epoch: 1 step: 720, loss is 2.302172\n",
      "epoch: 1 step: 721, loss is 2.3077228\n",
      "epoch: 1 step: 722, loss is 2.3149374\n",
      "epoch: 1 step: 723, loss is 2.321903\n",
      "epoch: 1 step: 724, loss is 2.3105717\n",
      "epoch: 1 step: 725, loss is 2.2992425\n",
      "epoch: 1 step: 726, loss is 2.300151\n",
      "epoch: 1 step: 727, loss is 2.3003213\n",
      "epoch: 1 step: 728, loss is 2.3072\n",
      "epoch: 1 step: 729, loss is 2.311048\n",
      "epoch: 1 step: 730, loss is 2.3048882\n",
      "epoch: 1 step: 731, loss is 2.2980275\n",
      "epoch: 1 step: 732, loss is 2.2974885\n",
      "epoch: 1 step: 733, loss is 2.295736\n",
      "epoch: 1 step: 734, loss is 2.303401\n",
      "epoch: 1 step: 735, loss is 2.29463\n",
      "epoch: 1 step: 736, loss is 2.3187785\n",
      "epoch: 1 step: 737, loss is 2.2825274\n",
      "epoch: 1 step: 738, loss is 2.3025236\n",
      "epoch: 1 step: 739, loss is 2.2952316\n",
      "epoch: 1 step: 740, loss is 2.308588\n",
      "epoch: 1 step: 741, loss is 2.2979877\n",
      "epoch: 1 step: 742, loss is 2.3066957\n",
      "epoch: 1 step: 743, loss is 2.3050082\n",
      "epoch: 1 step: 744, loss is 2.2946615\n",
      "epoch: 1 step: 745, loss is 2.29807\n",
      "epoch: 1 step: 746, loss is 2.3128211\n",
      "epoch: 1 step: 747, loss is 2.3081903\n",
      "epoch: 1 step: 748, loss is 2.3091848\n",
      "epoch: 1 step: 749, loss is 2.2999833\n",
      "epoch: 1 step: 750, loss is 2.2902906\n",
      "epoch: 1 step: 751, loss is 2.302201\n",
      "epoch: 1 step: 752, loss is 2.303585\n",
      "epoch: 1 step: 753, loss is 2.3011124\n",
      "epoch: 1 step: 754, loss is 2.3069932\n",
      "epoch: 1 step: 755, loss is 2.2885427\n",
      "epoch: 1 step: 756, loss is 2.3124585\n",
      "epoch: 1 step: 757, loss is 2.2955384\n",
      "epoch: 1 step: 758, loss is 2.2961204\n",
      "epoch: 1 step: 759, loss is 2.2872782\n",
      "epoch: 1 step: 760, loss is 2.3148544\n",
      "epoch: 1 step: 761, loss is 2.2964108\n",
      "epoch: 1 step: 762, loss is 2.2983449\n",
      "epoch: 1 step: 763, loss is 2.3179257\n",
      "epoch: 1 step: 764, loss is 2.3134692\n",
      "epoch: 1 step: 765, loss is 2.295947\n",
      "epoch: 1 step: 766, loss is 2.2833972\n",
      "epoch: 1 step: 767, loss is 2.2882404\n",
      "epoch: 1 step: 768, loss is 2.2993746\n",
      "epoch: 1 step: 769, loss is 2.3029695\n",
      "epoch: 1 step: 770, loss is 2.2959106\n",
      "epoch: 1 step: 771, loss is 2.2938519\n",
      "epoch: 1 step: 772, loss is 2.305522\n",
      "epoch: 1 step: 773, loss is 2.294361\n",
      "epoch: 1 step: 774, loss is 2.3154924\n",
      "epoch: 1 step: 775, loss is 2.3133793\n",
      "epoch: 1 step: 776, loss is 2.3027537\n",
      "epoch: 1 step: 777, loss is 2.3026536\n",
      "epoch: 1 step: 778, loss is 2.3030488\n",
      "epoch: 1 step: 779, loss is 2.3033097\n",
      "epoch: 1 step: 780, loss is 2.3075626\n",
      "epoch: 1 step: 781, loss is 2.3181753\n",
      "epoch: 1 step: 782, loss is 2.2853324\n",
      "epoch: 1 step: 783, loss is 2.290463\n",
      "epoch: 1 step: 784, loss is 2.2937121\n",
      "epoch: 1 step: 785, loss is 2.3021219\n",
      "epoch: 1 step: 786, loss is 2.285659\n",
      "epoch: 1 step: 787, loss is 2.2902076\n",
      "epoch: 1 step: 788, loss is 2.299692\n",
      "epoch: 1 step: 789, loss is 2.2999747\n",
      "epoch: 1 step: 790, loss is 2.2854106\n",
      "epoch: 1 step: 791, loss is 2.304365\n",
      "epoch: 1 step: 792, loss is 2.3058422\n",
      "epoch: 1 step: 793, loss is 2.3010733\n",
      "epoch: 1 step: 794, loss is 2.3025198\n",
      "epoch: 1 step: 795, loss is 2.3129182\n",
      "epoch: 1 step: 796, loss is 2.3184922\n",
      "epoch: 1 step: 797, loss is 2.3208468\n",
      "epoch: 1 step: 798, loss is 2.3229256\n",
      "epoch: 1 step: 799, loss is 2.2847962\n",
      "epoch: 1 step: 800, loss is 2.2900836\n",
      "epoch: 1 step: 801, loss is 2.3147871\n",
      "epoch: 1 step: 802, loss is 2.3179579\n",
      "epoch: 1 step: 803, loss is 2.305637\n",
      "epoch: 1 step: 804, loss is 2.301382\n",
      "epoch: 1 step: 805, loss is 2.2974308\n",
      "epoch: 1 step: 806, loss is 2.3048515\n",
      "epoch: 1 step: 807, loss is 2.2998939\n",
      "epoch: 1 step: 808, loss is 2.3170705\n",
      "epoch: 1 step: 809, loss is 2.3007472\n",
      "epoch: 1 step: 810, loss is 2.2999027\n",
      "epoch: 1 step: 811, loss is 2.307699\n",
      "epoch: 1 step: 812, loss is 2.3016357\n",
      "epoch: 1 step: 813, loss is 2.2900414\n",
      "epoch: 1 step: 814, loss is 2.294171\n",
      "epoch: 1 step: 815, loss is 2.2980204\n",
      "epoch: 1 step: 816, loss is 2.289143\n",
      "epoch: 1 step: 817, loss is 2.3157756\n",
      "epoch: 1 step: 818, loss is 2.3192317\n",
      "epoch: 1 step: 819, loss is 2.2968261\n",
      "epoch: 1 step: 820, loss is 2.3109899\n",
      "epoch: 1 step: 821, loss is 2.2971447\n",
      "epoch: 1 step: 822, loss is 2.2979338\n",
      "epoch: 1 step: 823, loss is 2.2955406\n",
      "epoch: 1 step: 824, loss is 2.2971122\n",
      "epoch: 1 step: 825, loss is 2.305658\n",
      "epoch: 1 step: 826, loss is 2.3034537\n",
      "epoch: 1 step: 827, loss is 2.2873821\n",
      "epoch: 1 step: 828, loss is 2.3087678\n",
      "epoch: 1 step: 829, loss is 2.2891855\n",
      "epoch: 1 step: 830, loss is 2.3093543\n",
      "epoch: 1 step: 831, loss is 2.3006544\n",
      "epoch: 1 step: 832, loss is 2.313808\n",
      "epoch: 1 step: 833, loss is 2.2985067\n",
      "epoch: 1 step: 834, loss is 2.287603\n",
      "epoch: 1 step: 835, loss is 2.2965925\n",
      "epoch: 1 step: 836, loss is 2.2992706\n",
      "epoch: 1 step: 837, loss is 2.314029\n",
      "epoch: 1 step: 838, loss is 2.3072047\n",
      "epoch: 1 step: 839, loss is 2.305898\n",
      "epoch: 1 step: 840, loss is 2.3147345\n",
      "epoch: 1 step: 841, loss is 2.2779918\n",
      "epoch: 1 step: 842, loss is 2.3039527\n",
      "epoch: 1 step: 843, loss is 2.306117\n",
      "epoch: 1 step: 844, loss is 2.3051875\n",
      "epoch: 1 step: 845, loss is 2.2927203\n",
      "epoch: 1 step: 846, loss is 2.3009913\n",
      "epoch: 1 step: 847, loss is 2.314399\n",
      "epoch: 1 step: 848, loss is 2.3148677\n",
      "epoch: 1 step: 849, loss is 2.3018699\n",
      "epoch: 1 step: 850, loss is 2.2723296\n",
      "epoch: 1 step: 851, loss is 2.3028169\n",
      "epoch: 1 step: 852, loss is 2.312001\n",
      "epoch: 1 step: 853, loss is 2.2865028\n",
      "epoch: 1 step: 854, loss is 2.3288672\n",
      "epoch: 1 step: 855, loss is 2.2953496\n",
      "epoch: 1 step: 856, loss is 2.3004355\n",
      "epoch: 1 step: 857, loss is 2.3038745\n",
      "epoch: 1 step: 858, loss is 2.3004336\n",
      "epoch: 1 step: 859, loss is 2.305252\n",
      "epoch: 1 step: 860, loss is 2.3102283\n",
      "epoch: 1 step: 861, loss is 2.3037024\n",
      "epoch: 1 step: 862, loss is 2.3095381\n",
      "epoch: 1 step: 863, loss is 2.29116\n",
      "epoch: 1 step: 864, loss is 2.2901523\n",
      "epoch: 1 step: 865, loss is 2.2959347\n",
      "epoch: 1 step: 866, loss is 2.3067114\n",
      "epoch: 1 step: 867, loss is 2.2979276\n",
      "epoch: 1 step: 868, loss is 2.2997694\n",
      "epoch: 1 step: 869, loss is 2.290311\n",
      "epoch: 1 step: 870, loss is 2.3177404\n",
      "epoch: 1 step: 871, loss is 2.3023138\n",
      "epoch: 1 step: 872, loss is 2.2880244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 873, loss is 2.3031623\n",
      "epoch: 1 step: 874, loss is 2.2871537\n",
      "epoch: 1 step: 875, loss is 2.3017702\n",
      "epoch: 1 step: 876, loss is 2.3099978\n",
      "epoch: 1 step: 877, loss is 2.313296\n",
      "epoch: 1 step: 878, loss is 2.3091583\n",
      "epoch: 1 step: 879, loss is 2.3114018\n",
      "epoch: 1 step: 880, loss is 2.2971528\n",
      "epoch: 1 step: 881, loss is 2.3112898\n",
      "epoch: 1 step: 882, loss is 2.29563\n",
      "epoch: 1 step: 883, loss is 2.2977839\n",
      "epoch: 1 step: 884, loss is 2.2919216\n",
      "epoch: 1 step: 885, loss is 2.2891948\n",
      "epoch: 1 step: 886, loss is 2.309874\n",
      "epoch: 1 step: 887, loss is 2.3052676\n",
      "epoch: 1 step: 888, loss is 2.3004248\n",
      "epoch: 1 step: 889, loss is 2.2973936\n",
      "epoch: 1 step: 890, loss is 2.2825267\n",
      "epoch: 1 step: 891, loss is 2.299543\n",
      "epoch: 1 step: 892, loss is 2.295973\n",
      "epoch: 1 step: 893, loss is 2.2887182\n",
      "epoch: 1 step: 894, loss is 2.3178341\n",
      "epoch: 1 step: 895, loss is 2.290558\n",
      "epoch: 1 step: 896, loss is 2.2920232\n",
      "epoch: 1 step: 897, loss is 2.3054895\n",
      "epoch: 1 step: 898, loss is 2.2950099\n",
      "epoch: 1 step: 899, loss is 2.3000224\n",
      "epoch: 1 step: 900, loss is 2.3158062\n",
      "epoch: 1 step: 901, loss is 2.3122413\n",
      "epoch: 1 step: 902, loss is 2.3002872\n",
      "epoch: 1 step: 903, loss is 2.308528\n",
      "epoch: 1 step: 904, loss is 2.3021264\n",
      "epoch: 1 step: 905, loss is 2.310541\n",
      "epoch: 1 step: 906, loss is 2.3030605\n",
      "epoch: 1 step: 907, loss is 2.2958143\n",
      "epoch: 1 step: 908, loss is 2.2817593\n",
      "epoch: 1 step: 909, loss is 2.2989738\n",
      "epoch: 1 step: 910, loss is 2.260397\n",
      "epoch: 1 step: 911, loss is 2.3027582\n",
      "epoch: 1 step: 912, loss is 2.3084278\n",
      "epoch: 1 step: 913, loss is 2.3044982\n",
      "epoch: 1 step: 914, loss is 2.3110743\n",
      "epoch: 1 step: 915, loss is 2.2857854\n",
      "epoch: 1 step: 916, loss is 2.3004816\n",
      "epoch: 1 step: 917, loss is 2.284527\n",
      "epoch: 1 step: 918, loss is 2.2762904\n",
      "epoch: 1 step: 919, loss is 2.289606\n",
      "epoch: 1 step: 920, loss is 2.2999635\n",
      "epoch: 1 step: 921, loss is 2.3216562\n",
      "epoch: 1 step: 922, loss is 2.2931342\n",
      "epoch: 1 step: 923, loss is 2.2884364\n",
      "epoch: 1 step: 924, loss is 2.2947524\n",
      "epoch: 1 step: 925, loss is 2.305177\n",
      "epoch: 1 step: 926, loss is 2.2992275\n",
      "epoch: 1 step: 927, loss is 2.3134334\n",
      "epoch: 1 step: 928, loss is 2.302908\n",
      "epoch: 1 step: 929, loss is 2.3023288\n",
      "epoch: 1 step: 930, loss is 2.2991161\n",
      "epoch: 1 step: 931, loss is 2.2864997\n",
      "epoch: 1 step: 932, loss is 2.2915423\n",
      "epoch: 1 step: 933, loss is 2.3226018\n",
      "epoch: 1 step: 934, loss is 2.301323\n",
      "epoch: 1 step: 935, loss is 2.3027115\n",
      "epoch: 1 step: 936, loss is 2.2985985\n",
      "epoch: 1 step: 937, loss is 2.305152\n",
      "epoch: 1 step: 938, loss is 2.296148\n",
      "epoch: 1 step: 939, loss is 2.317224\n",
      "epoch: 1 step: 940, loss is 2.3002205\n",
      "epoch: 1 step: 941, loss is 2.3034089\n",
      "epoch: 1 step: 942, loss is 2.3158052\n",
      "epoch: 1 step: 943, loss is 2.3189802\n",
      "epoch: 1 step: 944, loss is 2.311264\n",
      "epoch: 1 step: 945, loss is 2.3070526\n",
      "epoch: 1 step: 946, loss is 2.3012083\n",
      "epoch: 1 step: 947, loss is 2.307549\n",
      "epoch: 1 step: 948, loss is 2.2835033\n",
      "epoch: 1 step: 949, loss is 2.3010733\n",
      "epoch: 1 step: 950, loss is 2.3096607\n",
      "epoch: 1 step: 951, loss is 2.3152847\n",
      "epoch: 1 step: 952, loss is 2.288285\n",
      "epoch: 1 step: 953, loss is 2.302811\n",
      "epoch: 1 step: 954, loss is 2.2754002\n",
      "epoch: 1 step: 955, loss is 2.3158457\n",
      "epoch: 1 step: 956, loss is 2.2867475\n",
      "epoch: 1 step: 957, loss is 2.2823434\n",
      "epoch: 1 step: 958, loss is 2.2894948\n",
      "epoch: 1 step: 959, loss is 2.3004472\n",
      "epoch: 1 step: 960, loss is 2.2852335\n",
      "epoch: 1 step: 961, loss is 2.314036\n",
      "epoch: 1 step: 962, loss is 2.3139746\n",
      "epoch: 1 step: 963, loss is 2.2846377\n",
      "epoch: 1 step: 964, loss is 2.299121\n",
      "epoch: 1 step: 965, loss is 2.3161774\n",
      "epoch: 1 step: 966, loss is 2.3050766\n",
      "epoch: 1 step: 967, loss is 2.3023741\n",
      "epoch: 1 step: 968, loss is 2.2775714\n",
      "epoch: 1 step: 969, loss is 2.3225696\n",
      "epoch: 1 step: 970, loss is 2.302535\n",
      "epoch: 1 step: 971, loss is 2.3182652\n",
      "epoch: 1 step: 972, loss is 2.2897804\n",
      "epoch: 1 step: 973, loss is 2.3057866\n",
      "epoch: 1 step: 974, loss is 2.3038855\n",
      "epoch: 1 step: 975, loss is 2.3276873\n",
      "epoch: 1 step: 976, loss is 2.2749715\n",
      "epoch: 1 step: 977, loss is 2.3182576\n",
      "epoch: 1 step: 978, loss is 2.2976213\n",
      "epoch: 1 step: 979, loss is 2.2769318\n",
      "epoch: 1 step: 980, loss is 2.2819622\n",
      "epoch: 1 step: 981, loss is 2.3040094\n",
      "epoch: 1 step: 982, loss is 2.2917233\n",
      "epoch: 1 step: 983, loss is 2.3009284\n",
      "epoch: 1 step: 984, loss is 2.3173456\n",
      "epoch: 1 step: 985, loss is 2.3095\n",
      "epoch: 1 step: 986, loss is 2.290797\n",
      "epoch: 1 step: 987, loss is 2.2957413\n",
      "epoch: 1 step: 988, loss is 2.2953782\n",
      "epoch: 1 step: 989, loss is 2.3036861\n",
      "epoch: 1 step: 990, loss is 2.3033235\n",
      "epoch: 1 step: 991, loss is 2.3213384\n",
      "epoch: 1 step: 992, loss is 2.2933939\n",
      "epoch: 1 step: 993, loss is 2.2873414\n",
      "epoch: 1 step: 994, loss is 2.294809\n",
      "epoch: 1 step: 995, loss is 2.3068347\n",
      "epoch: 1 step: 996, loss is 2.3071966\n",
      "epoch: 1 step: 997, loss is 2.286962\n",
      "epoch: 1 step: 998, loss is 2.2982466\n",
      "epoch: 1 step: 999, loss is 2.3055248\n",
      "epoch: 1 step: 1000, loss is 2.2790484\n",
      "epoch: 1 step: 1001, loss is 2.2850049\n",
      "epoch: 1 step: 1002, loss is 2.2810247\n",
      "epoch: 1 step: 1003, loss is 2.3146987\n",
      "epoch: 1 step: 1004, loss is 2.306401\n",
      "epoch: 1 step: 1005, loss is 2.2933621\n",
      "epoch: 1 step: 1006, loss is 2.2911937\n",
      "epoch: 1 step: 1007, loss is 2.2822783\n",
      "epoch: 1 step: 1008, loss is 2.3091888\n",
      "epoch: 1 step: 1009, loss is 2.2893205\n",
      "epoch: 1 step: 1010, loss is 2.3024542\n",
      "epoch: 1 step: 1011, loss is 2.2923272\n",
      "epoch: 1 step: 1012, loss is 2.3017018\n",
      "epoch: 1 step: 1013, loss is 2.2918968\n",
      "epoch: 1 step: 1014, loss is 2.294882\n",
      "epoch: 1 step: 1015, loss is 2.3014958\n",
      "epoch: 1 step: 1016, loss is 2.2930398\n",
      "epoch: 1 step: 1017, loss is 2.292839\n",
      "epoch: 1 step: 1018, loss is 2.2904432\n",
      "epoch: 1 step: 1019, loss is 2.299513\n",
      "epoch: 1 step: 1020, loss is 2.3101714\n",
      "epoch: 1 step: 1021, loss is 2.2923307\n",
      "epoch: 1 step: 1022, loss is 2.2935321\n",
      "epoch: 1 step: 1023, loss is 2.2846434\n",
      "epoch: 1 step: 1024, loss is 2.3087125\n",
      "epoch: 1 step: 1025, loss is 2.2758074\n",
      "epoch: 1 step: 1026, loss is 2.3139558\n",
      "epoch: 1 step: 1027, loss is 2.2851965\n",
      "epoch: 1 step: 1028, loss is 2.289121\n",
      "epoch: 1 step: 1029, loss is 2.3018496\n",
      "epoch: 1 step: 1030, loss is 2.309409\n",
      "epoch: 1 step: 1031, loss is 2.2982922\n",
      "epoch: 1 step: 1032, loss is 2.296003\n",
      "epoch: 1 step: 1033, loss is 2.291399\n",
      "epoch: 1 step: 1034, loss is 2.325349\n",
      "epoch: 1 step: 1035, loss is 2.2994943\n",
      "epoch: 1 step: 1036, loss is 2.2955854\n",
      "epoch: 1 step: 1037, loss is 2.2990496\n",
      "epoch: 1 step: 1038, loss is 2.300613\n",
      "epoch: 1 step: 1039, loss is 2.310447\n",
      "epoch: 1 step: 1040, loss is 2.2823453\n",
      "epoch: 1 step: 1041, loss is 2.294619\n",
      "epoch: 1 step: 1042, loss is 2.2943785\n",
      "epoch: 1 step: 1043, loss is 2.3046243\n",
      "epoch: 1 step: 1044, loss is 2.29079\n",
      "epoch: 1 step: 1045, loss is 2.3065383\n",
      "epoch: 1 step: 1046, loss is 2.2817056\n",
      "epoch: 1 step: 1047, loss is 2.3094394\n",
      "epoch: 1 step: 1048, loss is 2.2999945\n",
      "epoch: 1 step: 1049, loss is 2.2668679\n",
      "epoch: 1 step: 1050, loss is 2.2692318\n",
      "epoch: 1 step: 1051, loss is 2.3116205\n",
      "epoch: 1 step: 1052, loss is 2.2870274\n",
      "epoch: 1 step: 1053, loss is 2.3049352\n",
      "epoch: 1 step: 1054, loss is 2.3161387\n",
      "epoch: 1 step: 1055, loss is 2.2877529\n",
      "epoch: 1 step: 1056, loss is 2.2754443\n",
      "epoch: 1 step: 1057, loss is 2.2855902\n",
      "epoch: 1 step: 1058, loss is 2.298867\n",
      "epoch: 1 step: 1059, loss is 2.310007\n",
      "epoch: 1 step: 1060, loss is 2.2670376\n",
      "epoch: 1 step: 1061, loss is 2.299917\n",
      "epoch: 1 step: 1062, loss is 2.2924013\n",
      "epoch: 1 step: 1063, loss is 2.2917247\n",
      "epoch: 1 step: 1064, loss is 2.297001\n",
      "epoch: 1 step: 1065, loss is 2.298205\n",
      "epoch: 1 step: 1066, loss is 2.287852\n",
      "epoch: 1 step: 1067, loss is 2.2842536\n",
      "epoch: 1 step: 1068, loss is 2.2948465\n",
      "epoch: 1 step: 1069, loss is 2.279472\n",
      "epoch: 1 step: 1070, loss is 2.2912345\n",
      "epoch: 1 step: 1071, loss is 2.247047\n",
      "epoch: 1 step: 1072, loss is 2.279539\n",
      "epoch: 1 step: 1073, loss is 2.2942338\n",
      "epoch: 1 step: 1074, loss is 2.2864666\n",
      "epoch: 1 step: 1075, loss is 2.2911808\n",
      "epoch: 1 step: 1076, loss is 2.3177366\n",
      "epoch: 1 step: 1077, loss is 2.2726336\n",
      "epoch: 1 step: 1078, loss is 2.2807295\n",
      "epoch: 1 step: 1079, loss is 2.261997\n",
      "epoch: 1 step: 1080, loss is 2.2877405\n",
      "epoch: 1 step: 1081, loss is 2.2827857\n",
      "epoch: 1 step: 1082, loss is 2.2749722\n",
      "epoch: 1 step: 1083, loss is 2.2844608\n",
      "epoch: 1 step: 1084, loss is 2.2624254\n",
      "epoch: 1 step: 1085, loss is 2.2860131\n",
      "epoch: 1 step: 1086, loss is 2.2978797\n",
      "epoch: 1 step: 1087, loss is 2.2536016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1088, loss is 2.2675424\n",
      "epoch: 1 step: 1089, loss is 2.2616596\n",
      "epoch: 1 step: 1090, loss is 2.249545\n",
      "epoch: 1 step: 1091, loss is 2.252111\n",
      "epoch: 1 step: 1092, loss is 2.264727\n",
      "epoch: 1 step: 1093, loss is 2.248279\n",
      "epoch: 1 step: 1094, loss is 2.2439728\n",
      "epoch: 1 step: 1095, loss is 2.2381198\n",
      "epoch: 1 step: 1096, loss is 2.2190907\n",
      "epoch: 1 step: 1097, loss is 2.2336812\n",
      "epoch: 1 step: 1098, loss is 2.2016509\n",
      "epoch: 1 step: 1099, loss is 2.2105992\n",
      "epoch: 1 step: 1100, loss is 2.2064588\n",
      "epoch: 1 step: 1101, loss is 2.1868873\n",
      "epoch: 1 step: 1102, loss is 2.1646192\n",
      "epoch: 1 step: 1103, loss is 2.1826506\n",
      "epoch: 1 step: 1104, loss is 2.1500094\n",
      "epoch: 1 step: 1105, loss is 2.1318865\n",
      "epoch: 1 step: 1106, loss is 2.165578\n",
      "epoch: 1 step: 1107, loss is 2.1332889\n",
      "epoch: 1 step: 1108, loss is 2.0587566\n",
      "epoch: 1 step: 1109, loss is 2.0721748\n",
      "epoch: 1 step: 1110, loss is 2.1108203\n",
      "epoch: 1 step: 1111, loss is 1.9950761\n",
      "epoch: 1 step: 1112, loss is 1.9934865\n",
      "epoch: 1 step: 1113, loss is 1.962456\n",
      "epoch: 1 step: 1114, loss is 1.8021653\n",
      "epoch: 1 step: 1115, loss is 1.7627298\n",
      "epoch: 1 step: 1116, loss is 1.8440189\n",
      "epoch: 1 step: 1117, loss is 1.6543678\n",
      "epoch: 1 step: 1118, loss is 1.7518617\n",
      "epoch: 1 step: 1119, loss is 1.4267776\n",
      "epoch: 1 step: 1120, loss is 2.0399766\n",
      "epoch: 1 step: 1121, loss is 1.344728\n",
      "epoch: 1 step: 1122, loss is 1.6597602\n",
      "epoch: 1 step: 1123, loss is 1.4110892\n",
      "epoch: 1 step: 1124, loss is 1.8112912\n",
      "epoch: 1 step: 1125, loss is 1.6982661\n",
      "epoch: 1 step: 1126, loss is 1.3850465\n",
      "epoch: 1 step: 1127, loss is 1.2072488\n",
      "epoch: 1 step: 1128, loss is 1.266442\n",
      "epoch: 1 step: 1129, loss is 1.5867492\n",
      "epoch: 1 step: 1130, loss is 1.3450906\n",
      "epoch: 1 step: 1131, loss is 1.3733654\n",
      "epoch: 1 step: 1132, loss is 1.7441196\n",
      "epoch: 1 step: 1133, loss is 1.1044799\n",
      "epoch: 1 step: 1134, loss is 1.2898557\n",
      "epoch: 1 step: 1135, loss is 1.1839144\n",
      "epoch: 1 step: 1136, loss is 1.4258687\n",
      "epoch: 1 step: 1137, loss is 1.0252545\n",
      "epoch: 1 step: 1138, loss is 1.358956\n",
      "epoch: 1 step: 1139, loss is 0.889505\n",
      "epoch: 1 step: 1140, loss is 1.3340707\n",
      "epoch: 1 step: 1141, loss is 1.1929129\n",
      "epoch: 1 step: 1142, loss is 0.99921536\n",
      "epoch: 1 step: 1143, loss is 1.2199134\n",
      "epoch: 1 step: 1144, loss is 1.2113255\n",
      "epoch: 1 step: 1145, loss is 1.1671268\n",
      "epoch: 1 step: 1146, loss is 0.75631946\n",
      "epoch: 1 step: 1147, loss is 1.1279317\n",
      "epoch: 1 step: 1148, loss is 1.181987\n",
      "epoch: 1 step: 1149, loss is 0.8700078\n",
      "epoch: 1 step: 1150, loss is 0.87349635\n",
      "epoch: 1 step: 1151, loss is 1.016213\n",
      "epoch: 1 step: 1152, loss is 1.0450869\n",
      "epoch: 1 step: 1153, loss is 1.1205993\n",
      "epoch: 1 step: 1154, loss is 0.9832516\n",
      "epoch: 1 step: 1155, loss is 0.8912687\n",
      "epoch: 1 step: 1156, loss is 1.0298913\n",
      "epoch: 1 step: 1157, loss is 1.1364913\n",
      "epoch: 1 step: 1158, loss is 1.335869\n",
      "epoch: 1 step: 1159, loss is 1.0604233\n",
      "epoch: 1 step: 1160, loss is 1.2719363\n",
      "epoch: 1 step: 1161, loss is 0.8753313\n",
      "epoch: 1 step: 1162, loss is 0.9091005\n",
      "epoch: 1 step: 1163, loss is 0.8246199\n",
      "epoch: 1 step: 1164, loss is 1.2287884\n",
      "epoch: 1 step: 1165, loss is 1.0109413\n",
      "epoch: 1 step: 1166, loss is 1.1589344\n",
      "epoch: 1 step: 1167, loss is 1.0861974\n",
      "epoch: 1 step: 1168, loss is 0.79021585\n",
      "epoch: 1 step: 1169, loss is 0.81106186\n",
      "epoch: 1 step: 1170, loss is 0.8004888\n",
      "epoch: 1 step: 1171, loss is 0.8298166\n",
      "epoch: 1 step: 1172, loss is 0.92321867\n",
      "epoch: 1 step: 1173, loss is 0.9171902\n",
      "epoch: 1 step: 1174, loss is 0.74357355\n",
      "epoch: 1 step: 1175, loss is 0.83413064\n",
      "epoch: 1 step: 1176, loss is 0.8281987\n",
      "epoch: 1 step: 1177, loss is 0.52291757\n",
      "epoch: 1 step: 1178, loss is 0.63825583\n",
      "epoch: 1 step: 1179, loss is 0.48538858\n",
      "epoch: 1 step: 1180, loss is 1.0049577\n",
      "epoch: 1 step: 1181, loss is 0.66766924\n",
      "epoch: 1 step: 1182, loss is 0.6371405\n",
      "epoch: 1 step: 1183, loss is 0.50470287\n",
      "epoch: 1 step: 1184, loss is 0.8589455\n",
      "epoch: 1 step: 1185, loss is 0.5707868\n",
      "epoch: 1 step: 1186, loss is 0.8727033\n",
      "epoch: 1 step: 1187, loss is 0.67563576\n",
      "epoch: 1 step: 1188, loss is 0.4035128\n",
      "epoch: 1 step: 1189, loss is 0.56325096\n",
      "epoch: 1 step: 1190, loss is 0.6003528\n",
      "epoch: 1 step: 1191, loss is 0.4568742\n",
      "epoch: 1 step: 1192, loss is 0.7816615\n",
      "epoch: 1 step: 1193, loss is 0.7571825\n",
      "epoch: 1 step: 1194, loss is 0.35880414\n",
      "epoch: 1 step: 1195, loss is 0.7622887\n",
      "epoch: 1 step: 1196, loss is 0.57417303\n",
      "epoch: 1 step: 1197, loss is 0.49608213\n",
      "epoch: 1 step: 1198, loss is 0.66174346\n",
      "epoch: 1 step: 1199, loss is 0.42482385\n",
      "epoch: 1 step: 1200, loss is 0.6566991\n",
      "epoch: 1 step: 1201, loss is 0.3534481\n",
      "epoch: 1 step: 1202, loss is 0.5303238\n",
      "epoch: 1 step: 1203, loss is 0.35278657\n",
      "epoch: 1 step: 1204, loss is 0.79563475\n",
      "epoch: 1 step: 1205, loss is 0.6099856\n",
      "epoch: 1 step: 1206, loss is 0.5263937\n",
      "epoch: 1 step: 1207, loss is 0.8238551\n",
      "epoch: 1 step: 1208, loss is 0.7319562\n",
      "epoch: 1 step: 1209, loss is 0.62540185\n",
      "epoch: 1 step: 1210, loss is 0.43119323\n",
      "epoch: 1 step: 1211, loss is 0.7514308\n",
      "epoch: 1 step: 1212, loss is 0.63058066\n",
      "epoch: 1 step: 1213, loss is 0.62007314\n",
      "epoch: 1 step: 1214, loss is 0.62158936\n",
      "epoch: 1 step: 1215, loss is 0.33234674\n",
      "epoch: 1 step: 1216, loss is 0.5488497\n",
      "epoch: 1 step: 1217, loss is 0.4003605\n",
      "epoch: 1 step: 1218, loss is 0.5697545\n",
      "epoch: 1 step: 1219, loss is 0.5179593\n",
      "epoch: 1 step: 1220, loss is 0.3042847\n",
      "epoch: 1 step: 1221, loss is 0.4231641\n",
      "epoch: 1 step: 1222, loss is 0.45118767\n",
      "epoch: 1 step: 1223, loss is 0.47887662\n",
      "epoch: 1 step: 1224, loss is 0.24010941\n",
      "epoch: 1 step: 1225, loss is 0.37969667\n",
      "epoch: 1 step: 1226, loss is 0.5543907\n",
      "epoch: 1 step: 1227, loss is 0.4451645\n",
      "epoch: 1 step: 1228, loss is 0.53973985\n",
      "epoch: 1 step: 1229, loss is 0.449368\n",
      "epoch: 1 step: 1230, loss is 0.4418477\n",
      "epoch: 1 step: 1231, loss is 0.32141188\n",
      "epoch: 1 step: 1232, loss is 0.64090466\n",
      "epoch: 1 step: 1233, loss is 0.29195318\n",
      "epoch: 1 step: 1234, loss is 0.40919775\n",
      "epoch: 1 step: 1235, loss is 0.64512074\n",
      "epoch: 1 step: 1236, loss is 0.2606329\n",
      "epoch: 1 step: 1237, loss is 0.30170467\n",
      "epoch: 1 step: 1238, loss is 0.6755674\n",
      "epoch: 1 step: 1239, loss is 0.75205904\n",
      "epoch: 1 step: 1240, loss is 0.31296515\n",
      "epoch: 1 step: 1241, loss is 0.3465672\n",
      "epoch: 1 step: 1242, loss is 0.3872115\n",
      "epoch: 1 step: 1243, loss is 0.31513497\n",
      "epoch: 1 step: 1244, loss is 0.58435327\n",
      "epoch: 1 step: 1245, loss is 0.4246274\n",
      "epoch: 1 step: 1246, loss is 0.38923728\n",
      "epoch: 1 step: 1247, loss is 0.39452356\n",
      "epoch: 1 step: 1248, loss is 0.3681321\n",
      "epoch: 1 step: 1249, loss is 0.36653063\n",
      "epoch: 1 step: 1250, loss is 0.44858274\n",
      "epoch: 1 step: 1251, loss is 0.39440924\n",
      "epoch: 1 step: 1252, loss is 0.15798843\n",
      "epoch: 1 step: 1253, loss is 0.46425024\n",
      "epoch: 1 step: 1254, loss is 0.6547521\n",
      "epoch: 1 step: 1255, loss is 0.5702702\n",
      "epoch: 1 step: 1256, loss is 0.19768727\n",
      "epoch: 1 step: 1257, loss is 0.76974803\n",
      "epoch: 1 step: 1258, loss is 0.55794585\n",
      "epoch: 1 step: 1259, loss is 0.3924219\n",
      "epoch: 1 step: 1260, loss is 0.4206511\n",
      "epoch: 1 step: 1261, loss is 0.30326003\n",
      "epoch: 1 step: 1262, loss is 0.5465646\n",
      "epoch: 1 step: 1263, loss is 0.74397963\n",
      "epoch: 1 step: 1264, loss is 0.21475743\n",
      "epoch: 1 step: 1265, loss is 0.4294496\n",
      "epoch: 1 step: 1266, loss is 0.3040828\n",
      "epoch: 1 step: 1267, loss is 0.69136983\n",
      "epoch: 1 step: 1268, loss is 0.27121472\n",
      "epoch: 1 step: 1269, loss is 0.22336547\n",
      "epoch: 1 step: 1270, loss is 0.8442833\n",
      "epoch: 1 step: 1271, loss is 0.5695241\n",
      "epoch: 1 step: 1272, loss is 0.53827494\n",
      "epoch: 1 step: 1273, loss is 0.33630002\n",
      "epoch: 1 step: 1274, loss is 0.2481943\n",
      "epoch: 1 step: 1275, loss is 0.6716192\n",
      "epoch: 1 step: 1276, loss is 0.69538486\n",
      "epoch: 1 step: 1277, loss is 0.29734686\n",
      "epoch: 1 step: 1278, loss is 0.50784653\n",
      "epoch: 1 step: 1279, loss is 0.19863367\n",
      "epoch: 1 step: 1280, loss is 0.20352091\n",
      "epoch: 1 step: 1281, loss is 0.27536684\n",
      "epoch: 1 step: 1282, loss is 0.38028738\n",
      "epoch: 1 step: 1283, loss is 0.3343928\n",
      "epoch: 1 step: 1284, loss is 0.31941888\n",
      "epoch: 1 step: 1285, loss is 0.1659676\n",
      "epoch: 1 step: 1286, loss is 0.38793993\n",
      "epoch: 1 step: 1287, loss is 0.38915795\n",
      "epoch: 1 step: 1288, loss is 0.26213443\n",
      "epoch: 1 step: 1289, loss is 0.19209059\n",
      "epoch: 1 step: 1290, loss is 0.22460903\n",
      "epoch: 1 step: 1291, loss is 0.10232078\n",
      "epoch: 1 step: 1292, loss is 0.28648528\n",
      "epoch: 1 step: 1293, loss is 0.20533009\n",
      "epoch: 1 step: 1294, loss is 0.9279644\n",
      "epoch: 1 step: 1295, loss is 0.19514966\n",
      "epoch: 1 step: 1296, loss is 0.5199964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1297, loss is 0.3854781\n",
      "epoch: 1 step: 1298, loss is 0.32408315\n",
      "epoch: 1 step: 1299, loss is 0.45080662\n",
      "epoch: 1 step: 1300, loss is 0.34287444\n",
      "epoch: 1 step: 1301, loss is 0.1597861\n",
      "epoch: 1 step: 1302, loss is 0.3018836\n",
      "epoch: 1 step: 1303, loss is 0.08226354\n",
      "epoch: 1 step: 1304, loss is 0.21084036\n",
      "epoch: 1 step: 1305, loss is 0.24858579\n",
      "epoch: 1 step: 1306, loss is 0.19615841\n",
      "epoch: 1 step: 1307, loss is 0.11026871\n",
      "epoch: 1 step: 1308, loss is 0.26140866\n",
      "epoch: 1 step: 1309, loss is 0.18871139\n",
      "epoch: 1 step: 1310, loss is 0.55763274\n",
      "epoch: 1 step: 1311, loss is 0.5341658\n",
      "epoch: 1 step: 1312, loss is 0.16067365\n",
      "epoch: 1 step: 1313, loss is 0.38775754\n",
      "epoch: 1 step: 1314, loss is 0.34632853\n",
      "epoch: 1 step: 1315, loss is 0.4954602\n",
      "epoch: 1 step: 1316, loss is 0.5197998\n",
      "epoch: 1 step: 1317, loss is 0.2841068\n",
      "epoch: 1 step: 1318, loss is 0.22425379\n",
      "epoch: 1 step: 1319, loss is 0.26798826\n",
      "epoch: 1 step: 1320, loss is 0.74513364\n",
      "epoch: 1 step: 1321, loss is 0.2402286\n",
      "epoch: 1 step: 1322, loss is 0.71783525\n",
      "epoch: 1 step: 1323, loss is 0.3234564\n",
      "epoch: 1 step: 1324, loss is 0.22796169\n",
      "epoch: 1 step: 1325, loss is 0.24066381\n",
      "epoch: 1 step: 1326, loss is 0.37664276\n",
      "epoch: 1 step: 1327, loss is 0.3628193\n",
      "epoch: 1 step: 1328, loss is 0.37375253\n",
      "epoch: 1 step: 1329, loss is 0.540103\n",
      "epoch: 1 step: 1330, loss is 0.37791353\n",
      "epoch: 1 step: 1331, loss is 0.27414024\n",
      "epoch: 1 step: 1332, loss is 0.32766572\n",
      "epoch: 1 step: 1333, loss is 0.24528122\n",
      "epoch: 1 step: 1334, loss is 0.13337582\n",
      "epoch: 1 step: 1335, loss is 0.18881024\n",
      "epoch: 1 step: 1336, loss is 0.48075244\n",
      "epoch: 1 step: 1337, loss is 0.45489568\n",
      "epoch: 1 step: 1338, loss is 0.22490233\n",
      "epoch: 1 step: 1339, loss is 0.08437877\n",
      "epoch: 1 step: 1340, loss is 0.14129649\n",
      "epoch: 1 step: 1341, loss is 0.19174923\n",
      "epoch: 1 step: 1342, loss is 0.29148576\n",
      "epoch: 1 step: 1343, loss is 0.12587216\n",
      "epoch: 1 step: 1344, loss is 0.49878752\n",
      "epoch: 1 step: 1345, loss is 0.21743554\n",
      "epoch: 1 step: 1346, loss is 0.68428457\n",
      "epoch: 1 step: 1347, loss is 0.2880196\n",
      "epoch: 1 step: 1348, loss is 0.56442344\n",
      "epoch: 1 step: 1349, loss is 0.31250986\n",
      "epoch: 1 step: 1350, loss is 0.5833905\n",
      "epoch: 1 step: 1351, loss is 0.77457356\n",
      "epoch: 1 step: 1352, loss is 0.43730986\n",
      "epoch: 1 step: 1353, loss is 0.6334087\n",
      "epoch: 1 step: 1354, loss is 0.13107157\n",
      "epoch: 1 step: 1355, loss is 0.3727188\n",
      "epoch: 1 step: 1356, loss is 0.22439909\n",
      "epoch: 1 step: 1357, loss is 0.3862233\n",
      "epoch: 1 step: 1358, loss is 0.46376646\n",
      "epoch: 1 step: 1359, loss is 0.4310743\n",
      "epoch: 1 step: 1360, loss is 0.7063513\n",
      "epoch: 1 step: 1361, loss is 0.8610723\n",
      "epoch: 1 step: 1362, loss is 0.49731025\n",
      "epoch: 1 step: 1363, loss is 0.7289326\n",
      "epoch: 1 step: 1364, loss is 0.6062721\n",
      "epoch: 1 step: 1365, loss is 0.37766355\n",
      "epoch: 1 step: 1366, loss is 0.4740498\n",
      "epoch: 1 step: 1367, loss is 0.51804316\n",
      "epoch: 1 step: 1368, loss is 0.12819642\n",
      "epoch: 1 step: 1369, loss is 0.29612786\n",
      "epoch: 1 step: 1370, loss is 0.44212487\n",
      "epoch: 1 step: 1371, loss is 0.576422\n",
      "epoch: 1 step: 1372, loss is 0.20018993\n",
      "epoch: 1 step: 1373, loss is 0.29887843\n",
      "epoch: 1 step: 1374, loss is 0.44047728\n",
      "epoch: 1 step: 1375, loss is 0.19355099\n",
      "epoch: 1 step: 1376, loss is 0.6134036\n",
      "epoch: 1 step: 1377, loss is 0.51351774\n",
      "epoch: 1 step: 1378, loss is 0.20854448\n",
      "epoch: 1 step: 1379, loss is 0.1344388\n",
      "epoch: 1 step: 1380, loss is 0.35840496\n",
      "epoch: 1 step: 1381, loss is 0.2540713\n",
      "epoch: 1 step: 1382, loss is 0.28166437\n",
      "epoch: 1 step: 1383, loss is 0.4604005\n",
      "epoch: 1 step: 1384, loss is 0.22906445\n",
      "epoch: 1 step: 1385, loss is 0.27815753\n",
      "epoch: 1 step: 1386, loss is 0.13056524\n",
      "epoch: 1 step: 1387, loss is 0.5211326\n",
      "epoch: 1 step: 1388, loss is 0.12234116\n",
      "epoch: 1 step: 1389, loss is 0.24042386\n",
      "epoch: 1 step: 1390, loss is 0.43135634\n",
      "epoch: 1 step: 1391, loss is 0.7880361\n",
      "epoch: 1 step: 1392, loss is 0.2242805\n",
      "epoch: 1 step: 1393, loss is 0.19019873\n",
      "epoch: 1 step: 1394, loss is 0.39199772\n",
      "epoch: 1 step: 1395, loss is 0.5131417\n",
      "epoch: 1 step: 1396, loss is 0.30536133\n",
      "epoch: 1 step: 1397, loss is 0.5590155\n",
      "epoch: 1 step: 1398, loss is 0.3737591\n",
      "epoch: 1 step: 1399, loss is 0.1807211\n",
      "epoch: 1 step: 1400, loss is 0.33587077\n",
      "epoch: 1 step: 1401, loss is 0.36884242\n",
      "epoch: 1 step: 1402, loss is 0.19782442\n",
      "epoch: 1 step: 1403, loss is 0.24156997\n",
      "epoch: 1 step: 1404, loss is 0.43675077\n",
      "epoch: 1 step: 1405, loss is 0.44478014\n",
      "epoch: 1 step: 1406, loss is 0.5151246\n",
      "epoch: 1 step: 1407, loss is 0.10427771\n",
      "epoch: 1 step: 1408, loss is 0.22531903\n",
      "epoch: 1 step: 1409, loss is 0.30538616\n",
      "epoch: 1 step: 1410, loss is 0.15340847\n",
      "epoch: 1 step: 1411, loss is 0.23944172\n",
      "epoch: 1 step: 1412, loss is 0.11887772\n",
      "epoch: 1 step: 1413, loss is 0.17569959\n",
      "epoch: 1 step: 1414, loss is 0.24977884\n",
      "epoch: 1 step: 1415, loss is 0.43137887\n",
      "epoch: 1 step: 1416, loss is 0.47902307\n",
      "epoch: 1 step: 1417, loss is 0.19596195\n",
      "epoch: 1 step: 1418, loss is 0.44997323\n",
      "epoch: 1 step: 1419, loss is 0.51166296\n",
      "epoch: 1 step: 1420, loss is 0.064197734\n",
      "epoch: 1 step: 1421, loss is 0.21727097\n",
      "epoch: 1 step: 1422, loss is 0.2449131\n",
      "epoch: 1 step: 1423, loss is 0.10780907\n",
      "epoch: 1 step: 1424, loss is 0.05666284\n",
      "epoch: 1 step: 1425, loss is 0.1707815\n",
      "epoch: 1 step: 1426, loss is 0.37280717\n",
      "epoch: 1 step: 1427, loss is 0.38143864\n",
      "epoch: 1 step: 1428, loss is 0.65608525\n",
      "epoch: 1 step: 1429, loss is 0.22432211\n",
      "epoch: 1 step: 1430, loss is 0.19523664\n",
      "epoch: 1 step: 1431, loss is 0.21319132\n",
      "epoch: 1 step: 1432, loss is 0.360333\n",
      "epoch: 1 step: 1433, loss is 0.24010727\n",
      "epoch: 1 step: 1434, loss is 0.19469966\n",
      "epoch: 1 step: 1435, loss is 0.14112549\n",
      "epoch: 1 step: 1436, loss is 0.4955961\n",
      "epoch: 1 step: 1437, loss is 0.16730316\n",
      "epoch: 1 step: 1438, loss is 0.40319303\n",
      "epoch: 1 step: 1439, loss is 0.12867506\n",
      "epoch: 1 step: 1440, loss is 0.29089665\n",
      "epoch: 1 step: 1441, loss is 0.15357435\n",
      "epoch: 1 step: 1442, loss is 0.09998944\n",
      "epoch: 1 step: 1443, loss is 0.06484097\n",
      "epoch: 1 step: 1444, loss is 0.25993454\n",
      "epoch: 1 step: 1445, loss is 0.29260904\n",
      "epoch: 1 step: 1446, loss is 0.1657858\n",
      "epoch: 1 step: 1447, loss is 0.10682638\n",
      "epoch: 1 step: 1448, loss is 0.29340845\n",
      "epoch: 1 step: 1449, loss is 0.08353969\n",
      "epoch: 1 step: 1450, loss is 0.1964581\n",
      "epoch: 1 step: 1451, loss is 0.18681161\n",
      "epoch: 1 step: 1452, loss is 0.29960385\n",
      "epoch: 1 step: 1453, loss is 0.4334611\n",
      "epoch: 1 step: 1454, loss is 0.12793465\n",
      "epoch: 1 step: 1455, loss is 0.075575285\n",
      "epoch: 1 step: 1456, loss is 0.119773276\n",
      "epoch: 1 step: 1457, loss is 0.053911958\n",
      "epoch: 1 step: 1458, loss is 0.14300245\n",
      "epoch: 1 step: 1459, loss is 0.08224532\n",
      "epoch: 1 step: 1460, loss is 0.052579943\n",
      "epoch: 1 step: 1461, loss is 0.38776803\n",
      "epoch: 1 step: 1462, loss is 0.32866412\n",
      "epoch: 1 step: 1463, loss is 0.5006408\n",
      "epoch: 1 step: 1464, loss is 0.21635742\n",
      "epoch: 1 step: 1465, loss is 0.28449252\n",
      "epoch: 1 step: 1466, loss is 0.32964128\n",
      "epoch: 1 step: 1467, loss is 0.09623784\n",
      "epoch: 1 step: 1468, loss is 0.5289073\n",
      "epoch: 1 step: 1469, loss is 0.017474841\n",
      "epoch: 1 step: 1470, loss is 0.15473385\n",
      "epoch: 1 step: 1471, loss is 0.37643883\n",
      "epoch: 1 step: 1472, loss is 0.13445865\n",
      "epoch: 1 step: 1473, loss is 0.1033312\n",
      "epoch: 1 step: 1474, loss is 0.23978633\n",
      "epoch: 1 step: 1475, loss is 0.068235934\n",
      "epoch: 1 step: 1476, loss is 0.23534772\n",
      "epoch: 1 step: 1477, loss is 0.2588498\n",
      "epoch: 1 step: 1478, loss is 0.1898528\n",
      "epoch: 1 step: 1479, loss is 0.33108893\n",
      "epoch: 1 step: 1480, loss is 0.19769356\n",
      "epoch: 1 step: 1481, loss is 0.13621987\n",
      "epoch: 1 step: 1482, loss is 0.083511345\n",
      "epoch: 1 step: 1483, loss is 0.11527356\n",
      "epoch: 1 step: 1484, loss is 0.31857944\n",
      "epoch: 1 step: 1485, loss is 0.06797314\n",
      "epoch: 1 step: 1486, loss is 0.16177167\n",
      "epoch: 1 step: 1487, loss is 0.19346674\n",
      "epoch: 1 step: 1488, loss is 0.20517072\n",
      "epoch: 1 step: 1489, loss is 0.13677403\n",
      "epoch: 1 step: 1490, loss is 0.42116326\n",
      "epoch: 1 step: 1491, loss is 0.2954758\n",
      "epoch: 1 step: 1492, loss is 0.25815696\n",
      "epoch: 1 step: 1493, loss is 0.04527365\n",
      "epoch: 1 step: 1494, loss is 0.12816747\n",
      "epoch: 1 step: 1495, loss is 0.41732106\n",
      "epoch: 1 step: 1496, loss is 0.33100504\n",
      "epoch: 1 step: 1497, loss is 0.056979485\n",
      "epoch: 1 step: 1498, loss is 0.31269103\n",
      "epoch: 1 step: 1499, loss is 0.20611747\n",
      "epoch: 1 step: 1500, loss is 0.06438655\n",
      "epoch: 1 step: 1501, loss is 0.06180172\n",
      "epoch: 1 step: 1502, loss is 0.13127123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1503, loss is 0.04577511\n",
      "epoch: 1 step: 1504, loss is 0.4675565\n",
      "epoch: 1 step: 1505, loss is 0.24827078\n",
      "epoch: 1 step: 1506, loss is 0.08002991\n",
      "epoch: 1 step: 1507, loss is 0.13118\n",
      "epoch: 1 step: 1508, loss is 0.18158872\n",
      "epoch: 1 step: 1509, loss is 0.26966745\n",
      "epoch: 1 step: 1510, loss is 0.038586423\n",
      "epoch: 1 step: 1511, loss is 0.10896028\n",
      "epoch: 1 step: 1512, loss is 0.3801732\n",
      "epoch: 1 step: 1513, loss is 0.20102066\n",
      "epoch: 1 step: 1514, loss is 0.16724867\n",
      "epoch: 1 step: 1515, loss is 0.36633563\n",
      "epoch: 1 step: 1516, loss is 0.51239884\n",
      "epoch: 1 step: 1517, loss is 0.18913437\n",
      "epoch: 1 step: 1518, loss is 0.26655206\n",
      "epoch: 1 step: 1519, loss is 0.17314501\n",
      "epoch: 1 step: 1520, loss is 0.33429506\n",
      "epoch: 1 step: 1521, loss is 0.14127572\n",
      "epoch: 1 step: 1522, loss is 0.36285126\n",
      "epoch: 1 step: 1523, loss is 0.22072968\n",
      "epoch: 1 step: 1524, loss is 0.25411192\n",
      "epoch: 1 step: 1525, loss is 0.48943597\n",
      "epoch: 1 step: 1526, loss is 0.20597874\n",
      "epoch: 1 step: 1527, loss is 0.3810883\n",
      "epoch: 1 step: 1528, loss is 0.40390834\n",
      "epoch: 1 step: 1529, loss is 0.2877275\n",
      "epoch: 1 step: 1530, loss is 0.269565\n",
      "epoch: 1 step: 1531, loss is 0.3014305\n",
      "epoch: 1 step: 1532, loss is 0.12793964\n",
      "epoch: 1 step: 1533, loss is 0.30016917\n",
      "epoch: 1 step: 1534, loss is 0.067349605\n",
      "epoch: 1 step: 1535, loss is 0.5389963\n",
      "epoch: 1 step: 1536, loss is 0.14297734\n",
      "epoch: 1 step: 1537, loss is 0.1288642\n",
      "epoch: 1 step: 1538, loss is 0.11302553\n",
      "epoch: 1 step: 1539, loss is 0.114408754\n",
      "epoch: 1 step: 1540, loss is 0.0749154\n",
      "epoch: 1 step: 1541, loss is 0.030292245\n",
      "epoch: 1 step: 1542, loss is 0.24959269\n",
      "epoch: 1 step: 1543, loss is 0.1354291\n",
      "epoch: 1 step: 1544, loss is 0.18541433\n",
      "epoch: 1 step: 1545, loss is 0.097700424\n",
      "epoch: 1 step: 1546, loss is 0.24369\n",
      "epoch: 1 step: 1547, loss is 0.34315032\n",
      "epoch: 1 step: 1548, loss is 0.062037244\n",
      "epoch: 1 step: 1549, loss is 0.17833787\n",
      "epoch: 1 step: 1550, loss is 0.48153397\n",
      "epoch: 1 step: 1551, loss is 0.41197988\n",
      "epoch: 1 step: 1552, loss is 0.2761368\n",
      "epoch: 1 step: 1553, loss is 0.24234627\n",
      "epoch: 1 step: 1554, loss is 0.324846\n",
      "epoch: 1 step: 1555, loss is 0.26960096\n",
      "epoch: 1 step: 1556, loss is 0.12899952\n",
      "epoch: 1 step: 1557, loss is 0.092449926\n",
      "epoch: 1 step: 1558, loss is 0.40091854\n",
      "epoch: 1 step: 1559, loss is 0.3085892\n",
      "epoch: 1 step: 1560, loss is 0.14102782\n",
      "epoch: 1 step: 1561, loss is 0.08759564\n",
      "epoch: 1 step: 1562, loss is 0.09147245\n",
      "epoch: 1 step: 1563, loss is 0.06427479\n",
      "epoch: 1 step: 1564, loss is 0.5039148\n",
      "epoch: 1 step: 1565, loss is 0.15978931\n",
      "epoch: 1 step: 1566, loss is 0.10647786\n",
      "epoch: 1 step: 1567, loss is 0.2817019\n",
      "epoch: 1 step: 1568, loss is 0.30854285\n",
      "epoch: 1 step: 1569, loss is 0.24998066\n",
      "epoch: 1 step: 1570, loss is 0.19044474\n",
      "epoch: 1 step: 1571, loss is 0.1710649\n",
      "epoch: 1 step: 1572, loss is 0.09898801\n",
      "epoch: 1 step: 1573, loss is 0.08231537\n",
      "epoch: 1 step: 1574, loss is 0.388299\n",
      "epoch: 1 step: 1575, loss is 0.122496806\n",
      "epoch: 1 step: 1576, loss is 0.16981676\n",
      "epoch: 1 step: 1577, loss is 0.3200408\n",
      "epoch: 1 step: 1578, loss is 0.2675657\n",
      "epoch: 1 step: 1579, loss is 0.6539628\n",
      "epoch: 1 step: 1580, loss is 0.21191218\n",
      "epoch: 1 step: 1581, loss is 0.15737079\n",
      "epoch: 1 step: 1582, loss is 0.30108866\n",
      "epoch: 1 step: 1583, loss is 0.14389344\n",
      "epoch: 1 step: 1584, loss is 0.43464598\n",
      "epoch: 1 step: 1585, loss is 0.19495465\n",
      "epoch: 1 step: 1586, loss is 0.14185704\n",
      "epoch: 1 step: 1587, loss is 0.12920947\n",
      "epoch: 1 step: 1588, loss is 0.15353031\n",
      "epoch: 1 step: 1589, loss is 0.27388328\n",
      "epoch: 1 step: 1590, loss is 0.1330608\n",
      "epoch: 1 step: 1591, loss is 0.26984835\n",
      "epoch: 1 step: 1592, loss is 0.14002901\n",
      "epoch: 1 step: 1593, loss is 0.22289205\n",
      "epoch: 1 step: 1594, loss is 0.39964187\n",
      "epoch: 1 step: 1595, loss is 0.23210682\n",
      "epoch: 1 step: 1596, loss is 0.22925514\n",
      "epoch: 1 step: 1597, loss is 0.13678373\n",
      "epoch: 1 step: 1598, loss is 0.1634599\n",
      "epoch: 1 step: 1599, loss is 0.1322659\n",
      "epoch: 1 step: 1600, loss is 0.104999125\n",
      "epoch: 1 step: 1601, loss is 0.5468647\n",
      "epoch: 1 step: 1602, loss is 0.43313628\n",
      "epoch: 1 step: 1603, loss is 0.2878808\n",
      "epoch: 1 step: 1604, loss is 0.22821735\n",
      "epoch: 1 step: 1605, loss is 0.13719033\n",
      "epoch: 1 step: 1606, loss is 0.25321352\n",
      "epoch: 1 step: 1607, loss is 0.1856558\n",
      "epoch: 1 step: 1608, loss is 0.23247322\n",
      "epoch: 1 step: 1609, loss is 0.23018493\n",
      "epoch: 1 step: 1610, loss is 0.3750836\n",
      "epoch: 1 step: 1611, loss is 0.3122783\n",
      "epoch: 1 step: 1612, loss is 0.2912089\n",
      "epoch: 1 step: 1613, loss is 0.23708074\n",
      "epoch: 1 step: 1614, loss is 0.16373949\n",
      "epoch: 1 step: 1615, loss is 0.2615692\n",
      "epoch: 1 step: 1616, loss is 0.14745997\n",
      "epoch: 1 step: 1617, loss is 0.09701515\n",
      "epoch: 1 step: 1618, loss is 0.18509866\n",
      "epoch: 1 step: 1619, loss is 0.23378247\n",
      "epoch: 1 step: 1620, loss is 0.03367675\n",
      "epoch: 1 step: 1621, loss is 0.17404771\n",
      "epoch: 1 step: 1622, loss is 0.39506587\n",
      "epoch: 1 step: 1623, loss is 0.04661528\n",
      "epoch: 1 step: 1624, loss is 0.15885396\n",
      "epoch: 1 step: 1625, loss is 0.109122336\n",
      "epoch: 1 step: 1626, loss is 0.31188905\n",
      "epoch: 1 step: 1627, loss is 0.029945353\n",
      "epoch: 1 step: 1628, loss is 0.093307845\n",
      "epoch: 1 step: 1629, loss is 0.22067621\n",
      "epoch: 1 step: 1630, loss is 0.062102035\n",
      "epoch: 1 step: 1631, loss is 0.2606829\n",
      "epoch: 1 step: 1632, loss is 0.06497831\n",
      "epoch: 1 step: 1633, loss is 0.2855936\n",
      "epoch: 1 step: 1634, loss is 0.17837201\n",
      "epoch: 1 step: 1635, loss is 0.16509637\n",
      "epoch: 1 step: 1636, loss is 0.36514106\n",
      "epoch: 1 step: 1637, loss is 0.3875508\n",
      "epoch: 1 step: 1638, loss is 0.09298558\n",
      "epoch: 1 step: 1639, loss is 0.16044879\n",
      "epoch: 1 step: 1640, loss is 0.19543204\n",
      "epoch: 1 step: 1641, loss is 0.12776196\n",
      "epoch: 1 step: 1642, loss is 0.2402749\n",
      "epoch: 1 step: 1643, loss is 0.13886988\n",
      "epoch: 1 step: 1644, loss is 0.11535056\n",
      "epoch: 1 step: 1645, loss is 0.056054603\n",
      "epoch: 1 step: 1646, loss is 0.293899\n",
      "epoch: 1 step: 1647, loss is 0.23287597\n",
      "epoch: 1 step: 1648, loss is 0.07946185\n",
      "epoch: 1 step: 1649, loss is 0.12242648\n",
      "epoch: 1 step: 1650, loss is 0.119485416\n",
      "epoch: 1 step: 1651, loss is 0.44765592\n",
      "epoch: 1 step: 1652, loss is 0.21382731\n",
      "epoch: 1 step: 1653, loss is 0.2916286\n",
      "epoch: 1 step: 1654, loss is 0.29354024\n",
      "epoch: 1 step: 1655, loss is 0.14735731\n",
      "epoch: 1 step: 1656, loss is 0.35932982\n",
      "epoch: 1 step: 1657, loss is 0.043313112\n",
      "epoch: 1 step: 1658, loss is 0.22114164\n",
      "epoch: 1 step: 1659, loss is 0.04459478\n",
      "epoch: 1 step: 1660, loss is 0.2944465\n",
      "epoch: 1 step: 1661, loss is 0.34763706\n",
      "epoch: 1 step: 1662, loss is 0.07291488\n",
      "epoch: 1 step: 1663, loss is 0.27387527\n",
      "epoch: 1 step: 1664, loss is 0.23144121\n",
      "epoch: 1 step: 1665, loss is 0.06958929\n",
      "epoch: 1 step: 1666, loss is 0.2177758\n",
      "epoch: 1 step: 1667, loss is 0.34933615\n",
      "epoch: 1 step: 1668, loss is 0.09359562\n",
      "epoch: 1 step: 1669, loss is 0.2750663\n",
      "epoch: 1 step: 1670, loss is 0.07420371\n",
      "epoch: 1 step: 1671, loss is 0.20527267\n",
      "epoch: 1 step: 1672, loss is 0.28491202\n",
      "epoch: 1 step: 1673, loss is 0.68460625\n",
      "epoch: 1 step: 1674, loss is 0.26978707\n",
      "epoch: 1 step: 1675, loss is 0.22899342\n",
      "epoch: 1 step: 1676, loss is 0.026820855\n",
      "epoch: 1 step: 1677, loss is 0.12852654\n",
      "epoch: 1 step: 1678, loss is 0.23211737\n",
      "epoch: 1 step: 1679, loss is 0.11690854\n",
      "epoch: 1 step: 1680, loss is 0.18029156\n",
      "epoch: 1 step: 1681, loss is 0.23618512\n",
      "epoch: 1 step: 1682, loss is 0.14908944\n",
      "epoch: 1 step: 1683, loss is 0.07812002\n",
      "epoch: 1 step: 1684, loss is 0.19435687\n",
      "epoch: 1 step: 1685, loss is 0.11953058\n",
      "epoch: 1 step: 1686, loss is 0.63247466\n",
      "epoch: 1 step: 1687, loss is 0.24988747\n",
      "epoch: 1 step: 1688, loss is 0.108251795\n",
      "epoch: 1 step: 1689, loss is 0.051821668\n",
      "epoch: 1 step: 1690, loss is 0.057108063\n",
      "epoch: 1 step: 1691, loss is 0.17946923\n",
      "epoch: 1 step: 1692, loss is 0.2916095\n",
      "epoch: 1 step: 1693, loss is 0.1576152\n",
      "epoch: 1 step: 1694, loss is 0.22199051\n",
      "epoch: 1 step: 1695, loss is 0.026768912\n",
      "epoch: 1 step: 1696, loss is 0.2682308\n",
      "epoch: 1 step: 1697, loss is 0.09181115\n",
      "epoch: 1 step: 1698, loss is 0.28276002\n",
      "epoch: 1 step: 1699, loss is 0.27457276\n",
      "epoch: 1 step: 1700, loss is 0.010981338\n",
      "epoch: 1 step: 1701, loss is 0.2686455\n",
      "epoch: 1 step: 1702, loss is 0.04209011\n",
      "epoch: 1 step: 1703, loss is 0.058048636\n",
      "epoch: 1 step: 1704, loss is 0.053389583\n",
      "epoch: 1 step: 1705, loss is 0.18657458\n",
      "epoch: 1 step: 1706, loss is 0.13259235\n",
      "epoch: 1 step: 1707, loss is 0.041194245\n",
      "epoch: 1 step: 1708, loss is 0.12595029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1709, loss is 0.18996589\n",
      "epoch: 1 step: 1710, loss is 0.09848397\n",
      "epoch: 1 step: 1711, loss is 0.2514619\n",
      "epoch: 1 step: 1712, loss is 0.11584256\n",
      "epoch: 1 step: 1713, loss is 0.4023314\n",
      "epoch: 1 step: 1714, loss is 0.03728634\n",
      "epoch: 1 step: 1715, loss is 0.03335476\n",
      "epoch: 1 step: 1716, loss is 0.18455644\n",
      "epoch: 1 step: 1717, loss is 0.18693241\n",
      "epoch: 1 step: 1718, loss is 0.31938165\n",
      "epoch: 1 step: 1719, loss is 0.06800351\n",
      "epoch: 1 step: 1720, loss is 0.1716736\n",
      "epoch: 1 step: 1721, loss is 0.0856242\n",
      "epoch: 1 step: 1722, loss is 0.10138575\n",
      "epoch: 1 step: 1723, loss is 0.17525963\n",
      "epoch: 1 step: 1724, loss is 0.37821215\n",
      "epoch: 1 step: 1725, loss is 0.44712958\n",
      "epoch: 1 step: 1726, loss is 0.1273179\n",
      "epoch: 1 step: 1727, loss is 0.4621757\n",
      "epoch: 1 step: 1728, loss is 0.31960788\n",
      "epoch: 1 step: 1729, loss is 0.059619106\n",
      "epoch: 1 step: 1730, loss is 0.13064215\n",
      "epoch: 1 step: 1731, loss is 0.1883901\n",
      "epoch: 1 step: 1732, loss is 0.52841705\n",
      "epoch: 1 step: 1733, loss is 0.10057476\n",
      "epoch: 1 step: 1734, loss is 0.26564106\n",
      "epoch: 1 step: 1735, loss is 0.25498036\n",
      "epoch: 1 step: 1736, loss is 0.29594973\n",
      "epoch: 1 step: 1737, loss is 0.2111396\n",
      "epoch: 1 step: 1738, loss is 0.09295493\n",
      "epoch: 1 step: 1739, loss is 0.1888599\n",
      "epoch: 1 step: 1740, loss is 0.06221797\n",
      "epoch: 1 step: 1741, loss is 0.14224768\n",
      "epoch: 1 step: 1742, loss is 0.1442137\n",
      "epoch: 1 step: 1743, loss is 0.44379678\n",
      "epoch: 1 step: 1744, loss is 0.20493308\n",
      "epoch: 1 step: 1745, loss is 0.13469692\n",
      "epoch: 1 step: 1746, loss is 0.088177465\n",
      "epoch: 1 step: 1747, loss is 0.20015611\n",
      "epoch: 1 step: 1748, loss is 0.020485725\n",
      "epoch: 1 step: 1749, loss is 0.04806113\n",
      "epoch: 1 step: 1750, loss is 0.2901675\n",
      "epoch: 1 step: 1751, loss is 0.34368828\n",
      "epoch: 1 step: 1752, loss is 0.33180317\n",
      "epoch: 1 step: 1753, loss is 0.033793386\n",
      "epoch: 1 step: 1754, loss is 0.25679824\n",
      "epoch: 1 step: 1755, loss is 0.055515543\n",
      "epoch: 1 step: 1756, loss is 0.037870705\n",
      "epoch: 1 step: 1757, loss is 0.2811432\n",
      "epoch: 1 step: 1758, loss is 0.11216414\n",
      "epoch: 1 step: 1759, loss is 0.10528563\n",
      "epoch: 1 step: 1760, loss is 0.11104162\n",
      "epoch: 1 step: 1761, loss is 0.05261641\n",
      "epoch: 1 step: 1762, loss is 0.26154533\n",
      "epoch: 1 step: 1763, loss is 0.10402546\n",
      "epoch: 1 step: 1764, loss is 0.069335\n",
      "epoch: 1 step: 1765, loss is 0.26100665\n",
      "epoch: 1 step: 1766, loss is 0.13899593\n",
      "epoch: 1 step: 1767, loss is 0.21130992\n",
      "epoch: 1 step: 1768, loss is 0.04973004\n",
      "epoch: 1 step: 1769, loss is 0.11816698\n",
      "epoch: 1 step: 1770, loss is 0.067563236\n",
      "epoch: 1 step: 1771, loss is 0.3304041\n",
      "epoch: 1 step: 1772, loss is 0.1133781\n",
      "epoch: 1 step: 1773, loss is 0.515371\n",
      "epoch: 1 step: 1774, loss is 0.0834554\n",
      "epoch: 1 step: 1775, loss is 0.15351607\n",
      "epoch: 1 step: 1776, loss is 0.63053197\n",
      "epoch: 1 step: 1777, loss is 0.24178836\n",
      "epoch: 1 step: 1778, loss is 0.10978284\n",
      "epoch: 1 step: 1779, loss is 0.332149\n",
      "epoch: 1 step: 1780, loss is 0.28106725\n",
      "epoch: 1 step: 1781, loss is 0.10885139\n",
      "epoch: 1 step: 1782, loss is 0.13460466\n",
      "epoch: 1 step: 1783, loss is 0.23607929\n",
      "epoch: 1 step: 1784, loss is 0.39985245\n",
      "epoch: 1 step: 1785, loss is 0.03076969\n",
      "epoch: 1 step: 1786, loss is 0.17570789\n",
      "epoch: 1 step: 1787, loss is 0.1791144\n",
      "epoch: 1 step: 1788, loss is 0.37320635\n",
      "epoch: 1 step: 1789, loss is 0.064873755\n",
      "epoch: 1 step: 1790, loss is 0.097462445\n",
      "epoch: 1 step: 1791, loss is 0.5037781\n",
      "epoch: 1 step: 1792, loss is 0.07342083\n",
      "epoch: 1 step: 1793, loss is 0.3229907\n",
      "epoch: 1 step: 1794, loss is 0.33633325\n",
      "epoch: 1 step: 1795, loss is 0.24564768\n",
      "epoch: 1 step: 1796, loss is 0.026818287\n",
      "epoch: 1 step: 1797, loss is 0.21693392\n",
      "epoch: 1 step: 1798, loss is 0.034689695\n",
      "epoch: 1 step: 1799, loss is 0.04259551\n",
      "epoch: 1 step: 1800, loss is 0.059759684\n",
      "epoch: 1 step: 1801, loss is 0.09573023\n",
      "epoch: 1 step: 1802, loss is 0.11194683\n",
      "epoch: 1 step: 1803, loss is 0.5117126\n",
      "epoch: 1 step: 1804, loss is 0.12972559\n",
      "epoch: 1 step: 1805, loss is 0.16777109\n",
      "epoch: 1 step: 1806, loss is 0.108013235\n",
      "epoch: 1 step: 1807, loss is 0.09571198\n",
      "epoch: 1 step: 1808, loss is 0.19556642\n",
      "epoch: 1 step: 1809, loss is 0.057153463\n",
      "epoch: 1 step: 1810, loss is 0.39566013\n",
      "epoch: 1 step: 1811, loss is 0.16521055\n",
      "epoch: 1 step: 1812, loss is 0.22669886\n",
      "epoch: 1 step: 1813, loss is 0.09561668\n",
      "epoch: 1 step: 1814, loss is 0.31219855\n",
      "epoch: 1 step: 1815, loss is 0.10368125\n",
      "epoch: 1 step: 1816, loss is 0.027657013\n",
      "epoch: 1 step: 1817, loss is 0.14990596\n",
      "epoch: 1 step: 1818, loss is 0.073287845\n",
      "epoch: 1 step: 1819, loss is 0.09105793\n",
      "epoch: 1 step: 1820, loss is 0.38383123\n",
      "epoch: 1 step: 1821, loss is 0.07369503\n",
      "epoch: 1 step: 1822, loss is 0.1680624\n",
      "epoch: 1 step: 1823, loss is 0.19424164\n",
      "epoch: 1 step: 1824, loss is 0.07480049\n",
      "epoch: 1 step: 1825, loss is 0.31662667\n",
      "epoch: 1 step: 1826, loss is 0.2007893\n",
      "epoch: 1 step: 1827, loss is 0.38720316\n",
      "epoch: 1 step: 1828, loss is 0.35741842\n",
      "epoch: 1 step: 1829, loss is 0.090903625\n",
      "epoch: 1 step: 1830, loss is 0.015565403\n",
      "epoch: 1 step: 1831, loss is 0.15376087\n",
      "epoch: 1 step: 1832, loss is 0.20879877\n",
      "epoch: 1 step: 1833, loss is 0.20284514\n",
      "epoch: 1 step: 1834, loss is 0.19802941\n",
      "epoch: 1 step: 1835, loss is 0.058662605\n",
      "epoch: 1 step: 1836, loss is 0.033934403\n",
      "epoch: 1 step: 1837, loss is 0.16122349\n",
      "epoch: 1 step: 1838, loss is 0.20680855\n",
      "epoch: 1 step: 1839, loss is 0.524882\n",
      "epoch: 1 step: 1840, loss is 0.19154426\n",
      "epoch: 1 step: 1841, loss is 0.28908885\n",
      "epoch: 1 step: 1842, loss is 0.027972134\n",
      "epoch: 1 step: 1843, loss is 0.17860675\n",
      "epoch: 1 step: 1844, loss is 0.10696634\n",
      "epoch: 1 step: 1845, loss is 0.10611375\n",
      "epoch: 1 step: 1846, loss is 0.057919066\n",
      "epoch: 1 step: 1847, loss is 0.26989004\n",
      "epoch: 1 step: 1848, loss is 0.28302336\n",
      "epoch: 1 step: 1849, loss is 0.08169668\n",
      "epoch: 1 step: 1850, loss is 0.044146046\n",
      "epoch: 1 step: 1851, loss is 0.27058443\n",
      "epoch: 1 step: 1852, loss is 0.0430263\n",
      "epoch: 1 step: 1853, loss is 0.238763\n",
      "epoch: 1 step: 1854, loss is 0.07407514\n",
      "epoch: 1 step: 1855, loss is 0.027805606\n",
      "epoch: 1 step: 1856, loss is 0.10535525\n",
      "epoch: 1 step: 1857, loss is 0.4266397\n",
      "epoch: 1 step: 1858, loss is 0.03953247\n",
      "epoch: 1 step: 1859, loss is 0.33166918\n",
      "epoch: 1 step: 1860, loss is 0.20957583\n",
      "epoch: 1 step: 1861, loss is 0.01634515\n",
      "epoch: 1 step: 1862, loss is 0.099369965\n",
      "epoch: 1 step: 1863, loss is 0.12265829\n",
      "epoch: 1 step: 1864, loss is 0.11892902\n",
      "epoch: 1 step: 1865, loss is 0.11459213\n",
      "epoch: 1 step: 1866, loss is 0.033525396\n",
      "epoch: 1 step: 1867, loss is 0.24085051\n",
      "epoch: 1 step: 1868, loss is 0.40663156\n",
      "epoch: 1 step: 1869, loss is 0.245727\n",
      "epoch: 1 step: 1870, loss is 0.04197606\n",
      "epoch: 1 step: 1871, loss is 0.17116773\n",
      "epoch: 1 step: 1872, loss is 0.0673913\n",
      "epoch: 1 step: 1873, loss is 0.19576305\n",
      "epoch: 1 step: 1874, loss is 0.28627965\n",
      "epoch: 1 step: 1875, loss is 0.12474595\n",
      "************************Finished training*************************\n",
      "************************Start evaluation*************************\n",
      "============== Accuracy:{'Accuracy': 0.9621394230769231} ==============\n"
     ]
    }
   ],
   "source": [
    "# check lenet folder exists or not\n",
    "ckpt_folder = '/etc/tinyms/serving/lenet5'\n",
    "ckpt_path = '/etc/tinyms/serving/lenet5/lenet5.ckpt'\n",
    "if not os.path.exists(ckpt_folder):\n",
    "    !mkdir -p  /etc/tinyms/serving/lenet5\n",
    "else:\n",
    "    print('lenet5 ckpt folder already exists')\n",
    "\n",
    "# set environment parameters\n",
    "device_target = \"CPU\"\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=device_target)  \n",
    "dataset_sink_mode = False\n",
    "\n",
    "# define the training and evaluation dataset\n",
    "train_dataset = MnistDataset(os.path.join(mnist_path, \"train\"), shuffle=True)\n",
    "train_dataset = mnist_transform.apply_ds(train_dataset)\n",
    "eval_dataset = MnistDataset(os.path.join(mnist_path, \"test\"), shuffle=True)\n",
    "eval_dataset = mnist_transform.apply_ds(eval_dataset)\n",
    "\n",
    "# parameters for training\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "epoch_size = 1\n",
    "batch_size = 32\n",
    "\n",
    "# define the loss function\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "# define the optimizer\n",
    "net_opt = opt.Momentum(net.trainable_params(), lr, momentum)\n",
    "net_metrics={\"Accuracy\": Accuracy()}\n",
    "model.compile(loss_fn=net_loss, optimizer=net_opt, metrics=net_metrics)\n",
    "\n",
    "print('************************Start training*************************')\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10))\n",
    "model.train(epoch_size, train_dataset, callbacks=[ckpoint_cb, LossMonitor()],dataset_sink_mode=dataset_sink_mode)\n",
    "print('************************Finished training*************************')\n",
    "model.save_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "model.load_checkpoint(ckpt_path)\n",
    "print('************************Start evaluation*************************')\n",
    "acc = model.eval(eval_dataset, dataset_sink_mode=dataset_sink_mode)\n",
    "print(\"============== Accuracy:{} ==============\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-hazard",
   "metadata": {},
   "source": [
    "### 4. Define servable.json\n",
    "\n",
    "Define the lenet5 servable json file for model name, format and number of classes for serving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "colored-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "servable_json = [{'name': 'lenet5', \n",
    "                  'description': 'This servable hosts a lenet5 model predicting numbers', \n",
    "                  'model': {\n",
    "                      \"name\": \"lenet5\", \n",
    "                      \"format\": \"ckpt\", \n",
    "                      \"class_num\": 10}}]\n",
    "os.chdir(\"/etc/tinyms/serving\")\n",
    "json_data = json.dumps(servable_json, indent=4)\n",
    "\n",
    "with open('servable.json', 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-matthew",
   "metadata": {},
   "source": [
    "### 5. Start server\n",
    "\n",
    "#### 5.1 Introduction\n",
    "TinyMS Serving is a C/S(client/server) structure. TinyMS using [Flask](https://flask.palletsprojects.com/en/1.1.x/) which is a micro web framework written in python as the C/S communication tool. In order to serve a model, user must start server first. If successfully started, the server will be run in a subprocess and listening to POST requests from 127.0.0.1 port 5000 sent by client and handle the requests using MindSpore backend which constructs the model, run the prediction and send the result back to the client.\n",
    "\n",
    "#### 5.2 Start server\n",
    "\n",
    "Run the following code block to start the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "convinced-theorem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server starts at host 127.0.0.1, port 5000\n"
     ]
    }
   ],
   "source": [
    "start_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-dutch",
   "metadata": {},
   "source": [
    "### 6. Make predictions\n",
    "\n",
    "#### 6.1 Upload the pic\n",
    "\n",
    "A picture of a single digit number is required to be the input. The picture we use in this tutorial can be found [HERE](https://ascend-tutorials.obs.cn-north-4.myhuaweicloud.com/tinyms-test-pics/numbers/7.png), then save the picture to the root folder, and rename it to `7.png` (or any other name you like).\n",
    "\n",
    "Or run the following code to download the pic for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consistent-particular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-17 11:54:41--  https://ascend-tutorials.obs.cn-north-4.myhuaweicloud.com/tinyms-test-pics/numbers/7.png\n",
      "Resolving ascend-tutorials.obs.cn-north-4.myhuaweicloud.com (ascend-tutorials.obs.cn-north-4.myhuaweicloud.com)... 121.36.121.44, 49.4.112.90, 49.4.112.5, ...\n",
      "Connecting to ascend-tutorials.obs.cn-north-4.myhuaweicloud.com (ascend-tutorials.obs.cn-north-4.myhuaweicloud.com)|121.36.121.44|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34970 (34K) [image/png]\n",
      "Saving to: ‘/root/7.png’\n",
      "\n",
      "7.png               100%[===================>]  34.15K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-03-17 11:54:43 (581 KB/s) - ‘/root/7.png’ saved [34970/34970]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/root/7.png'):\n",
    "    !wget -P /root/ https://ascend-tutorials.obs.cn-north-4.myhuaweicloud.com/tinyms-test-pics/numbers/7.png\n",
    "else:\n",
    "    print('7.png already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-focus",
   "metadata": {},
   "source": [
    "#### 6.2 List servables\n",
    "\n",
    "Use `list_servables` function to check what model is being served right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "recreational-scale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'This servable hosts a lenet5 model predicting numbers',\n",
       "  'model': {'class_num': 10, 'format': 'ckpt', 'name': 'lenet5'},\n",
       "  'name': 'lenet5'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_servables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-consolidation",
   "metadata": {},
   "source": [
    "If the output `description` shows it is a `lenet5` model, then we can continue to next step to send our request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-recording",
   "metadata": {},
   "source": [
    "#### 6.3 Sending request and get the result\n",
    "\n",
    "Run `predict` function to send the request, select between `TOP1_CLASS` and `TOP5_CLASS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "listed-reaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=P size=300x300 at 0x7FEE15338850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP1: 7, score: 0.99962902069091796875\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/root/7.png\"\n",
    "strategy = \"TOP1_CLASS\"\n",
    "\n",
    "# predict(image_path, servable_name, dataset='mnist', strategy='TOP1_CLASS')\n",
    "if server_started() is True:\n",
    "    display(Image.open(image_path).resize((300, 300), Image.ANTIALIAS))\n",
    "    print(predict(image_path, 'lenet5', 'mnist', strategy))\n",
    "else:\n",
    "    print(\"Server not started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-withdrawal",
   "metadata": {},
   "source": [
    "If user can see the output similar to this:  \n",
    "```\n",
    "TOP1: 7, score: 0.99934917688369750977\n",
    "```\n",
    "that means the prediction is successfully performed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-chick",
   "metadata": {},
   "source": [
    "## Shutdown server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hybrid-jacob",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Server shutting down...'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
