{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affecting-convenience",
   "metadata": {},
   "source": [
    "# TinyMS LeNet5 教程\n",
    "\n",
    "在本教程中，我们会演示使用TinyMS API构建LeNet5模型，下载数据集，训练，启动服务器和推理的过程。\n",
    "\n",
    "## 环境要求\n",
    " - Ubuntu: `18.04`\n",
    " - Python: `3.7.x`\n",
    " - Flask: `1.1.2`\n",
    " - MindSpore: `CPU-1.1.1`\n",
    " - TinyMS: `0.1.0`\n",
    " - numpy: `1.17.5`\n",
    " - opencv-python: `4.5.1.48`\n",
    " - Pillow: `8.1.0`\n",
    " - pip: `21.0.1`\n",
    " - requests: `2.18.4`\n",
    " \n",
    "## 介绍\n",
    "\n",
    "TinyMS是一个高级API，目的是让新手用户能够更加轻松地上手深度学习。TinyMS可以有效地减少用户在构建、训练、验证和推理一个模型过程中的操作次数。TinyMS也提供了教程和文档帮助开发者更好的上手和开发。\n",
    "\n",
    "本教程包括6部分，`构建模型`、`下载数据集`、`训练`，`定义servable json`, `启动服务器`和`推理`，其中服务器将在一个子进程中启动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "phantom-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(20419:139790455768896,MainProcess):2021-03-17-16:30:19.184.319 [mindspore/ops/operations/array_ops.py:2302] WARN_DEPRECATED: The usage of Pack is deprecated. Please use Stack.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 'ControlDepend' is deprecated from version 1.1 and will be removed in a future version, use 'Depend' instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tinyms as ts\n",
    "import tinyms.optimizers as opt\n",
    "\n",
    "from PIL import Image\n",
    "from tinyms import context\n",
    "from tinyms.data import MnistDataset, download_dataset\n",
    "from tinyms.vision import mnist_transform\n",
    "from tinyms.model import Model, lenet5\n",
    "from tinyms.serving import start_server, predict, list_servables, shutdown, server_started\n",
    "from tinyms.metrics import Accuracy\n",
    "from tinyms.losses import SoftmaxCrossEntropyWithLogits\n",
    "from tinyms.callbacks import ModelCheckpoint, CheckpointConfig, LossMonitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-medline",
   "metadata": {},
   "source": [
    "### 1. 构建模型\n",
    "\n",
    "TinyMS封装了MindSpore LeNet5模型中的init和construct函数，代码行数能够大大减少，原有的大量代码段行数会被极限压缩:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "matched-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建网络\n",
    "net = lenet5(class_num=10)\n",
    "model = Model(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-activation",
   "metadata": {},
   "source": [
    "### 2. 下载数据集\n",
    "\n",
    "如果根目录下没有创建`mnist`文件夹则MNIST数据集会被自动下载并存放到根目录，如果`mnist`文件夹已经存在于根目录 ，则此步操作会被跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "laden-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************Dataset already exists.**************\n"
     ]
    }
   ],
   "source": [
    "# 下载数据集\n",
    "mnist_path = '/root/mnist'\n",
    "if not os.path.exists(mnist_path):\n",
    "    ts.data.download_dataset('mnist', '/root')\n",
    "    print('************Download complete*************')\n",
    "else:\n",
    "    print('************Dataset already exists.**************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-bradford",
   "metadata": {},
   "source": [
    "### 3. 训练模型\n",
    "\n",
    "数据集中的训练集、验证集都会在此步骤中定义，同时也会定义训练参数。训练后生成的ckpt文件会保存到`/etc/tinyms/serving/lenet5`文件夹以便后续使用，训练完成后会进行验证并输出 `Accuracy`指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bearing-showcase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenet5 ckpt folder already exists\n",
      "************************Start training*************************\n",
      "epoch: 1 step: 1, loss is 2.3025777\n",
      "epoch: 1 step: 2, loss is 2.3023536\n",
      "epoch: 1 step: 3, loss is 2.3025866\n",
      "epoch: 1 step: 4, loss is 2.3026567\n",
      "epoch: 1 step: 5, loss is 2.3029456\n",
      "epoch: 1 step: 6, loss is 2.3029842\n",
      "epoch: 1 step: 7, loss is 2.3041089\n",
      "epoch: 1 step: 8, loss is 2.3036628\n",
      "epoch: 1 step: 9, loss is 2.3030944\n",
      "epoch: 1 step: 10, loss is 2.3029993\n",
      "epoch: 1 step: 11, loss is 2.302108\n",
      "epoch: 1 step: 12, loss is 2.3020008\n",
      "epoch: 1 step: 13, loss is 2.304099\n",
      "epoch: 1 step: 14, loss is 2.301239\n",
      "epoch: 1 step: 15, loss is 2.3052783\n",
      "epoch: 1 step: 16, loss is 2.3051472\n",
      "epoch: 1 step: 17, loss is 2.301609\n",
      "epoch: 1 step: 18, loss is 2.304048\n",
      "epoch: 1 step: 19, loss is 2.3017287\n",
      "epoch: 1 step: 20, loss is 2.3040388\n",
      "epoch: 1 step: 21, loss is 2.3002918\n",
      "epoch: 1 step: 22, loss is 2.2996142\n",
      "epoch: 1 step: 23, loss is 2.3003268\n",
      "epoch: 1 step: 24, loss is 2.3015232\n",
      "epoch: 1 step: 25, loss is 2.2986877\n",
      "epoch: 1 step: 26, loss is 2.3012168\n",
      "epoch: 1 step: 27, loss is 2.302098\n",
      "epoch: 1 step: 28, loss is 2.3040326\n",
      "epoch: 1 step: 29, loss is 2.2996476\n",
      "epoch: 1 step: 30, loss is 2.3001325\n",
      "epoch: 1 step: 31, loss is 2.2969413\n",
      "epoch: 1 step: 32, loss is 2.2983642\n",
      "epoch: 1 step: 33, loss is 2.3047464\n",
      "epoch: 1 step: 34, loss is 2.3035424\n",
      "epoch: 1 step: 35, loss is 2.3021731\n",
      "epoch: 1 step: 36, loss is 2.3037534\n",
      "epoch: 1 step: 37, loss is 2.3004465\n",
      "epoch: 1 step: 38, loss is 2.295411\n",
      "epoch: 1 step: 39, loss is 2.2962604\n",
      "epoch: 1 step: 40, loss is 2.3030088\n",
      "epoch: 1 step: 41, loss is 2.301313\n",
      "epoch: 1 step: 42, loss is 2.303934\n",
      "epoch: 1 step: 43, loss is 2.304152\n",
      "epoch: 1 step: 44, loss is 2.2864833\n",
      "epoch: 1 step: 45, loss is 2.2902598\n",
      "epoch: 1 step: 46, loss is 2.306141\n",
      "epoch: 1 step: 47, loss is 2.2936015\n",
      "epoch: 1 step: 48, loss is 2.2887454\n",
      "epoch: 1 step: 49, loss is 2.2993295\n",
      "epoch: 1 step: 50, loss is 2.2971027\n",
      "epoch: 1 step: 51, loss is 2.2953045\n",
      "epoch: 1 step: 52, loss is 2.2965019\n",
      "epoch: 1 step: 53, loss is 2.2932482\n",
      "epoch: 1 step: 54, loss is 2.2970684\n",
      "epoch: 1 step: 55, loss is 2.294038\n",
      "epoch: 1 step: 56, loss is 2.2990327\n",
      "epoch: 1 step: 57, loss is 2.299712\n",
      "epoch: 1 step: 58, loss is 2.3038454\n",
      "epoch: 1 step: 59, loss is 2.3069117\n",
      "epoch: 1 step: 60, loss is 2.2978706\n",
      "epoch: 1 step: 61, loss is 2.3046072\n",
      "epoch: 1 step: 62, loss is 2.3138485\n",
      "epoch: 1 step: 63, loss is 2.316494\n",
      "epoch: 1 step: 64, loss is 2.3015292\n",
      "epoch: 1 step: 65, loss is 2.3105412\n",
      "epoch: 1 step: 66, loss is 2.304998\n",
      "epoch: 1 step: 67, loss is 2.2820985\n",
      "epoch: 1 step: 68, loss is 2.2979634\n",
      "epoch: 1 step: 69, loss is 2.3158307\n",
      "epoch: 1 step: 70, loss is 2.3105614\n",
      "epoch: 1 step: 71, loss is 2.3042247\n",
      "epoch: 1 step: 72, loss is 2.3093388\n",
      "epoch: 1 step: 73, loss is 2.3057523\n",
      "epoch: 1 step: 74, loss is 2.3108587\n",
      "epoch: 1 step: 75, loss is 2.3016555\n",
      "epoch: 1 step: 76, loss is 2.319075\n",
      "epoch: 1 step: 77, loss is 2.3074062\n",
      "epoch: 1 step: 78, loss is 2.298133\n",
      "epoch: 1 step: 79, loss is 2.2932143\n",
      "epoch: 1 step: 80, loss is 2.3060021\n",
      "epoch: 1 step: 81, loss is 2.319804\n",
      "epoch: 1 step: 82, loss is 2.3013933\n",
      "epoch: 1 step: 83, loss is 2.3070526\n",
      "epoch: 1 step: 84, loss is 2.3117838\n",
      "epoch: 1 step: 85, loss is 2.3125896\n",
      "epoch: 1 step: 86, loss is 2.2985246\n",
      "epoch: 1 step: 87, loss is 2.2902546\n",
      "epoch: 1 step: 88, loss is 2.3071375\n",
      "epoch: 1 step: 89, loss is 2.2933016\n",
      "epoch: 1 step: 90, loss is 2.3087313\n",
      "epoch: 1 step: 91, loss is 2.3117182\n",
      "epoch: 1 step: 92, loss is 2.311144\n",
      "epoch: 1 step: 93, loss is 2.3235095\n",
      "epoch: 1 step: 94, loss is 2.28889\n",
      "epoch: 1 step: 95, loss is 2.3053174\n",
      "epoch: 1 step: 96, loss is 2.3018963\n",
      "epoch: 1 step: 97, loss is 2.305249\n",
      "epoch: 1 step: 98, loss is 2.3056056\n",
      "epoch: 1 step: 99, loss is 2.2971737\n",
      "epoch: 1 step: 100, loss is 2.3022103\n",
      "epoch: 1 step: 101, loss is 2.306956\n",
      "epoch: 1 step: 102, loss is 2.2908053\n",
      "epoch: 1 step: 103, loss is 2.3042505\n",
      "epoch: 1 step: 104, loss is 2.310902\n",
      "epoch: 1 step: 105, loss is 2.3088663\n",
      "epoch: 1 step: 106, loss is 2.2921612\n",
      "epoch: 1 step: 107, loss is 2.3025334\n",
      "epoch: 1 step: 108, loss is 2.3115406\n",
      "epoch: 1 step: 109, loss is 2.307263\n",
      "epoch: 1 step: 110, loss is 2.298709\n",
      "epoch: 1 step: 111, loss is 2.2961903\n",
      "epoch: 1 step: 112, loss is 2.309616\n",
      "epoch: 1 step: 113, loss is 2.313856\n",
      "epoch: 1 step: 114, loss is 2.316779\n",
      "epoch: 1 step: 115, loss is 2.3143358\n",
      "epoch: 1 step: 116, loss is 2.3038878\n",
      "epoch: 1 step: 117, loss is 2.2909327\n",
      "epoch: 1 step: 118, loss is 2.3067489\n",
      "epoch: 1 step: 119, loss is 2.295125\n",
      "epoch: 1 step: 120, loss is 2.27931\n",
      "epoch: 1 step: 121, loss is 2.2984688\n",
      "epoch: 1 step: 122, loss is 2.3092642\n",
      "epoch: 1 step: 123, loss is 2.2927394\n",
      "epoch: 1 step: 124, loss is 2.307096\n",
      "epoch: 1 step: 125, loss is 2.305861\n",
      "epoch: 1 step: 126, loss is 2.3118167\n",
      "epoch: 1 step: 127, loss is 2.305659\n",
      "epoch: 1 step: 128, loss is 2.3023725\n",
      "epoch: 1 step: 129, loss is 2.3186753\n",
      "epoch: 1 step: 130, loss is 2.3096812\n",
      "epoch: 1 step: 131, loss is 2.3016055\n",
      "epoch: 1 step: 132, loss is 2.277419\n",
      "epoch: 1 step: 133, loss is 2.3035889\n",
      "epoch: 1 step: 134, loss is 2.29915\n",
      "epoch: 1 step: 135, loss is 2.3101373\n",
      "epoch: 1 step: 136, loss is 2.3075602\n",
      "epoch: 1 step: 137, loss is 2.3100867\n",
      "epoch: 1 step: 138, loss is 2.3120475\n",
      "epoch: 1 step: 139, loss is 2.300913\n",
      "epoch: 1 step: 140, loss is 2.3208406\n",
      "epoch: 1 step: 141, loss is 2.286677\n",
      "epoch: 1 step: 142, loss is 2.305866\n",
      "epoch: 1 step: 143, loss is 2.3039782\n",
      "epoch: 1 step: 144, loss is 2.3055387\n",
      "epoch: 1 step: 145, loss is 2.29686\n",
      "epoch: 1 step: 146, loss is 2.3170726\n",
      "epoch: 1 step: 147, loss is 2.308333\n",
      "epoch: 1 step: 148, loss is 2.2970064\n",
      "epoch: 1 step: 149, loss is 2.3049307\n",
      "epoch: 1 step: 150, loss is 2.2985425\n",
      "epoch: 1 step: 151, loss is 2.3009138\n",
      "epoch: 1 step: 152, loss is 2.305407\n",
      "epoch: 1 step: 153, loss is 2.315118\n",
      "epoch: 1 step: 154, loss is 2.2955341\n",
      "epoch: 1 step: 155, loss is 2.3087773\n",
      "epoch: 1 step: 156, loss is 2.305591\n",
      "epoch: 1 step: 157, loss is 2.3016481\n",
      "epoch: 1 step: 158, loss is 2.2926302\n",
      "epoch: 1 step: 159, loss is 2.2906299\n",
      "epoch: 1 step: 160, loss is 2.2802582\n",
      "epoch: 1 step: 161, loss is 2.2914386\n",
      "epoch: 1 step: 162, loss is 2.2976618\n",
      "epoch: 1 step: 163, loss is 2.3000793\n",
      "epoch: 1 step: 164, loss is 2.3024464\n",
      "epoch: 1 step: 165, loss is 2.299026\n",
      "epoch: 1 step: 166, loss is 2.3089583\n",
      "epoch: 1 step: 167, loss is 2.29448\n",
      "epoch: 1 step: 168, loss is 2.2973058\n",
      "epoch: 1 step: 169, loss is 2.3046048\n",
      "epoch: 1 step: 170, loss is 2.3146415\n",
      "epoch: 1 step: 171, loss is 2.2969704\n",
      "epoch: 1 step: 172, loss is 2.2994008\n",
      "epoch: 1 step: 173, loss is 2.2859542\n",
      "epoch: 1 step: 174, loss is 2.2947044\n",
      "epoch: 1 step: 175, loss is 2.307949\n",
      "epoch: 1 step: 176, loss is 2.2979946\n",
      "epoch: 1 step: 177, loss is 2.2977266\n",
      "epoch: 1 step: 178, loss is 2.3100991\n",
      "epoch: 1 step: 179, loss is 2.30435\n",
      "epoch: 1 step: 180, loss is 2.3059742\n",
      "epoch: 1 step: 181, loss is 2.3103561\n",
      "epoch: 1 step: 182, loss is 2.3020923\n",
      "epoch: 1 step: 183, loss is 2.2990754\n",
      "epoch: 1 step: 184, loss is 2.276675\n",
      "epoch: 1 step: 185, loss is 2.2981014\n",
      "epoch: 1 step: 186, loss is 2.3082461\n",
      "epoch: 1 step: 187, loss is 2.29707\n",
      "epoch: 1 step: 188, loss is 2.3000772\n",
      "epoch: 1 step: 189, loss is 2.3058252\n",
      "epoch: 1 step: 190, loss is 2.3041775\n",
      "epoch: 1 step: 191, loss is 2.3028116\n",
      "epoch: 1 step: 192, loss is 2.3060672\n",
      "epoch: 1 step: 193, loss is 2.309583\n",
      "epoch: 1 step: 194, loss is 2.313974\n",
      "epoch: 1 step: 195, loss is 2.297438\n",
      "epoch: 1 step: 196, loss is 2.313046\n",
      "epoch: 1 step: 197, loss is 2.3042238\n",
      "epoch: 1 step: 198, loss is 2.2858784\n",
      "epoch: 1 step: 199, loss is 2.308574\n",
      "epoch: 1 step: 200, loss is 2.3131158\n",
      "epoch: 1 step: 201, loss is 2.3127377\n",
      "epoch: 1 step: 202, loss is 2.293942\n",
      "epoch: 1 step: 203, loss is 2.3005652\n",
      "epoch: 1 step: 204, loss is 2.3090816\n",
      "epoch: 1 step: 205, loss is 2.285322\n",
      "epoch: 1 step: 206, loss is 2.306556\n",
      "epoch: 1 step: 207, loss is 2.3037827\n",
      "epoch: 1 step: 208, loss is 2.303156\n",
      "epoch: 1 step: 209, loss is 2.2882388\n",
      "epoch: 1 step: 210, loss is 2.2950292\n",
      "epoch: 1 step: 211, loss is 2.3012886\n",
      "epoch: 1 step: 212, loss is 2.3040147\n",
      "epoch: 1 step: 213, loss is 2.3075962\n",
      "epoch: 1 step: 214, loss is 2.3023088\n",
      "epoch: 1 step: 215, loss is 2.3008966\n",
      "epoch: 1 step: 216, loss is 2.3163142\n",
      "epoch: 1 step: 217, loss is 2.3107097\n",
      "epoch: 1 step: 218, loss is 2.2896872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 219, loss is 2.2939663\n",
      "epoch: 1 step: 220, loss is 2.2928524\n",
      "epoch: 1 step: 221, loss is 2.3152244\n",
      "epoch: 1 step: 222, loss is 2.3061361\n",
      "epoch: 1 step: 223, loss is 2.279769\n",
      "epoch: 1 step: 224, loss is 2.3070345\n",
      "epoch: 1 step: 225, loss is 2.300171\n",
      "epoch: 1 step: 226, loss is 2.310479\n",
      "epoch: 1 step: 227, loss is 2.3122015\n",
      "epoch: 1 step: 228, loss is 2.304152\n",
      "epoch: 1 step: 229, loss is 2.302532\n",
      "epoch: 1 step: 230, loss is 2.2945035\n",
      "epoch: 1 step: 231, loss is 2.3043995\n",
      "epoch: 1 step: 232, loss is 2.3016453\n",
      "epoch: 1 step: 233, loss is 2.2991867\n",
      "epoch: 1 step: 234, loss is 2.3200743\n",
      "epoch: 1 step: 235, loss is 2.3098545\n",
      "epoch: 1 step: 236, loss is 2.2987309\n",
      "epoch: 1 step: 237, loss is 2.3038387\n",
      "epoch: 1 step: 238, loss is 2.3094444\n",
      "epoch: 1 step: 239, loss is 2.300459\n",
      "epoch: 1 step: 240, loss is 2.3098435\n",
      "epoch: 1 step: 241, loss is 2.3092139\n",
      "epoch: 1 step: 242, loss is 2.3189032\n",
      "epoch: 1 step: 243, loss is 2.3043907\n",
      "epoch: 1 step: 244, loss is 2.3151603\n",
      "epoch: 1 step: 245, loss is 2.3170726\n",
      "epoch: 1 step: 246, loss is 2.3100684\n",
      "epoch: 1 step: 247, loss is 2.3049383\n",
      "epoch: 1 step: 248, loss is 2.2949774\n",
      "epoch: 1 step: 249, loss is 2.2913618\n",
      "epoch: 1 step: 250, loss is 2.3027906\n",
      "epoch: 1 step: 251, loss is 2.3005278\n",
      "epoch: 1 step: 252, loss is 2.3027847\n",
      "epoch: 1 step: 253, loss is 2.2999249\n",
      "epoch: 1 step: 254, loss is 2.3043456\n",
      "epoch: 1 step: 255, loss is 2.300815\n",
      "epoch: 1 step: 256, loss is 2.2958884\n",
      "epoch: 1 step: 257, loss is 2.292315\n",
      "epoch: 1 step: 258, loss is 2.3084424\n",
      "epoch: 1 step: 259, loss is 2.3041465\n",
      "epoch: 1 step: 260, loss is 2.2904391\n",
      "epoch: 1 step: 261, loss is 2.3007145\n",
      "epoch: 1 step: 262, loss is 2.3004506\n",
      "epoch: 1 step: 263, loss is 2.28857\n",
      "epoch: 1 step: 264, loss is 2.3111625\n",
      "epoch: 1 step: 265, loss is 2.301449\n",
      "epoch: 1 step: 266, loss is 2.3042426\n",
      "epoch: 1 step: 267, loss is 2.307665\n",
      "epoch: 1 step: 268, loss is 2.28767\n",
      "epoch: 1 step: 269, loss is 2.306003\n",
      "epoch: 1 step: 270, loss is 2.3039308\n",
      "epoch: 1 step: 271, loss is 2.3095183\n",
      "epoch: 1 step: 272, loss is 2.29645\n",
      "epoch: 1 step: 273, loss is 2.308899\n",
      "epoch: 1 step: 274, loss is 2.3055103\n",
      "epoch: 1 step: 275, loss is 2.2945135\n",
      "epoch: 1 step: 276, loss is 2.304501\n",
      "epoch: 1 step: 277, loss is 2.3029115\n",
      "epoch: 1 step: 278, loss is 2.3078852\n",
      "epoch: 1 step: 279, loss is 2.2961133\n",
      "epoch: 1 step: 280, loss is 2.2897162\n",
      "epoch: 1 step: 281, loss is 2.3120458\n",
      "epoch: 1 step: 282, loss is 2.2925832\n",
      "epoch: 1 step: 283, loss is 2.2952917\n",
      "epoch: 1 step: 284, loss is 2.2914953\n",
      "epoch: 1 step: 285, loss is 2.29425\n",
      "epoch: 1 step: 286, loss is 2.3075352\n",
      "epoch: 1 step: 287, loss is 2.2926824\n",
      "epoch: 1 step: 288, loss is 2.3021612\n",
      "epoch: 1 step: 289, loss is 2.3039358\n",
      "epoch: 1 step: 290, loss is 2.3134975\n",
      "epoch: 1 step: 291, loss is 2.3078492\n",
      "epoch: 1 step: 292, loss is 2.2980535\n",
      "epoch: 1 step: 293, loss is 2.3087695\n",
      "epoch: 1 step: 294, loss is 2.3114576\n",
      "epoch: 1 step: 295, loss is 2.3090816\n",
      "epoch: 1 step: 296, loss is 2.3021362\n",
      "epoch: 1 step: 297, loss is 2.3074818\n",
      "epoch: 1 step: 298, loss is 2.294948\n",
      "epoch: 1 step: 299, loss is 2.2988377\n",
      "epoch: 1 step: 300, loss is 2.3206172\n",
      "epoch: 1 step: 301, loss is 2.2901928\n",
      "epoch: 1 step: 302, loss is 2.2991784\n",
      "epoch: 1 step: 303, loss is 2.3172574\n",
      "epoch: 1 step: 304, loss is 2.302924\n",
      "epoch: 1 step: 305, loss is 2.303686\n",
      "epoch: 1 step: 306, loss is 2.299971\n",
      "epoch: 1 step: 307, loss is 2.314704\n",
      "epoch: 1 step: 308, loss is 2.3057897\n",
      "epoch: 1 step: 309, loss is 2.2896705\n",
      "epoch: 1 step: 310, loss is 2.3035862\n",
      "epoch: 1 step: 311, loss is 2.3005025\n",
      "epoch: 1 step: 312, loss is 2.2975674\n",
      "epoch: 1 step: 313, loss is 2.3081143\n",
      "epoch: 1 step: 314, loss is 2.2897081\n",
      "epoch: 1 step: 315, loss is 2.3023639\n",
      "epoch: 1 step: 316, loss is 2.3030188\n",
      "epoch: 1 step: 317, loss is 2.315201\n",
      "epoch: 1 step: 318, loss is 2.3004909\n",
      "epoch: 1 step: 319, loss is 2.3165245\n",
      "epoch: 1 step: 320, loss is 2.3137422\n",
      "epoch: 1 step: 321, loss is 2.299635\n",
      "epoch: 1 step: 322, loss is 2.288423\n",
      "epoch: 1 step: 323, loss is 2.2956316\n",
      "epoch: 1 step: 324, loss is 2.3070283\n",
      "epoch: 1 step: 325, loss is 2.3059359\n",
      "epoch: 1 step: 326, loss is 2.3038557\n",
      "epoch: 1 step: 327, loss is 2.2987807\n",
      "epoch: 1 step: 328, loss is 2.3384001\n",
      "epoch: 1 step: 329, loss is 2.3122172\n",
      "epoch: 1 step: 330, loss is 2.3109195\n",
      "epoch: 1 step: 331, loss is 2.2949624\n",
      "epoch: 1 step: 332, loss is 2.2955806\n",
      "epoch: 1 step: 333, loss is 2.296404\n",
      "epoch: 1 step: 334, loss is 2.303116\n",
      "epoch: 1 step: 335, loss is 2.3105\n",
      "epoch: 1 step: 336, loss is 2.3070652\n",
      "epoch: 1 step: 337, loss is 2.3049607\n",
      "epoch: 1 step: 338, loss is 2.3265643\n",
      "epoch: 1 step: 339, loss is 2.295408\n",
      "epoch: 1 step: 340, loss is 2.3048558\n",
      "epoch: 1 step: 341, loss is 2.3050833\n",
      "epoch: 1 step: 342, loss is 2.309676\n",
      "epoch: 1 step: 343, loss is 2.3122947\n",
      "epoch: 1 step: 344, loss is 2.3048937\n",
      "epoch: 1 step: 345, loss is 2.308335\n",
      "epoch: 1 step: 346, loss is 2.3118176\n",
      "epoch: 1 step: 347, loss is 2.3160152\n",
      "epoch: 1 step: 348, loss is 2.302988\n",
      "epoch: 1 step: 349, loss is 2.3096855\n",
      "epoch: 1 step: 350, loss is 2.3074052\n",
      "epoch: 1 step: 351, loss is 2.298897\n",
      "epoch: 1 step: 352, loss is 2.3088198\n",
      "epoch: 1 step: 353, loss is 2.3014982\n",
      "epoch: 1 step: 354, loss is 2.2995827\n",
      "epoch: 1 step: 355, loss is 2.299207\n",
      "epoch: 1 step: 356, loss is 2.3023777\n",
      "epoch: 1 step: 357, loss is 2.3135784\n",
      "epoch: 1 step: 358, loss is 2.3040583\n",
      "epoch: 1 step: 359, loss is 2.3058867\n",
      "epoch: 1 step: 360, loss is 2.2977643\n",
      "epoch: 1 step: 361, loss is 2.301953\n",
      "epoch: 1 step: 362, loss is 2.3041666\n",
      "epoch: 1 step: 363, loss is 2.304083\n",
      "epoch: 1 step: 364, loss is 2.3041644\n",
      "epoch: 1 step: 365, loss is 2.3041198\n",
      "epoch: 1 step: 366, loss is 2.3029122\n",
      "epoch: 1 step: 367, loss is 2.3023963\n",
      "epoch: 1 step: 368, loss is 2.3056247\n",
      "epoch: 1 step: 369, loss is 2.303238\n",
      "epoch: 1 step: 370, loss is 2.3017461\n",
      "epoch: 1 step: 371, loss is 2.3026617\n",
      "epoch: 1 step: 372, loss is 2.3004055\n",
      "epoch: 1 step: 373, loss is 2.3032386\n",
      "epoch: 1 step: 374, loss is 2.3051543\n",
      "epoch: 1 step: 375, loss is 2.299418\n",
      "epoch: 1 step: 376, loss is 2.3077924\n",
      "epoch: 1 step: 377, loss is 2.2994251\n",
      "epoch: 1 step: 378, loss is 2.3057752\n",
      "epoch: 1 step: 379, loss is 2.3063536\n",
      "epoch: 1 step: 380, loss is 2.2957604\n",
      "epoch: 1 step: 381, loss is 2.3054032\n",
      "epoch: 1 step: 382, loss is 2.3023548\n",
      "epoch: 1 step: 383, loss is 2.3056371\n",
      "epoch: 1 step: 384, loss is 2.304625\n",
      "epoch: 1 step: 385, loss is 2.2984521\n",
      "epoch: 1 step: 386, loss is 2.297698\n",
      "epoch: 1 step: 387, loss is 2.2984688\n",
      "epoch: 1 step: 388, loss is 2.2959552\n",
      "epoch: 1 step: 389, loss is 2.2944791\n",
      "epoch: 1 step: 390, loss is 2.3064976\n",
      "epoch: 1 step: 391, loss is 2.3014483\n",
      "epoch: 1 step: 392, loss is 2.3000877\n",
      "epoch: 1 step: 393, loss is 2.3001373\n",
      "epoch: 1 step: 394, loss is 2.2946665\n",
      "epoch: 1 step: 395, loss is 2.2978954\n",
      "epoch: 1 step: 396, loss is 2.2986336\n",
      "epoch: 1 step: 397, loss is 2.30372\n",
      "epoch: 1 step: 398, loss is 2.3037179\n",
      "epoch: 1 step: 399, loss is 2.3050284\n",
      "epoch: 1 step: 400, loss is 2.302219\n",
      "epoch: 1 step: 401, loss is 2.3043468\n",
      "epoch: 1 step: 402, loss is 2.3053412\n",
      "epoch: 1 step: 403, loss is 2.3016813\n",
      "epoch: 1 step: 404, loss is 2.3093262\n",
      "epoch: 1 step: 405, loss is 2.3035467\n",
      "epoch: 1 step: 406, loss is 2.3001711\n",
      "epoch: 1 step: 407, loss is 2.2968023\n",
      "epoch: 1 step: 408, loss is 2.2934294\n",
      "epoch: 1 step: 409, loss is 2.3071206\n",
      "epoch: 1 step: 410, loss is 2.2995129\n",
      "epoch: 1 step: 411, loss is 2.3045597\n",
      "epoch: 1 step: 412, loss is 2.299839\n",
      "epoch: 1 step: 413, loss is 2.2918646\n",
      "epoch: 1 step: 414, loss is 2.311743\n",
      "epoch: 1 step: 415, loss is 2.29851\n",
      "epoch: 1 step: 416, loss is 2.2975576\n",
      "epoch: 1 step: 417, loss is 2.3037715\n",
      "epoch: 1 step: 418, loss is 2.3039136\n",
      "epoch: 1 step: 419, loss is 2.2966452\n",
      "epoch: 1 step: 420, loss is 2.3028982\n",
      "epoch: 1 step: 421, loss is 2.3019562\n",
      "epoch: 1 step: 422, loss is 2.2997124\n",
      "epoch: 1 step: 423, loss is 2.2929816\n",
      "epoch: 1 step: 424, loss is 2.3031058\n",
      "epoch: 1 step: 425, loss is 2.3064256\n",
      "epoch: 1 step: 426, loss is 2.3058755\n",
      "epoch: 1 step: 427, loss is 2.307268\n",
      "epoch: 1 step: 428, loss is 2.3073032\n",
      "epoch: 1 step: 429, loss is 2.2857544\n",
      "epoch: 1 step: 430, loss is 2.302085\n",
      "epoch: 1 step: 431, loss is 2.286757\n",
      "epoch: 1 step: 432, loss is 2.2958236\n",
      "epoch: 1 step: 433, loss is 2.2919497\n",
      "epoch: 1 step: 434, loss is 2.299336\n",
      "epoch: 1 step: 435, loss is 2.3045895\n",
      "epoch: 1 step: 436, loss is 2.2995403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 437, loss is 2.3037019\n",
      "epoch: 1 step: 438, loss is 2.2810555\n",
      "epoch: 1 step: 439, loss is 2.3046095\n",
      "epoch: 1 step: 440, loss is 2.3064158\n",
      "epoch: 1 step: 441, loss is 2.3039155\n",
      "epoch: 1 step: 442, loss is 2.2979496\n",
      "epoch: 1 step: 443, loss is 2.2950149\n",
      "epoch: 1 step: 444, loss is 2.3039205\n",
      "epoch: 1 step: 445, loss is 2.313383\n",
      "epoch: 1 step: 446, loss is 2.2997854\n",
      "epoch: 1 step: 447, loss is 2.3025367\n",
      "epoch: 1 step: 448, loss is 2.3072577\n",
      "epoch: 1 step: 449, loss is 2.2864583\n",
      "epoch: 1 step: 450, loss is 2.2989352\n",
      "epoch: 1 step: 451, loss is 2.2963326\n",
      "epoch: 1 step: 452, loss is 2.2991695\n",
      "epoch: 1 step: 453, loss is 2.2902248\n",
      "epoch: 1 step: 454, loss is 2.2939851\n",
      "epoch: 1 step: 455, loss is 2.2982528\n",
      "epoch: 1 step: 456, loss is 2.3092663\n",
      "epoch: 1 step: 457, loss is 2.3012593\n",
      "epoch: 1 step: 458, loss is 2.3137474\n",
      "epoch: 1 step: 459, loss is 2.2921197\n",
      "epoch: 1 step: 460, loss is 2.3031266\n",
      "epoch: 1 step: 461, loss is 2.3182535\n",
      "epoch: 1 step: 462, loss is 2.281737\n",
      "epoch: 1 step: 463, loss is 2.297018\n",
      "epoch: 1 step: 464, loss is 2.304447\n",
      "epoch: 1 step: 465, loss is 2.2909663\n",
      "epoch: 1 step: 466, loss is 2.3063776\n",
      "epoch: 1 step: 467, loss is 2.2973776\n",
      "epoch: 1 step: 468, loss is 2.2844567\n",
      "epoch: 1 step: 469, loss is 2.2983832\n",
      "epoch: 1 step: 470, loss is 2.3088114\n",
      "epoch: 1 step: 471, loss is 2.286289\n",
      "epoch: 1 step: 472, loss is 2.3030393\n",
      "epoch: 1 step: 473, loss is 2.2845724\n",
      "epoch: 1 step: 474, loss is 2.3175433\n",
      "epoch: 1 step: 475, loss is 2.3090677\n",
      "epoch: 1 step: 476, loss is 2.310976\n",
      "epoch: 1 step: 477, loss is 2.31286\n",
      "epoch: 1 step: 478, loss is 2.301442\n",
      "epoch: 1 step: 479, loss is 2.2983446\n",
      "epoch: 1 step: 480, loss is 2.3105628\n",
      "epoch: 1 step: 481, loss is 2.2896252\n",
      "epoch: 1 step: 482, loss is 2.2927222\n",
      "epoch: 1 step: 483, loss is 2.3107233\n",
      "epoch: 1 step: 484, loss is 2.3017995\n",
      "epoch: 1 step: 485, loss is 2.2973535\n",
      "epoch: 1 step: 486, loss is 2.3064444\n",
      "epoch: 1 step: 487, loss is 2.2923577\n",
      "epoch: 1 step: 488, loss is 2.2866507\n",
      "epoch: 1 step: 489, loss is 2.2935874\n",
      "epoch: 1 step: 490, loss is 2.2949586\n",
      "epoch: 1 step: 491, loss is 2.301411\n",
      "epoch: 1 step: 492, loss is 2.30794\n",
      "epoch: 1 step: 493, loss is 2.3068573\n",
      "epoch: 1 step: 494, loss is 2.285414\n",
      "epoch: 1 step: 495, loss is 2.3042307\n",
      "epoch: 1 step: 496, loss is 2.293687\n",
      "epoch: 1 step: 497, loss is 2.3121533\n",
      "epoch: 1 step: 498, loss is 2.2961075\n",
      "epoch: 1 step: 499, loss is 2.2980483\n",
      "epoch: 1 step: 500, loss is 2.2927668\n",
      "epoch: 1 step: 501, loss is 2.3106706\n",
      "epoch: 1 step: 502, loss is 2.2971745\n",
      "epoch: 1 step: 503, loss is 2.3042264\n",
      "epoch: 1 step: 504, loss is 2.291675\n",
      "epoch: 1 step: 505, loss is 2.3056967\n",
      "epoch: 1 step: 506, loss is 2.3037593\n",
      "epoch: 1 step: 507, loss is 2.2975652\n",
      "epoch: 1 step: 508, loss is 2.2825708\n",
      "epoch: 1 step: 509, loss is 2.294485\n",
      "epoch: 1 step: 510, loss is 2.3039582\n",
      "epoch: 1 step: 511, loss is 2.2990103\n",
      "epoch: 1 step: 512, loss is 2.2895262\n",
      "epoch: 1 step: 513, loss is 2.2954092\n",
      "epoch: 1 step: 514, loss is 2.3019998\n",
      "epoch: 1 step: 515, loss is 2.310996\n",
      "epoch: 1 step: 516, loss is 2.2936292\n",
      "epoch: 1 step: 517, loss is 2.293864\n",
      "epoch: 1 step: 518, loss is 2.2864537\n",
      "epoch: 1 step: 519, loss is 2.2824092\n",
      "epoch: 1 step: 520, loss is 2.2858613\n",
      "epoch: 1 step: 521, loss is 2.287664\n",
      "epoch: 1 step: 522, loss is 2.2927835\n",
      "epoch: 1 step: 523, loss is 2.2959802\n",
      "epoch: 1 step: 524, loss is 2.295443\n",
      "epoch: 1 step: 525, loss is 2.2962005\n",
      "epoch: 1 step: 526, loss is 2.2835739\n",
      "epoch: 1 step: 527, loss is 2.2779243\n",
      "epoch: 1 step: 528, loss is 2.3023195\n",
      "epoch: 1 step: 529, loss is 2.2898662\n",
      "epoch: 1 step: 530, loss is 2.3129048\n",
      "epoch: 1 step: 531, loss is 2.2754962\n",
      "epoch: 1 step: 532, loss is 2.2849786\n",
      "epoch: 1 step: 533, loss is 2.305452\n",
      "epoch: 1 step: 534, loss is 2.2785735\n",
      "epoch: 1 step: 535, loss is 2.2919214\n",
      "epoch: 1 step: 536, loss is 2.2748961\n",
      "epoch: 1 step: 537, loss is 2.2904456\n",
      "epoch: 1 step: 538, loss is 2.2622948\n",
      "epoch: 1 step: 539, loss is 2.3192163\n",
      "epoch: 1 step: 540, loss is 2.276269\n",
      "epoch: 1 step: 541, loss is 2.2742305\n",
      "epoch: 1 step: 542, loss is 2.28903\n",
      "epoch: 1 step: 543, loss is 2.2902837\n",
      "epoch: 1 step: 544, loss is 2.2754092\n",
      "epoch: 1 step: 545, loss is 2.2985737\n",
      "epoch: 1 step: 546, loss is 2.2824154\n",
      "epoch: 1 step: 547, loss is 2.2934108\n",
      "epoch: 1 step: 548, loss is 2.2687771\n",
      "epoch: 1 step: 549, loss is 2.2570894\n",
      "epoch: 1 step: 550, loss is 2.296007\n",
      "epoch: 1 step: 551, loss is 2.2502906\n",
      "epoch: 1 step: 552, loss is 2.261427\n",
      "epoch: 1 step: 553, loss is 2.2363572\n",
      "epoch: 1 step: 554, loss is 2.2535095\n",
      "epoch: 1 step: 555, loss is 2.233262\n",
      "epoch: 1 step: 556, loss is 2.25543\n",
      "epoch: 1 step: 557, loss is 2.1800566\n",
      "epoch: 1 step: 558, loss is 2.2849631\n",
      "epoch: 1 step: 559, loss is 2.2327855\n",
      "epoch: 1 step: 560, loss is 2.2725902\n",
      "epoch: 1 step: 561, loss is 2.1980217\n",
      "epoch: 1 step: 562, loss is 2.1626506\n",
      "epoch: 1 step: 563, loss is 2.2263672\n",
      "epoch: 1 step: 564, loss is 2.1974854\n",
      "epoch: 1 step: 565, loss is 2.1305287\n",
      "epoch: 1 step: 566, loss is 2.1706536\n",
      "epoch: 1 step: 567, loss is 2.240377\n",
      "epoch: 1 step: 568, loss is 2.124879\n",
      "epoch: 1 step: 569, loss is 2.1372316\n",
      "epoch: 1 step: 570, loss is 2.0342925\n",
      "epoch: 1 step: 571, loss is 2.1225634\n",
      "epoch: 1 step: 572, loss is 2.1749146\n",
      "epoch: 1 step: 573, loss is 1.9436445\n",
      "epoch: 1 step: 574, loss is 1.987088\n",
      "epoch: 1 step: 575, loss is 2.0826316\n",
      "epoch: 1 step: 576, loss is 2.0021124\n",
      "epoch: 1 step: 577, loss is 1.8728462\n",
      "epoch: 1 step: 578, loss is 1.8945693\n",
      "epoch: 1 step: 579, loss is 1.8437761\n",
      "epoch: 1 step: 580, loss is 1.760005\n",
      "epoch: 1 step: 581, loss is 1.7513224\n",
      "epoch: 1 step: 582, loss is 1.8651316\n",
      "epoch: 1 step: 583, loss is 1.486839\n",
      "epoch: 1 step: 584, loss is 1.6488385\n",
      "epoch: 1 step: 585, loss is 1.9198813\n",
      "epoch: 1 step: 586, loss is 1.4915346\n",
      "epoch: 1 step: 587, loss is 1.5871031\n",
      "epoch: 1 step: 588, loss is 1.7131716\n",
      "epoch: 1 step: 589, loss is 1.7728405\n",
      "epoch: 1 step: 590, loss is 1.6968455\n",
      "epoch: 1 step: 591, loss is 1.9730389\n",
      "epoch: 1 step: 592, loss is 1.7651347\n",
      "epoch: 1 step: 593, loss is 1.4536812\n",
      "epoch: 1 step: 594, loss is 1.5517067\n",
      "epoch: 1 step: 595, loss is 1.4698296\n",
      "epoch: 1 step: 596, loss is 1.5814941\n",
      "epoch: 1 step: 597, loss is 1.4208045\n",
      "epoch: 1 step: 598, loss is 1.4235165\n",
      "epoch: 1 step: 599, loss is 1.2541041\n",
      "epoch: 1 step: 600, loss is 1.7211993\n",
      "epoch: 1 step: 601, loss is 1.554903\n",
      "epoch: 1 step: 602, loss is 1.236535\n",
      "epoch: 1 step: 603, loss is 1.325782\n",
      "epoch: 1 step: 604, loss is 1.2360181\n",
      "epoch: 1 step: 605, loss is 1.3914837\n",
      "epoch: 1 step: 606, loss is 1.8409051\n",
      "epoch: 1 step: 607, loss is 1.49041\n",
      "epoch: 1 step: 608, loss is 1.8880188\n",
      "epoch: 1 step: 609, loss is 1.2270085\n",
      "epoch: 1 step: 610, loss is 1.7772974\n",
      "epoch: 1 step: 611, loss is 1.7755667\n",
      "epoch: 1 step: 612, loss is 1.4869051\n",
      "epoch: 1 step: 613, loss is 1.6955366\n",
      "epoch: 1 step: 614, loss is 1.5383924\n",
      "epoch: 1 step: 615, loss is 1.4169885\n",
      "epoch: 1 step: 616, loss is 1.5322284\n",
      "epoch: 1 step: 617, loss is 1.5765584\n",
      "epoch: 1 step: 618, loss is 1.4125448\n",
      "epoch: 1 step: 619, loss is 1.2292573\n",
      "epoch: 1 step: 620, loss is 1.4733444\n",
      "epoch: 1 step: 621, loss is 1.1842532\n",
      "epoch: 1 step: 622, loss is 1.3748337\n",
      "epoch: 1 step: 623, loss is 1.0613437\n",
      "epoch: 1 step: 624, loss is 1.8065594\n",
      "epoch: 1 step: 625, loss is 1.400167\n",
      "epoch: 1 step: 626, loss is 1.1308419\n",
      "epoch: 1 step: 627, loss is 0.89430285\n",
      "epoch: 1 step: 628, loss is 0.90912056\n",
      "epoch: 1 step: 629, loss is 1.039036\n",
      "epoch: 1 step: 630, loss is 1.0114801\n",
      "epoch: 1 step: 631, loss is 1.3359385\n",
      "epoch: 1 step: 632, loss is 1.3090512\n",
      "epoch: 1 step: 633, loss is 0.9204058\n",
      "epoch: 1 step: 634, loss is 1.7042382\n",
      "epoch: 1 step: 635, loss is 1.084518\n",
      "epoch: 1 step: 636, loss is 0.89473957\n",
      "epoch: 1 step: 637, loss is 1.0123662\n",
      "epoch: 1 step: 638, loss is 0.8759681\n",
      "epoch: 1 step: 639, loss is 0.78898627\n",
      "epoch: 1 step: 640, loss is 0.96745425\n",
      "epoch: 1 step: 641, loss is 1.0411485\n",
      "epoch: 1 step: 642, loss is 1.2333581\n",
      "epoch: 1 step: 643, loss is 1.2108265\n",
      "epoch: 1 step: 644, loss is 0.9464791\n",
      "epoch: 1 step: 645, loss is 1.0698395\n",
      "epoch: 1 step: 646, loss is 0.91981566\n",
      "epoch: 1 step: 647, loss is 0.6789798\n",
      "epoch: 1 step: 648, loss is 1.225414\n",
      "epoch: 1 step: 649, loss is 0.7945188\n",
      "epoch: 1 step: 650, loss is 0.79097134\n",
      "epoch: 1 step: 651, loss is 0.5408182\n",
      "epoch: 1 step: 652, loss is 0.77321297\n",
      "epoch: 1 step: 653, loss is 1.4699391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 654, loss is 0.8525744\n",
      "epoch: 1 step: 655, loss is 0.6584808\n",
      "epoch: 1 step: 656, loss is 0.7571933\n",
      "epoch: 1 step: 657, loss is 0.9151435\n",
      "epoch: 1 step: 658, loss is 0.64045644\n",
      "epoch: 1 step: 659, loss is 0.68640834\n",
      "epoch: 1 step: 660, loss is 0.8460708\n",
      "epoch: 1 step: 661, loss is 0.6060891\n",
      "epoch: 1 step: 662, loss is 0.55792475\n",
      "epoch: 1 step: 663, loss is 0.63292235\n",
      "epoch: 1 step: 664, loss is 0.5693629\n",
      "epoch: 1 step: 665, loss is 0.62148523\n",
      "epoch: 1 step: 666, loss is 1.056205\n",
      "epoch: 1 step: 667, loss is 1.0021751\n",
      "epoch: 1 step: 668, loss is 0.41490448\n",
      "epoch: 1 step: 669, loss is 0.7679162\n",
      "epoch: 1 step: 670, loss is 0.76246095\n",
      "epoch: 1 step: 671, loss is 0.84235364\n",
      "epoch: 1 step: 672, loss is 0.787255\n",
      "epoch: 1 step: 673, loss is 0.5855223\n",
      "epoch: 1 step: 674, loss is 0.55998546\n",
      "epoch: 1 step: 675, loss is 0.53138906\n",
      "epoch: 1 step: 676, loss is 0.70548004\n",
      "epoch: 1 step: 677, loss is 0.43360138\n",
      "epoch: 1 step: 678, loss is 0.68392205\n",
      "epoch: 1 step: 679, loss is 0.44937983\n",
      "epoch: 1 step: 680, loss is 0.65978307\n",
      "epoch: 1 step: 681, loss is 0.518846\n",
      "epoch: 1 step: 682, loss is 0.470238\n",
      "epoch: 1 step: 683, loss is 0.5372563\n",
      "epoch: 1 step: 684, loss is 0.39487284\n",
      "epoch: 1 step: 685, loss is 0.774147\n",
      "epoch: 1 step: 686, loss is 0.5124532\n",
      "epoch: 1 step: 687, loss is 0.22846499\n",
      "epoch: 1 step: 688, loss is 0.37373662\n",
      "epoch: 1 step: 689, loss is 0.9442717\n",
      "epoch: 1 step: 690, loss is 0.4056824\n",
      "epoch: 1 step: 691, loss is 0.18757391\n",
      "epoch: 1 step: 692, loss is 0.52923506\n",
      "epoch: 1 step: 693, loss is 0.50214976\n",
      "epoch: 1 step: 694, loss is 0.42055804\n",
      "epoch: 1 step: 695, loss is 0.75039434\n",
      "epoch: 1 step: 696, loss is 0.6962369\n",
      "epoch: 1 step: 697, loss is 0.6254055\n",
      "epoch: 1 step: 698, loss is 0.4446853\n",
      "epoch: 1 step: 699, loss is 0.41806257\n",
      "epoch: 1 step: 700, loss is 0.5673735\n",
      "epoch: 1 step: 701, loss is 0.38857532\n",
      "epoch: 1 step: 702, loss is 0.6099291\n",
      "epoch: 1 step: 703, loss is 0.6763281\n",
      "epoch: 1 step: 704, loss is 0.96534115\n",
      "epoch: 1 step: 705, loss is 0.15722565\n",
      "epoch: 1 step: 706, loss is 0.48979983\n",
      "epoch: 1 step: 707, loss is 0.55260503\n",
      "epoch: 1 step: 708, loss is 0.5697015\n",
      "epoch: 1 step: 709, loss is 0.5107684\n",
      "epoch: 1 step: 710, loss is 0.32080153\n",
      "epoch: 1 step: 711, loss is 0.42896548\n",
      "epoch: 1 step: 712, loss is 0.5138072\n",
      "epoch: 1 step: 713, loss is 0.34913576\n",
      "epoch: 1 step: 714, loss is 0.5311668\n",
      "epoch: 1 step: 715, loss is 0.5017703\n",
      "epoch: 1 step: 716, loss is 0.54165316\n",
      "epoch: 1 step: 717, loss is 0.46109068\n",
      "epoch: 1 step: 718, loss is 0.48081928\n",
      "epoch: 1 step: 719, loss is 0.6604157\n",
      "epoch: 1 step: 720, loss is 0.3558001\n",
      "epoch: 1 step: 721, loss is 0.40338835\n",
      "epoch: 1 step: 722, loss is 0.28278834\n",
      "epoch: 1 step: 723, loss is 0.27863166\n",
      "epoch: 1 step: 724, loss is 0.70884657\n",
      "epoch: 1 step: 725, loss is 0.3888444\n",
      "epoch: 1 step: 726, loss is 0.31100386\n",
      "epoch: 1 step: 727, loss is 0.75061625\n",
      "epoch: 1 step: 728, loss is 0.41315717\n",
      "epoch: 1 step: 729, loss is 0.6411028\n",
      "epoch: 1 step: 730, loss is 0.3447298\n",
      "epoch: 1 step: 731, loss is 0.19943649\n",
      "epoch: 1 step: 732, loss is 0.751188\n",
      "epoch: 1 step: 733, loss is 0.5024434\n",
      "epoch: 1 step: 734, loss is 0.2498039\n",
      "epoch: 1 step: 735, loss is 0.27823865\n",
      "epoch: 1 step: 736, loss is 0.21108006\n",
      "epoch: 1 step: 737, loss is 0.78700125\n",
      "epoch: 1 step: 738, loss is 0.3816507\n",
      "epoch: 1 step: 739, loss is 0.5432632\n",
      "epoch: 1 step: 740, loss is 0.5221502\n",
      "epoch: 1 step: 741, loss is 0.45307273\n",
      "epoch: 1 step: 742, loss is 0.6697862\n",
      "epoch: 1 step: 743, loss is 0.28617844\n",
      "epoch: 1 step: 744, loss is 0.5098031\n",
      "epoch: 1 step: 745, loss is 0.09737957\n",
      "epoch: 1 step: 746, loss is 0.2535792\n",
      "epoch: 1 step: 747, loss is 0.5527522\n",
      "epoch: 1 step: 748, loss is 0.2854712\n",
      "epoch: 1 step: 749, loss is 0.39285368\n",
      "epoch: 1 step: 750, loss is 0.48440212\n",
      "epoch: 1 step: 751, loss is 0.3887898\n",
      "epoch: 1 step: 752, loss is 0.6973347\n",
      "epoch: 1 step: 753, loss is 0.3196931\n",
      "epoch: 1 step: 754, loss is 0.5535801\n",
      "epoch: 1 step: 755, loss is 0.8034488\n",
      "epoch: 1 step: 756, loss is 0.3476255\n",
      "epoch: 1 step: 757, loss is 0.86342746\n",
      "epoch: 1 step: 758, loss is 0.5475422\n",
      "epoch: 1 step: 759, loss is 0.35062706\n",
      "epoch: 1 step: 760, loss is 0.39375445\n",
      "epoch: 1 step: 761, loss is 0.3300565\n",
      "epoch: 1 step: 762, loss is 0.38793087\n",
      "epoch: 1 step: 763, loss is 0.47876412\n",
      "epoch: 1 step: 764, loss is 0.3258079\n",
      "epoch: 1 step: 765, loss is 0.45289448\n",
      "epoch: 1 step: 766, loss is 0.4858533\n",
      "epoch: 1 step: 767, loss is 0.33348763\n",
      "epoch: 1 step: 768, loss is 0.6126089\n",
      "epoch: 1 step: 769, loss is 0.3966344\n",
      "epoch: 1 step: 770, loss is 0.24922356\n",
      "epoch: 1 step: 771, loss is 0.2391935\n",
      "epoch: 1 step: 772, loss is 0.34923902\n",
      "epoch: 1 step: 773, loss is 0.46760005\n",
      "epoch: 1 step: 774, loss is 0.2598846\n",
      "epoch: 1 step: 775, loss is 0.13749781\n",
      "epoch: 1 step: 776, loss is 0.21168692\n",
      "epoch: 1 step: 777, loss is 0.50296676\n",
      "epoch: 1 step: 778, loss is 0.58922803\n",
      "epoch: 1 step: 779, loss is 0.33728772\n",
      "epoch: 1 step: 780, loss is 0.26184893\n",
      "epoch: 1 step: 781, loss is 0.6056508\n",
      "epoch: 1 step: 782, loss is 0.19050896\n",
      "epoch: 1 step: 783, loss is 0.35130173\n",
      "epoch: 1 step: 784, loss is 0.31972963\n",
      "epoch: 1 step: 785, loss is 0.11820244\n",
      "epoch: 1 step: 786, loss is 0.16253947\n",
      "epoch: 1 step: 787, loss is 0.61420417\n",
      "epoch: 1 step: 788, loss is 0.1510855\n",
      "epoch: 1 step: 789, loss is 0.26463372\n",
      "epoch: 1 step: 790, loss is 0.20634863\n",
      "epoch: 1 step: 791, loss is 0.09650604\n",
      "epoch: 1 step: 792, loss is 0.051482216\n",
      "epoch: 1 step: 793, loss is 0.42631587\n",
      "epoch: 1 step: 794, loss is 0.2276571\n",
      "epoch: 1 step: 795, loss is 0.22455202\n",
      "epoch: 1 step: 796, loss is 0.24399996\n",
      "epoch: 1 step: 797, loss is 0.43167815\n",
      "epoch: 1 step: 798, loss is 0.25508675\n",
      "epoch: 1 step: 799, loss is 0.15025766\n",
      "epoch: 1 step: 800, loss is 0.29979956\n",
      "epoch: 1 step: 801, loss is 0.06606274\n",
      "epoch: 1 step: 802, loss is 0.29331836\n",
      "epoch: 1 step: 803, loss is 0.26845768\n",
      "epoch: 1 step: 804, loss is 0.035930652\n",
      "epoch: 1 step: 805, loss is 0.17485678\n",
      "epoch: 1 step: 806, loss is 0.06984688\n",
      "epoch: 1 step: 807, loss is 0.2423698\n",
      "epoch: 1 step: 808, loss is 0.15900351\n",
      "epoch: 1 step: 809, loss is 0.38040912\n",
      "epoch: 1 step: 810, loss is 0.3334202\n",
      "epoch: 1 step: 811, loss is 0.1309945\n",
      "epoch: 1 step: 812, loss is 0.05910575\n",
      "epoch: 1 step: 813, loss is 0.22105464\n",
      "epoch: 1 step: 814, loss is 0.43993247\n",
      "epoch: 1 step: 815, loss is 0.293498\n",
      "epoch: 1 step: 816, loss is 0.17054121\n",
      "epoch: 1 step: 817, loss is 0.635722\n",
      "epoch: 1 step: 818, loss is 0.16608825\n",
      "epoch: 1 step: 819, loss is 0.36090636\n",
      "epoch: 1 step: 820, loss is 0.4646164\n",
      "epoch: 1 step: 821, loss is 0.2753629\n",
      "epoch: 1 step: 822, loss is 0.03979256\n",
      "epoch: 1 step: 823, loss is 0.35090357\n",
      "epoch: 1 step: 824, loss is 0.24335843\n",
      "epoch: 1 step: 825, loss is 0.20644361\n",
      "epoch: 1 step: 826, loss is 0.3023719\n",
      "epoch: 1 step: 827, loss is 0.34499198\n",
      "epoch: 1 step: 828, loss is 0.38100332\n",
      "epoch: 1 step: 829, loss is 0.51705045\n",
      "epoch: 1 step: 830, loss is 0.29646257\n",
      "epoch: 1 step: 831, loss is 0.36916202\n",
      "epoch: 1 step: 832, loss is 0.31276086\n",
      "epoch: 1 step: 833, loss is 0.22410144\n",
      "epoch: 1 step: 834, loss is 0.22578959\n",
      "epoch: 1 step: 835, loss is 0.34319445\n",
      "epoch: 1 step: 836, loss is 0.6651651\n",
      "epoch: 1 step: 837, loss is 0.3762535\n",
      "epoch: 1 step: 838, loss is 0.12987477\n",
      "epoch: 1 step: 839, loss is 0.43633658\n",
      "epoch: 1 step: 840, loss is 0.42391643\n",
      "epoch: 1 step: 841, loss is 0.43051323\n",
      "epoch: 1 step: 842, loss is 0.2482318\n",
      "epoch: 1 step: 843, loss is 0.42737356\n",
      "epoch: 1 step: 844, loss is 0.28303322\n",
      "epoch: 1 step: 845, loss is 0.38375017\n",
      "epoch: 1 step: 846, loss is 0.17311904\n",
      "epoch: 1 step: 847, loss is 0.11641649\n",
      "epoch: 1 step: 848, loss is 0.18326204\n",
      "epoch: 1 step: 849, loss is 0.4096591\n",
      "epoch: 1 step: 850, loss is 0.5347956\n",
      "epoch: 1 step: 851, loss is 0.1486141\n",
      "epoch: 1 step: 852, loss is 0.23619011\n",
      "epoch: 1 step: 853, loss is 0.8430123\n",
      "epoch: 1 step: 854, loss is 0.40619645\n",
      "epoch: 1 step: 855, loss is 0.2941581\n",
      "epoch: 1 step: 856, loss is 0.30685002\n",
      "epoch: 1 step: 857, loss is 0.2550453\n",
      "epoch: 1 step: 858, loss is 0.15390457\n",
      "epoch: 1 step: 859, loss is 0.2106281\n",
      "epoch: 1 step: 860, loss is 0.31543103\n",
      "epoch: 1 step: 861, loss is 0.19751678\n",
      "epoch: 1 step: 862, loss is 0.24177358\n",
      "epoch: 1 step: 863, loss is 0.3034391\n",
      "epoch: 1 step: 864, loss is 0.4336312\n",
      "epoch: 1 step: 865, loss is 0.19880295\n",
      "epoch: 1 step: 866, loss is 0.28246328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 867, loss is 0.13261117\n",
      "epoch: 1 step: 868, loss is 0.33647352\n",
      "epoch: 1 step: 869, loss is 0.15571825\n",
      "epoch: 1 step: 870, loss is 0.11454045\n",
      "epoch: 1 step: 871, loss is 0.16231136\n",
      "epoch: 1 step: 872, loss is 0.14572567\n",
      "epoch: 1 step: 873, loss is 0.15818222\n",
      "epoch: 1 step: 874, loss is 0.19414617\n",
      "epoch: 1 step: 875, loss is 0.057220258\n",
      "epoch: 1 step: 876, loss is 0.5399907\n",
      "epoch: 1 step: 877, loss is 0.50323814\n",
      "epoch: 1 step: 878, loss is 0.16099009\n",
      "epoch: 1 step: 879, loss is 0.15590623\n",
      "epoch: 1 step: 880, loss is 0.36842492\n",
      "epoch: 1 step: 881, loss is 0.4031075\n",
      "epoch: 1 step: 882, loss is 0.28387004\n",
      "epoch: 1 step: 883, loss is 0.36674473\n",
      "epoch: 1 step: 884, loss is 0.27427325\n",
      "epoch: 1 step: 885, loss is 0.29653215\n",
      "epoch: 1 step: 886, loss is 0.405498\n",
      "epoch: 1 step: 887, loss is 0.51243156\n",
      "epoch: 1 step: 888, loss is 0.1636527\n",
      "epoch: 1 step: 889, loss is 0.32325193\n",
      "epoch: 1 step: 890, loss is 0.38569173\n",
      "epoch: 1 step: 891, loss is 0.33114237\n",
      "epoch: 1 step: 892, loss is 0.23036212\n",
      "epoch: 1 step: 893, loss is 0.34302738\n",
      "epoch: 1 step: 894, loss is 0.42341274\n",
      "epoch: 1 step: 895, loss is 0.5721187\n",
      "epoch: 1 step: 896, loss is 0.24268883\n",
      "epoch: 1 step: 897, loss is 0.26567635\n",
      "epoch: 1 step: 898, loss is 0.15431066\n",
      "epoch: 1 step: 899, loss is 0.49642733\n",
      "epoch: 1 step: 900, loss is 0.37852836\n",
      "epoch: 1 step: 901, loss is 0.24281918\n",
      "epoch: 1 step: 902, loss is 0.15340944\n",
      "epoch: 1 step: 903, loss is 0.21479277\n",
      "epoch: 1 step: 904, loss is 0.35741562\n",
      "epoch: 1 step: 905, loss is 0.8168282\n",
      "epoch: 1 step: 906, loss is 0.12152854\n",
      "epoch: 1 step: 907, loss is 0.23672974\n",
      "epoch: 1 step: 908, loss is 0.106953606\n",
      "epoch: 1 step: 909, loss is 0.31295404\n",
      "epoch: 1 step: 910, loss is 0.36725423\n",
      "epoch: 1 step: 911, loss is 0.6316326\n",
      "epoch: 1 step: 912, loss is 0.2730532\n",
      "epoch: 1 step: 913, loss is 0.24552502\n",
      "epoch: 1 step: 914, loss is 0.4535338\n",
      "epoch: 1 step: 915, loss is 0.49312305\n",
      "epoch: 1 step: 916, loss is 0.14472815\n",
      "epoch: 1 step: 917, loss is 0.2350901\n",
      "epoch: 1 step: 918, loss is 0.42439386\n",
      "epoch: 1 step: 919, loss is 0.4145828\n",
      "epoch: 1 step: 920, loss is 0.32853124\n",
      "epoch: 1 step: 921, loss is 0.14529964\n",
      "epoch: 1 step: 922, loss is 0.20663688\n",
      "epoch: 1 step: 923, loss is 0.25973728\n",
      "epoch: 1 step: 924, loss is 0.09983406\n",
      "epoch: 1 step: 925, loss is 0.7727717\n",
      "epoch: 1 step: 926, loss is 0.15835637\n",
      "epoch: 1 step: 927, loss is 0.156995\n",
      "epoch: 1 step: 928, loss is 0.070231445\n",
      "epoch: 1 step: 929, loss is 0.34435204\n",
      "epoch: 1 step: 930, loss is 0.11504002\n",
      "epoch: 1 step: 931, loss is 0.27602383\n",
      "epoch: 1 step: 932, loss is 0.27494603\n",
      "epoch: 1 step: 933, loss is 0.08809048\n",
      "epoch: 1 step: 934, loss is 0.23061073\n",
      "epoch: 1 step: 935, loss is 0.27878955\n",
      "epoch: 1 step: 936, loss is 0.104918405\n",
      "epoch: 1 step: 937, loss is 0.06960229\n",
      "epoch: 1 step: 938, loss is 0.1954393\n",
      "epoch: 1 step: 939, loss is 0.13694651\n",
      "epoch: 1 step: 940, loss is 0.20117114\n",
      "epoch: 1 step: 941, loss is 0.10676757\n",
      "epoch: 1 step: 942, loss is 0.27816278\n",
      "epoch: 1 step: 943, loss is 0.2496934\n",
      "epoch: 1 step: 944, loss is 0.31945655\n",
      "epoch: 1 step: 945, loss is 0.018349124\n",
      "epoch: 1 step: 946, loss is 0.45097843\n",
      "epoch: 1 step: 947, loss is 0.32813275\n",
      "epoch: 1 step: 948, loss is 0.24825905\n",
      "epoch: 1 step: 949, loss is 0.114975736\n",
      "epoch: 1 step: 950, loss is 0.07487761\n",
      "epoch: 1 step: 951, loss is 0.14431831\n",
      "epoch: 1 step: 952, loss is 0.2787541\n",
      "epoch: 1 step: 953, loss is 0.17247796\n",
      "epoch: 1 step: 954, loss is 0.34492528\n",
      "epoch: 1 step: 955, loss is 0.12488407\n",
      "epoch: 1 step: 956, loss is 0.26682663\n",
      "epoch: 1 step: 957, loss is 0.30238432\n",
      "epoch: 1 step: 958, loss is 0.20296614\n",
      "epoch: 1 step: 959, loss is 0.50145775\n",
      "epoch: 1 step: 960, loss is 0.4274895\n",
      "epoch: 1 step: 961, loss is 0.14661118\n",
      "epoch: 1 step: 962, loss is 0.061830297\n",
      "epoch: 1 step: 963, loss is 0.5491759\n",
      "epoch: 1 step: 964, loss is 0.1242258\n",
      "epoch: 1 step: 965, loss is 0.33207682\n",
      "epoch: 1 step: 966, loss is 0.33089525\n",
      "epoch: 1 step: 967, loss is 0.51878697\n",
      "epoch: 1 step: 968, loss is 0.09709303\n",
      "epoch: 1 step: 969, loss is 0.079545364\n",
      "epoch: 1 step: 970, loss is 0.3969571\n",
      "epoch: 1 step: 971, loss is 0.034179594\n",
      "epoch: 1 step: 972, loss is 0.17744036\n",
      "epoch: 1 step: 973, loss is 0.06498264\n",
      "epoch: 1 step: 974, loss is 0.24748239\n",
      "epoch: 1 step: 975, loss is 0.18408279\n",
      "epoch: 1 step: 976, loss is 0.08667181\n",
      "epoch: 1 step: 977, loss is 0.23883715\n",
      "epoch: 1 step: 978, loss is 0.2933791\n",
      "epoch: 1 step: 979, loss is 0.22531742\n",
      "epoch: 1 step: 980, loss is 0.0349311\n",
      "epoch: 1 step: 981, loss is 0.17523998\n",
      "epoch: 1 step: 982, loss is 0.31554395\n",
      "epoch: 1 step: 983, loss is 0.08052339\n",
      "epoch: 1 step: 984, loss is 0.08252954\n",
      "epoch: 1 step: 985, loss is 0.14225243\n",
      "epoch: 1 step: 986, loss is 0.1274084\n",
      "epoch: 1 step: 987, loss is 0.16942145\n",
      "epoch: 1 step: 988, loss is 0.23896368\n",
      "epoch: 1 step: 989, loss is 0.038572136\n",
      "epoch: 1 step: 990, loss is 0.1258058\n",
      "epoch: 1 step: 991, loss is 0.27780485\n",
      "epoch: 1 step: 992, loss is 0.5113864\n",
      "epoch: 1 step: 993, loss is 0.1664241\n",
      "epoch: 1 step: 994, loss is 0.18132572\n",
      "epoch: 1 step: 995, loss is 0.40860337\n",
      "epoch: 1 step: 996, loss is 0.18314971\n",
      "epoch: 1 step: 997, loss is 0.15589061\n",
      "epoch: 1 step: 998, loss is 0.029365035\n",
      "epoch: 1 step: 999, loss is 0.18620704\n",
      "epoch: 1 step: 1000, loss is 0.34180355\n",
      "epoch: 1 step: 1001, loss is 0.053462338\n",
      "epoch: 1 step: 1002, loss is 0.35592875\n",
      "epoch: 1 step: 1003, loss is 0.376888\n",
      "epoch: 1 step: 1004, loss is 0.30225727\n",
      "epoch: 1 step: 1005, loss is 0.16612932\n",
      "epoch: 1 step: 1006, loss is 0.11643228\n",
      "epoch: 1 step: 1007, loss is 0.2593607\n",
      "epoch: 1 step: 1008, loss is 0.16108982\n",
      "epoch: 1 step: 1009, loss is 0.47117695\n",
      "epoch: 1 step: 1010, loss is 0.055423405\n",
      "epoch: 1 step: 1011, loss is 0.1334152\n",
      "epoch: 1 step: 1012, loss is 0.34609047\n",
      "epoch: 1 step: 1013, loss is 0.28962412\n",
      "epoch: 1 step: 1014, loss is 0.18935175\n",
      "epoch: 1 step: 1015, loss is 0.36603206\n",
      "epoch: 1 step: 1016, loss is 0.24747561\n",
      "epoch: 1 step: 1017, loss is 0.35218138\n",
      "epoch: 1 step: 1018, loss is 0.06796711\n",
      "epoch: 1 step: 1019, loss is 0.36160043\n",
      "epoch: 1 step: 1020, loss is 0.110590644\n",
      "epoch: 1 step: 1021, loss is 0.16373557\n",
      "epoch: 1 step: 1022, loss is 0.07337717\n",
      "epoch: 1 step: 1023, loss is 0.25197765\n",
      "epoch: 1 step: 1024, loss is 0.1382446\n",
      "epoch: 1 step: 1025, loss is 0.2308607\n",
      "epoch: 1 step: 1026, loss is 0.40690306\n",
      "epoch: 1 step: 1027, loss is 0.26574266\n",
      "epoch: 1 step: 1028, loss is 0.32272935\n",
      "epoch: 1 step: 1029, loss is 0.36866254\n",
      "epoch: 1 step: 1030, loss is 0.23934\n",
      "epoch: 1 step: 1031, loss is 0.10160569\n",
      "epoch: 1 step: 1032, loss is 0.37527984\n",
      "epoch: 1 step: 1033, loss is 0.09767547\n",
      "epoch: 1 step: 1034, loss is 0.038781337\n",
      "epoch: 1 step: 1035, loss is 0.22044261\n",
      "epoch: 1 step: 1036, loss is 0.23661816\n",
      "epoch: 1 step: 1037, loss is 0.15334864\n",
      "epoch: 1 step: 1038, loss is 0.111811996\n",
      "epoch: 1 step: 1039, loss is 0.25681275\n",
      "epoch: 1 step: 1040, loss is 0.42307636\n",
      "epoch: 1 step: 1041, loss is 0.09835564\n",
      "epoch: 1 step: 1042, loss is 0.26294625\n",
      "epoch: 1 step: 1043, loss is 0.13063361\n",
      "epoch: 1 step: 1044, loss is 0.3426407\n",
      "epoch: 1 step: 1045, loss is 0.12190026\n",
      "epoch: 1 step: 1046, loss is 0.31873417\n",
      "epoch: 1 step: 1047, loss is 0.073972724\n",
      "epoch: 1 step: 1048, loss is 0.31763497\n",
      "epoch: 1 step: 1049, loss is 0.56187415\n",
      "epoch: 1 step: 1050, loss is 0.08616809\n",
      "epoch: 1 step: 1051, loss is 0.11812176\n",
      "epoch: 1 step: 1052, loss is 0.4409964\n",
      "epoch: 1 step: 1053, loss is 0.43535858\n",
      "epoch: 1 step: 1054, loss is 0.11469355\n",
      "epoch: 1 step: 1055, loss is 0.410482\n",
      "epoch: 1 step: 1056, loss is 0.36007234\n",
      "epoch: 1 step: 1057, loss is 0.31815174\n",
      "epoch: 1 step: 1058, loss is 0.26310107\n",
      "epoch: 1 step: 1059, loss is 0.5034247\n",
      "epoch: 1 step: 1060, loss is 0.054956272\n",
      "epoch: 1 step: 1061, loss is 0.3012221\n",
      "epoch: 1 step: 1062, loss is 0.31989446\n",
      "epoch: 1 step: 1063, loss is 0.46551272\n",
      "epoch: 1 step: 1064, loss is 0.20632836\n",
      "epoch: 1 step: 1065, loss is 0.25251827\n",
      "epoch: 1 step: 1066, loss is 0.21025665\n",
      "epoch: 1 step: 1067, loss is 0.22050281\n",
      "epoch: 1 step: 1068, loss is 0.2784381\n",
      "epoch: 1 step: 1069, loss is 0.23404464\n",
      "epoch: 1 step: 1070, loss is 0.088359684\n",
      "epoch: 1 step: 1071, loss is 0.18019982\n",
      "epoch: 1 step: 1072, loss is 0.108970456\n",
      "epoch: 1 step: 1073, loss is 0.20570382\n",
      "epoch: 1 step: 1074, loss is 0.2769875\n",
      "epoch: 1 step: 1075, loss is 0.16737488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1076, loss is 0.13103503\n",
      "epoch: 1 step: 1077, loss is 0.52587456\n",
      "epoch: 1 step: 1078, loss is 0.10299886\n",
      "epoch: 1 step: 1079, loss is 0.066845395\n",
      "epoch: 1 step: 1080, loss is 0.11382544\n",
      "epoch: 1 step: 1081, loss is 0.2933532\n",
      "epoch: 1 step: 1082, loss is 0.062687084\n",
      "epoch: 1 step: 1083, loss is 0.6439765\n",
      "epoch: 1 step: 1084, loss is 0.46292502\n",
      "epoch: 1 step: 1085, loss is 0.0806005\n",
      "epoch: 1 step: 1086, loss is 0.3541138\n",
      "epoch: 1 step: 1087, loss is 0.1060416\n",
      "epoch: 1 step: 1088, loss is 0.11656676\n",
      "epoch: 1 step: 1089, loss is 0.08570526\n",
      "epoch: 1 step: 1090, loss is 0.30926794\n",
      "epoch: 1 step: 1091, loss is 0.06504152\n",
      "epoch: 1 step: 1092, loss is 0.1294982\n",
      "epoch: 1 step: 1093, loss is 0.06666623\n",
      "epoch: 1 step: 1094, loss is 0.08977629\n",
      "epoch: 1 step: 1095, loss is 0.012176578\n",
      "epoch: 1 step: 1096, loss is 0.045609307\n",
      "epoch: 1 step: 1097, loss is 0.27441457\n",
      "epoch: 1 step: 1098, loss is 0.08697969\n",
      "epoch: 1 step: 1099, loss is 0.12002273\n",
      "epoch: 1 step: 1100, loss is 0.10471344\n",
      "epoch: 1 step: 1101, loss is 0.44123158\n",
      "epoch: 1 step: 1102, loss is 0.09887429\n",
      "epoch: 1 step: 1103, loss is 0.054553326\n",
      "epoch: 1 step: 1104, loss is 0.124743916\n",
      "epoch: 1 step: 1105, loss is 0.3632087\n",
      "epoch: 1 step: 1106, loss is 0.22825377\n",
      "epoch: 1 step: 1107, loss is 0.22171138\n",
      "epoch: 1 step: 1108, loss is 0.20774503\n",
      "epoch: 1 step: 1109, loss is 0.07895908\n",
      "epoch: 1 step: 1110, loss is 0.16962983\n",
      "epoch: 1 step: 1111, loss is 0.061587512\n",
      "epoch: 1 step: 1112, loss is 0.114845596\n",
      "epoch: 1 step: 1113, loss is 0.09878223\n",
      "epoch: 1 step: 1114, loss is 0.06693791\n",
      "epoch: 1 step: 1115, loss is 0.39542907\n",
      "epoch: 1 step: 1116, loss is 0.06845131\n",
      "epoch: 1 step: 1117, loss is 0.3918103\n",
      "epoch: 1 step: 1118, loss is 0.52045006\n",
      "epoch: 1 step: 1119, loss is 0.2592623\n",
      "epoch: 1 step: 1120, loss is 0.1950202\n",
      "epoch: 1 step: 1121, loss is 0.29700392\n",
      "epoch: 1 step: 1122, loss is 0.1832544\n",
      "epoch: 1 step: 1123, loss is 0.17407154\n",
      "epoch: 1 step: 1124, loss is 0.112820335\n",
      "epoch: 1 step: 1125, loss is 0.16302021\n",
      "epoch: 1 step: 1126, loss is 0.18920374\n",
      "epoch: 1 step: 1127, loss is 0.19272606\n",
      "epoch: 1 step: 1128, loss is 0.20288095\n",
      "epoch: 1 step: 1129, loss is 0.043344572\n",
      "epoch: 1 step: 1130, loss is 0.06465291\n",
      "epoch: 1 step: 1131, loss is 0.37542784\n",
      "epoch: 1 step: 1132, loss is 0.034852758\n",
      "epoch: 1 step: 1133, loss is 0.069677435\n",
      "epoch: 1 step: 1134, loss is 0.08241012\n",
      "epoch: 1 step: 1135, loss is 0.038857713\n",
      "epoch: 1 step: 1136, loss is 0.20070812\n",
      "epoch: 1 step: 1137, loss is 0.049577955\n",
      "epoch: 1 step: 1138, loss is 0.28729042\n",
      "epoch: 1 step: 1139, loss is 0.21802785\n",
      "epoch: 1 step: 1140, loss is 0.37743303\n",
      "epoch: 1 step: 1141, loss is 0.1460032\n",
      "epoch: 1 step: 1142, loss is 0.5701656\n",
      "epoch: 1 step: 1143, loss is 0.2352565\n",
      "epoch: 1 step: 1144, loss is 0.13319434\n",
      "epoch: 1 step: 1145, loss is 0.30803782\n",
      "epoch: 1 step: 1146, loss is 0.25211772\n",
      "epoch: 1 step: 1147, loss is 0.15588658\n",
      "epoch: 1 step: 1148, loss is 0.14707214\n",
      "epoch: 1 step: 1149, loss is 0.25674894\n",
      "epoch: 1 step: 1150, loss is 0.24759874\n",
      "epoch: 1 step: 1151, loss is 0.22430786\n",
      "epoch: 1 step: 1152, loss is 0.1401869\n",
      "epoch: 1 step: 1153, loss is 0.07728846\n",
      "epoch: 1 step: 1154, loss is 0.08476456\n",
      "epoch: 1 step: 1155, loss is 0.44032267\n",
      "epoch: 1 step: 1156, loss is 0.17974459\n",
      "epoch: 1 step: 1157, loss is 0.39847508\n",
      "epoch: 1 step: 1158, loss is 0.119021885\n",
      "epoch: 1 step: 1159, loss is 0.17337015\n",
      "epoch: 1 step: 1160, loss is 0.094390586\n",
      "epoch: 1 step: 1161, loss is 0.2781651\n",
      "epoch: 1 step: 1162, loss is 0.11873365\n",
      "epoch: 1 step: 1163, loss is 0.12928937\n",
      "epoch: 1 step: 1164, loss is 0.12747855\n",
      "epoch: 1 step: 1165, loss is 0.17025445\n",
      "epoch: 1 step: 1166, loss is 0.14992976\n",
      "epoch: 1 step: 1167, loss is 0.18179464\n",
      "epoch: 1 step: 1168, loss is 0.06783525\n",
      "epoch: 1 step: 1169, loss is 0.14594446\n",
      "epoch: 1 step: 1170, loss is 0.036532726\n",
      "epoch: 1 step: 1171, loss is 0.10265686\n",
      "epoch: 1 step: 1172, loss is 0.34323606\n",
      "epoch: 1 step: 1173, loss is 0.190417\n",
      "epoch: 1 step: 1174, loss is 0.22591558\n",
      "epoch: 1 step: 1175, loss is 0.32835016\n",
      "epoch: 1 step: 1176, loss is 0.040694352\n",
      "epoch: 1 step: 1177, loss is 0.14934261\n",
      "epoch: 1 step: 1178, loss is 0.049961466\n",
      "epoch: 1 step: 1179, loss is 0.15758656\n",
      "epoch: 1 step: 1180, loss is 0.26670334\n",
      "epoch: 1 step: 1181, loss is 0.011396315\n",
      "epoch: 1 step: 1182, loss is 0.24515851\n",
      "epoch: 1 step: 1183, loss is 0.028216185\n",
      "epoch: 1 step: 1184, loss is 0.081559606\n",
      "epoch: 1 step: 1185, loss is 0.077143505\n",
      "epoch: 1 step: 1186, loss is 0.23561285\n",
      "epoch: 1 step: 1187, loss is 0.114611395\n",
      "epoch: 1 step: 1188, loss is 0.15202385\n",
      "epoch: 1 step: 1189, loss is 0.010135927\n",
      "epoch: 1 step: 1190, loss is 0.10129381\n",
      "epoch: 1 step: 1191, loss is 0.1573498\n",
      "epoch: 1 step: 1192, loss is 0.03765364\n",
      "epoch: 1 step: 1193, loss is 0.43521827\n",
      "epoch: 1 step: 1194, loss is 0.13477924\n",
      "epoch: 1 step: 1195, loss is 0.36987802\n",
      "epoch: 1 step: 1196, loss is 0.069329664\n",
      "epoch: 1 step: 1197, loss is 0.2918318\n",
      "epoch: 1 step: 1198, loss is 0.20573185\n",
      "epoch: 1 step: 1199, loss is 0.0938844\n",
      "epoch: 1 step: 1200, loss is 0.11159877\n",
      "epoch: 1 step: 1201, loss is 0.18885927\n",
      "epoch: 1 step: 1202, loss is 0.44593784\n",
      "epoch: 1 step: 1203, loss is 0.18077\n",
      "epoch: 1 step: 1204, loss is 0.058083408\n",
      "epoch: 1 step: 1205, loss is 0.5453595\n",
      "epoch: 1 step: 1206, loss is 0.36011437\n",
      "epoch: 1 step: 1207, loss is 0.22290711\n",
      "epoch: 1 step: 1208, loss is 0.196266\n",
      "epoch: 1 step: 1209, loss is 0.049206547\n",
      "epoch: 1 step: 1210, loss is 0.08135601\n",
      "epoch: 1 step: 1211, loss is 0.17288467\n",
      "epoch: 1 step: 1212, loss is 0.1721118\n",
      "epoch: 1 step: 1213, loss is 0.29180467\n",
      "epoch: 1 step: 1214, loss is 0.3301466\n",
      "epoch: 1 step: 1215, loss is 0.5020781\n",
      "epoch: 1 step: 1216, loss is 0.3347965\n",
      "epoch: 1 step: 1217, loss is 0.15589601\n",
      "epoch: 1 step: 1218, loss is 0.050254572\n",
      "epoch: 1 step: 1219, loss is 0.23191118\n",
      "epoch: 1 step: 1220, loss is 0.10349661\n",
      "epoch: 1 step: 1221, loss is 0.31548738\n",
      "epoch: 1 step: 1222, loss is 0.29564384\n",
      "epoch: 1 step: 1223, loss is 0.3058967\n",
      "epoch: 1 step: 1224, loss is 0.6117856\n",
      "epoch: 1 step: 1225, loss is 0.2598191\n",
      "epoch: 1 step: 1226, loss is 0.054421574\n",
      "epoch: 1 step: 1227, loss is 0.060750786\n",
      "epoch: 1 step: 1228, loss is 0.17281425\n",
      "epoch: 1 step: 1229, loss is 0.14063254\n",
      "epoch: 1 step: 1230, loss is 0.22019324\n",
      "epoch: 1 step: 1231, loss is 0.1992556\n",
      "epoch: 1 step: 1232, loss is 0.313507\n",
      "epoch: 1 step: 1233, loss is 0.047601\n",
      "epoch: 1 step: 1234, loss is 0.32095274\n",
      "epoch: 1 step: 1235, loss is 0.095970355\n",
      "epoch: 1 step: 1236, loss is 0.34165156\n",
      "epoch: 1 step: 1237, loss is 0.077190936\n",
      "epoch: 1 step: 1238, loss is 0.037735954\n",
      "epoch: 1 step: 1239, loss is 0.26297835\n",
      "epoch: 1 step: 1240, loss is 0.3130402\n",
      "epoch: 1 step: 1241, loss is 0.06935564\n",
      "epoch: 1 step: 1242, loss is 0.3280826\n",
      "epoch: 1 step: 1243, loss is 0.24250925\n",
      "epoch: 1 step: 1244, loss is 0.45094004\n",
      "epoch: 1 step: 1245, loss is 0.53109956\n",
      "epoch: 1 step: 1246, loss is 0.13614146\n",
      "epoch: 1 step: 1247, loss is 0.27445346\n",
      "epoch: 1 step: 1248, loss is 0.27138132\n",
      "epoch: 1 step: 1249, loss is 0.22413065\n",
      "epoch: 1 step: 1250, loss is 0.39470667\n",
      "epoch: 1 step: 1251, loss is 0.35179472\n",
      "epoch: 1 step: 1252, loss is 0.31049493\n",
      "epoch: 1 step: 1253, loss is 0.31425437\n",
      "epoch: 1 step: 1254, loss is 0.19770174\n",
      "epoch: 1 step: 1255, loss is 0.052386284\n",
      "epoch: 1 step: 1256, loss is 0.40694454\n",
      "epoch: 1 step: 1257, loss is 0.37566334\n",
      "epoch: 1 step: 1258, loss is 0.28329965\n",
      "epoch: 1 step: 1259, loss is 0.33605158\n",
      "epoch: 1 step: 1260, loss is 0.16764435\n",
      "epoch: 1 step: 1261, loss is 0.13218252\n",
      "epoch: 1 step: 1262, loss is 0.21229364\n",
      "epoch: 1 step: 1263, loss is 0.23930809\n",
      "epoch: 1 step: 1264, loss is 0.12060567\n",
      "epoch: 1 step: 1265, loss is 0.37880793\n",
      "epoch: 1 step: 1266, loss is 0.25179908\n",
      "epoch: 1 step: 1267, loss is 0.21131927\n",
      "epoch: 1 step: 1268, loss is 0.047967322\n",
      "epoch: 1 step: 1269, loss is 0.10086147\n",
      "epoch: 1 step: 1270, loss is 0.088120595\n",
      "epoch: 1 step: 1271, loss is 0.2834859\n",
      "epoch: 1 step: 1272, loss is 0.18788773\n",
      "epoch: 1 step: 1273, loss is 0.18371096\n",
      "epoch: 1 step: 1274, loss is 0.067871474\n",
      "epoch: 1 step: 1275, loss is 0.39902177\n",
      "epoch: 1 step: 1276, loss is 0.13647169\n",
      "epoch: 1 step: 1277, loss is 0.047299486\n",
      "epoch: 1 step: 1278, loss is 0.22704387\n",
      "epoch: 1 step: 1279, loss is 0.19847134\n",
      "epoch: 1 step: 1280, loss is 0.038441114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1281, loss is 0.030004023\n",
      "epoch: 1 step: 1282, loss is 0.07597791\n",
      "epoch: 1 step: 1283, loss is 0.073831655\n",
      "epoch: 1 step: 1284, loss is 0.29810286\n",
      "epoch: 1 step: 1285, loss is 0.1132233\n",
      "epoch: 1 step: 1286, loss is 0.07252343\n",
      "epoch: 1 step: 1287, loss is 0.012536226\n",
      "epoch: 1 step: 1288, loss is 0.21477328\n",
      "epoch: 1 step: 1289, loss is 0.08646393\n",
      "epoch: 1 step: 1290, loss is 0.21891351\n",
      "epoch: 1 step: 1291, loss is 0.028665408\n",
      "epoch: 1 step: 1292, loss is 0.13709432\n",
      "epoch: 1 step: 1293, loss is 0.017844783\n",
      "epoch: 1 step: 1294, loss is 0.12268145\n",
      "epoch: 1 step: 1295, loss is 0.11659356\n",
      "epoch: 1 step: 1296, loss is 0.15494885\n",
      "epoch: 1 step: 1297, loss is 0.122669786\n",
      "epoch: 1 step: 1298, loss is 0.01046409\n",
      "epoch: 1 step: 1299, loss is 0.056485437\n",
      "epoch: 1 step: 1300, loss is 0.2121078\n",
      "epoch: 1 step: 1301, loss is 0.10384044\n",
      "epoch: 1 step: 1302, loss is 0.029057024\n",
      "epoch: 1 step: 1303, loss is 0.11940734\n",
      "epoch: 1 step: 1304, loss is 0.13816424\n",
      "epoch: 1 step: 1305, loss is 0.08834583\n",
      "epoch: 1 step: 1306, loss is 0.2593716\n",
      "epoch: 1 step: 1307, loss is 0.04634049\n",
      "epoch: 1 step: 1308, loss is 0.09140728\n",
      "epoch: 1 step: 1309, loss is 0.074608624\n",
      "epoch: 1 step: 1310, loss is 0.013653912\n",
      "epoch: 1 step: 1311, loss is 0.039637443\n",
      "epoch: 1 step: 1312, loss is 0.041759793\n",
      "epoch: 1 step: 1313, loss is 0.23351929\n",
      "epoch: 1 step: 1314, loss is 0.33243248\n",
      "epoch: 1 step: 1315, loss is 0.015912695\n",
      "epoch: 1 step: 1316, loss is 0.18483646\n",
      "epoch: 1 step: 1317, loss is 0.32364985\n",
      "epoch: 1 step: 1318, loss is 0.2840396\n",
      "epoch: 1 step: 1319, loss is 0.15333723\n",
      "epoch: 1 step: 1320, loss is 0.68716496\n",
      "epoch: 1 step: 1321, loss is 0.16414016\n",
      "epoch: 1 step: 1322, loss is 0.19118789\n",
      "epoch: 1 step: 1323, loss is 0.08328929\n",
      "epoch: 1 step: 1324, loss is 0.03852087\n",
      "epoch: 1 step: 1325, loss is 0.15543489\n",
      "epoch: 1 step: 1326, loss is 0.1442565\n",
      "epoch: 1 step: 1327, loss is 0.23333775\n",
      "epoch: 1 step: 1328, loss is 0.08696288\n",
      "epoch: 1 step: 1329, loss is 0.059169758\n",
      "epoch: 1 step: 1330, loss is 0.31171525\n",
      "epoch: 1 step: 1331, loss is 0.01238359\n",
      "epoch: 1 step: 1332, loss is 0.035312403\n",
      "epoch: 1 step: 1333, loss is 0.10604936\n",
      "epoch: 1 step: 1334, loss is 0.18601954\n",
      "epoch: 1 step: 1335, loss is 0.16292849\n",
      "epoch: 1 step: 1336, loss is 0.30451462\n",
      "epoch: 1 step: 1337, loss is 0.27445364\n",
      "epoch: 1 step: 1338, loss is 0.06946749\n",
      "epoch: 1 step: 1339, loss is 0.3113907\n",
      "epoch: 1 step: 1340, loss is 0.06370472\n",
      "epoch: 1 step: 1341, loss is 0.065791026\n",
      "epoch: 1 step: 1342, loss is 0.01892422\n",
      "epoch: 1 step: 1343, loss is 0.2213203\n",
      "epoch: 1 step: 1344, loss is 0.30412108\n",
      "epoch: 1 step: 1345, loss is 0.19640672\n",
      "epoch: 1 step: 1346, loss is 0.1400979\n",
      "epoch: 1 step: 1347, loss is 0.07222734\n",
      "epoch: 1 step: 1348, loss is 0.06820166\n",
      "epoch: 1 step: 1349, loss is 0.17023645\n",
      "epoch: 1 step: 1350, loss is 0.27778193\n",
      "epoch: 1 step: 1351, loss is 0.25124463\n",
      "epoch: 1 step: 1352, loss is 0.19337843\n",
      "epoch: 1 step: 1353, loss is 0.05637085\n",
      "epoch: 1 step: 1354, loss is 0.161658\n",
      "epoch: 1 step: 1355, loss is 0.11737389\n",
      "epoch: 1 step: 1356, loss is 0.17466423\n",
      "epoch: 1 step: 1357, loss is 0.13448827\n",
      "epoch: 1 step: 1358, loss is 0.19478692\n",
      "epoch: 1 step: 1359, loss is 0.3633579\n",
      "epoch: 1 step: 1360, loss is 0.26851878\n",
      "epoch: 1 step: 1361, loss is 0.5076872\n",
      "epoch: 1 step: 1362, loss is 0.08558829\n",
      "epoch: 1 step: 1363, loss is 0.26364765\n",
      "epoch: 1 step: 1364, loss is 0.19765362\n",
      "epoch: 1 step: 1365, loss is 0.09663299\n",
      "epoch: 1 step: 1366, loss is 0.19996533\n",
      "epoch: 1 step: 1367, loss is 0.3468359\n",
      "epoch: 1 step: 1368, loss is 0.36123702\n",
      "epoch: 1 step: 1369, loss is 0.035248183\n",
      "epoch: 1 step: 1370, loss is 0.11860904\n",
      "epoch: 1 step: 1371, loss is 0.10312848\n",
      "epoch: 1 step: 1372, loss is 0.17621076\n",
      "epoch: 1 step: 1373, loss is 0.32173407\n",
      "epoch: 1 step: 1374, loss is 0.16227527\n",
      "epoch: 1 step: 1375, loss is 0.2873512\n",
      "epoch: 1 step: 1376, loss is 0.11051424\n",
      "epoch: 1 step: 1377, loss is 0.2495335\n",
      "epoch: 1 step: 1378, loss is 0.02681749\n",
      "epoch: 1 step: 1379, loss is 0.06679486\n",
      "epoch: 1 step: 1380, loss is 0.10354947\n",
      "epoch: 1 step: 1381, loss is 0.1561051\n",
      "epoch: 1 step: 1382, loss is 0.01530307\n",
      "epoch: 1 step: 1383, loss is 0.31844094\n",
      "epoch: 1 step: 1384, loss is 0.06835628\n",
      "epoch: 1 step: 1385, loss is 0.18641713\n",
      "epoch: 1 step: 1386, loss is 0.26734197\n",
      "epoch: 1 step: 1387, loss is 0.20724517\n",
      "epoch: 1 step: 1388, loss is 0.42465937\n",
      "epoch: 1 step: 1389, loss is 0.20365162\n",
      "epoch: 1 step: 1390, loss is 0.035896238\n",
      "epoch: 1 step: 1391, loss is 0.111271575\n",
      "epoch: 1 step: 1392, loss is 0.1020528\n",
      "epoch: 1 step: 1393, loss is 0.10386815\n",
      "epoch: 1 step: 1394, loss is 0.20943134\n",
      "epoch: 1 step: 1395, loss is 0.19416949\n",
      "epoch: 1 step: 1396, loss is 0.6548252\n",
      "epoch: 1 step: 1397, loss is 0.13672888\n",
      "epoch: 1 step: 1398, loss is 0.16618672\n",
      "epoch: 1 step: 1399, loss is 0.33303344\n",
      "epoch: 1 step: 1400, loss is 0.20552573\n",
      "epoch: 1 step: 1401, loss is 0.12043615\n",
      "epoch: 1 step: 1402, loss is 0.11911503\n",
      "epoch: 1 step: 1403, loss is 0.11967438\n",
      "epoch: 1 step: 1404, loss is 0.11846678\n",
      "epoch: 1 step: 1405, loss is 0.1630381\n",
      "epoch: 1 step: 1406, loss is 0.022974217\n",
      "epoch: 1 step: 1407, loss is 0.3842898\n",
      "epoch: 1 step: 1408, loss is 0.07352545\n",
      "epoch: 1 step: 1409, loss is 0.14488232\n",
      "epoch: 1 step: 1410, loss is 0.21717094\n",
      "epoch: 1 step: 1411, loss is 0.23175049\n",
      "epoch: 1 step: 1412, loss is 0.087769195\n",
      "epoch: 1 step: 1413, loss is 0.093163796\n",
      "epoch: 1 step: 1414, loss is 0.11485554\n",
      "epoch: 1 step: 1415, loss is 0.28014633\n",
      "epoch: 1 step: 1416, loss is 0.06670242\n",
      "epoch: 1 step: 1417, loss is 0.21717842\n",
      "epoch: 1 step: 1418, loss is 0.2907421\n",
      "epoch: 1 step: 1419, loss is 0.15744354\n",
      "epoch: 1 step: 1420, loss is 0.23110963\n",
      "epoch: 1 step: 1421, loss is 0.015545519\n",
      "epoch: 1 step: 1422, loss is 0.13384724\n",
      "epoch: 1 step: 1423, loss is 0.22246599\n",
      "epoch: 1 step: 1424, loss is 0.2547528\n",
      "epoch: 1 step: 1425, loss is 0.23433594\n",
      "epoch: 1 step: 1426, loss is 0.1901317\n",
      "epoch: 1 step: 1427, loss is 0.29857105\n",
      "epoch: 1 step: 1428, loss is 0.21523938\n",
      "epoch: 1 step: 1429, loss is 0.041813802\n",
      "epoch: 1 step: 1430, loss is 0.5761401\n",
      "epoch: 1 step: 1431, loss is 0.028835122\n",
      "epoch: 1 step: 1432, loss is 0.15209176\n",
      "epoch: 1 step: 1433, loss is 0.12426143\n",
      "epoch: 1 step: 1434, loss is 0.33469284\n",
      "epoch: 1 step: 1435, loss is 0.18927696\n",
      "epoch: 1 step: 1436, loss is 0.3405954\n",
      "epoch: 1 step: 1437, loss is 0.35035872\n",
      "epoch: 1 step: 1438, loss is 0.26588082\n",
      "epoch: 1 step: 1439, loss is 0.33848992\n",
      "epoch: 1 step: 1440, loss is 0.08913925\n",
      "epoch: 1 step: 1441, loss is 0.2291478\n",
      "epoch: 1 step: 1442, loss is 0.23076221\n",
      "epoch: 1 step: 1443, loss is 0.28139666\n",
      "epoch: 1 step: 1444, loss is 0.110126615\n",
      "epoch: 1 step: 1445, loss is 0.095828705\n",
      "epoch: 1 step: 1446, loss is 0.17567767\n",
      "epoch: 1 step: 1447, loss is 0.20717715\n",
      "epoch: 1 step: 1448, loss is 0.03426958\n",
      "epoch: 1 step: 1449, loss is 0.14071093\n",
      "epoch: 1 step: 1450, loss is 0.13549419\n",
      "epoch: 1 step: 1451, loss is 0.13834612\n",
      "epoch: 1 step: 1452, loss is 0.39766973\n",
      "epoch: 1 step: 1453, loss is 0.0777603\n",
      "epoch: 1 step: 1454, loss is 0.31535232\n",
      "epoch: 1 step: 1455, loss is 0.29367444\n",
      "epoch: 1 step: 1456, loss is 0.11482871\n",
      "epoch: 1 step: 1457, loss is 0.041437626\n",
      "epoch: 1 step: 1458, loss is 0.23227389\n",
      "epoch: 1 step: 1459, loss is 0.037008192\n",
      "epoch: 1 step: 1460, loss is 0.061720423\n",
      "epoch: 1 step: 1461, loss is 0.09054213\n",
      "epoch: 1 step: 1462, loss is 0.103674896\n",
      "epoch: 1 step: 1463, loss is 0.06662679\n",
      "epoch: 1 step: 1464, loss is 0.1830682\n",
      "epoch: 1 step: 1465, loss is 0.5418561\n",
      "epoch: 1 step: 1466, loss is 0.23297377\n",
      "epoch: 1 step: 1467, loss is 0.21544342\n",
      "epoch: 1 step: 1468, loss is 0.6570592\n",
      "epoch: 1 step: 1469, loss is 0.23926456\n",
      "epoch: 1 step: 1470, loss is 0.08337666\n",
      "epoch: 1 step: 1471, loss is 0.2114595\n",
      "epoch: 1 step: 1472, loss is 0.21663783\n",
      "epoch: 1 step: 1473, loss is 0.28372166\n",
      "epoch: 1 step: 1474, loss is 0.11265243\n",
      "epoch: 1 step: 1475, loss is 0.08298786\n",
      "epoch: 1 step: 1476, loss is 0.16769415\n",
      "epoch: 1 step: 1477, loss is 0.180654\n",
      "epoch: 1 step: 1478, loss is 0.1473037\n",
      "epoch: 1 step: 1479, loss is 0.09227646\n",
      "epoch: 1 step: 1480, loss is 0.12319959\n",
      "epoch: 1 step: 1481, loss is 0.20263949\n",
      "epoch: 1 step: 1482, loss is 0.1171675\n",
      "epoch: 1 step: 1483, loss is 0.088836424\n",
      "epoch: 1 step: 1484, loss is 0.14286458\n",
      "epoch: 1 step: 1485, loss is 0.11671945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1486, loss is 0.06912272\n",
      "epoch: 1 step: 1487, loss is 0.07840847\n",
      "epoch: 1 step: 1488, loss is 0.14771412\n",
      "epoch: 1 step: 1489, loss is 0.1400881\n",
      "epoch: 1 step: 1490, loss is 0.17096727\n",
      "epoch: 1 step: 1491, loss is 0.16490632\n",
      "epoch: 1 step: 1492, loss is 0.17573768\n",
      "epoch: 1 step: 1493, loss is 0.13727075\n",
      "epoch: 1 step: 1494, loss is 0.021594124\n",
      "epoch: 1 step: 1495, loss is 0.2077843\n",
      "epoch: 1 step: 1496, loss is 0.15017173\n",
      "epoch: 1 step: 1497, loss is 0.088545844\n",
      "epoch: 1 step: 1498, loss is 0.085715264\n",
      "epoch: 1 step: 1499, loss is 0.21272267\n",
      "epoch: 1 step: 1500, loss is 0.1819271\n",
      "epoch: 1 step: 1501, loss is 0.021068264\n",
      "epoch: 1 step: 1502, loss is 0.04373883\n",
      "epoch: 1 step: 1503, loss is 0.0073648244\n",
      "epoch: 1 step: 1504, loss is 0.0132490145\n",
      "epoch: 1 step: 1505, loss is 0.33317026\n",
      "epoch: 1 step: 1506, loss is 0.1650341\n",
      "epoch: 1 step: 1507, loss is 0.05693629\n",
      "epoch: 1 step: 1508, loss is 0.29916528\n",
      "epoch: 1 step: 1509, loss is 0.1890237\n",
      "epoch: 1 step: 1510, loss is 0.19690672\n",
      "epoch: 1 step: 1511, loss is 0.14677987\n",
      "epoch: 1 step: 1512, loss is 0.05485638\n",
      "epoch: 1 step: 1513, loss is 0.03899035\n",
      "epoch: 1 step: 1514, loss is 0.23035137\n",
      "epoch: 1 step: 1515, loss is 0.112869844\n",
      "epoch: 1 step: 1516, loss is 0.2249818\n",
      "epoch: 1 step: 1517, loss is 0.17724898\n",
      "epoch: 1 step: 1518, loss is 0.13939808\n",
      "epoch: 1 step: 1519, loss is 0.11180441\n",
      "epoch: 1 step: 1520, loss is 0.1294095\n",
      "epoch: 1 step: 1521, loss is 0.32339078\n",
      "epoch: 1 step: 1522, loss is 0.0912177\n",
      "epoch: 1 step: 1523, loss is 0.041473303\n",
      "epoch: 1 step: 1524, loss is 0.24248303\n",
      "epoch: 1 step: 1525, loss is 0.13012742\n",
      "epoch: 1 step: 1526, loss is 0.086474486\n",
      "epoch: 1 step: 1527, loss is 0.050236378\n",
      "epoch: 1 step: 1528, loss is 0.16469808\n",
      "epoch: 1 step: 1529, loss is 0.2962309\n",
      "epoch: 1 step: 1530, loss is 0.19530395\n",
      "epoch: 1 step: 1531, loss is 0.06736516\n",
      "epoch: 1 step: 1532, loss is 0.07710972\n",
      "epoch: 1 step: 1533, loss is 0.18646315\n",
      "epoch: 1 step: 1534, loss is 0.07895495\n",
      "epoch: 1 step: 1535, loss is 0.0144727575\n",
      "epoch: 1 step: 1536, loss is 0.13384183\n",
      "epoch: 1 step: 1537, loss is 0.27084693\n",
      "epoch: 1 step: 1538, loss is 0.40545914\n",
      "epoch: 1 step: 1539, loss is 0.042475004\n",
      "epoch: 1 step: 1540, loss is 0.104733065\n",
      "epoch: 1 step: 1541, loss is 0.15273927\n",
      "epoch: 1 step: 1542, loss is 0.49948683\n",
      "epoch: 1 step: 1543, loss is 0.04434921\n",
      "epoch: 1 step: 1544, loss is 0.10921406\n",
      "epoch: 1 step: 1545, loss is 0.061362725\n",
      "epoch: 1 step: 1546, loss is 0.06576102\n",
      "epoch: 1 step: 1547, loss is 0.031060375\n",
      "epoch: 1 step: 1548, loss is 0.18757424\n",
      "epoch: 1 step: 1549, loss is 0.07537481\n",
      "epoch: 1 step: 1550, loss is 0.055628587\n",
      "epoch: 1 step: 1551, loss is 0.28255025\n",
      "epoch: 1 step: 1552, loss is 0.09366954\n",
      "epoch: 1 step: 1553, loss is 0.17681661\n",
      "epoch: 1 step: 1554, loss is 0.1054406\n",
      "epoch: 1 step: 1555, loss is 0.18327224\n",
      "epoch: 1 step: 1556, loss is 0.02039021\n",
      "epoch: 1 step: 1557, loss is 0.23835607\n",
      "epoch: 1 step: 1558, loss is 0.099063195\n",
      "epoch: 1 step: 1559, loss is 0.077503525\n",
      "epoch: 1 step: 1560, loss is 0.09377991\n",
      "epoch: 1 step: 1561, loss is 0.2452944\n",
      "epoch: 1 step: 1562, loss is 0.17762004\n",
      "epoch: 1 step: 1563, loss is 0.093298815\n",
      "epoch: 1 step: 1564, loss is 0.116764784\n",
      "epoch: 1 step: 1565, loss is 0.019866813\n",
      "epoch: 1 step: 1566, loss is 0.061431922\n",
      "epoch: 1 step: 1567, loss is 0.22315788\n",
      "epoch: 1 step: 1568, loss is 0.09534703\n",
      "epoch: 1 step: 1569, loss is 0.2220955\n",
      "epoch: 1 step: 1570, loss is 0.13382143\n",
      "epoch: 1 step: 1571, loss is 0.038250998\n",
      "epoch: 1 step: 1572, loss is 0.16433157\n",
      "epoch: 1 step: 1573, loss is 0.073995546\n",
      "epoch: 1 step: 1574, loss is 0.06757239\n",
      "epoch: 1 step: 1575, loss is 0.051049624\n",
      "epoch: 1 step: 1576, loss is 0.18537688\n",
      "epoch: 1 step: 1577, loss is 0.023228511\n",
      "epoch: 1 step: 1578, loss is 0.015664395\n",
      "epoch: 1 step: 1579, loss is 0.16582571\n",
      "epoch: 1 step: 1580, loss is 0.4531794\n",
      "epoch: 1 step: 1581, loss is 0.05457745\n",
      "epoch: 1 step: 1582, loss is 0.2335661\n",
      "epoch: 1 step: 1583, loss is 0.17762992\n",
      "epoch: 1 step: 1584, loss is 0.0487762\n",
      "epoch: 1 step: 1585, loss is 0.036715236\n",
      "epoch: 1 step: 1586, loss is 0.12728411\n",
      "epoch: 1 step: 1587, loss is 0.03077582\n",
      "epoch: 1 step: 1588, loss is 0.04022311\n",
      "epoch: 1 step: 1589, loss is 0.081304185\n",
      "epoch: 1 step: 1590, loss is 0.37499338\n",
      "epoch: 1 step: 1591, loss is 0.08647748\n",
      "epoch: 1 step: 1592, loss is 0.1194441\n",
      "epoch: 1 step: 1593, loss is 0.09077908\n",
      "epoch: 1 step: 1594, loss is 0.092437044\n",
      "epoch: 1 step: 1595, loss is 0.1460568\n",
      "epoch: 1 step: 1596, loss is 0.14034255\n",
      "epoch: 1 step: 1597, loss is 0.13292143\n",
      "epoch: 1 step: 1598, loss is 0.078910984\n",
      "epoch: 1 step: 1599, loss is 0.01104415\n",
      "epoch: 1 step: 1600, loss is 0.022102611\n",
      "epoch: 1 step: 1601, loss is 0.04850782\n",
      "epoch: 1 step: 1602, loss is 0.04974729\n",
      "epoch: 1 step: 1603, loss is 0.19834001\n",
      "epoch: 1 step: 1604, loss is 0.0324876\n",
      "epoch: 1 step: 1605, loss is 0.2745791\n",
      "epoch: 1 step: 1606, loss is 0.31713748\n",
      "epoch: 1 step: 1607, loss is 0.15826584\n",
      "epoch: 1 step: 1608, loss is 0.019678665\n",
      "epoch: 1 step: 1609, loss is 0.12687755\n",
      "epoch: 1 step: 1610, loss is 0.21980438\n",
      "epoch: 1 step: 1611, loss is 0.1451211\n",
      "epoch: 1 step: 1612, loss is 0.24864426\n",
      "epoch: 1 step: 1613, loss is 0.030988624\n",
      "epoch: 1 step: 1614, loss is 0.5612055\n",
      "epoch: 1 step: 1615, loss is 0.03695114\n",
      "epoch: 1 step: 1616, loss is 0.16087972\n",
      "epoch: 1 step: 1617, loss is 0.27271098\n",
      "epoch: 1 step: 1618, loss is 0.20901328\n",
      "epoch: 1 step: 1619, loss is 0.23420103\n",
      "epoch: 1 step: 1620, loss is 0.14590435\n",
      "epoch: 1 step: 1621, loss is 0.042186413\n",
      "epoch: 1 step: 1622, loss is 0.25282332\n",
      "epoch: 1 step: 1623, loss is 0.09036187\n",
      "epoch: 1 step: 1624, loss is 0.06585581\n",
      "epoch: 1 step: 1625, loss is 0.1369748\n",
      "epoch: 1 step: 1626, loss is 0.09464093\n",
      "epoch: 1 step: 1627, loss is 0.2726054\n",
      "epoch: 1 step: 1628, loss is 0.047652952\n",
      "epoch: 1 step: 1629, loss is 0.06971765\n",
      "epoch: 1 step: 1630, loss is 0.0777553\n",
      "epoch: 1 step: 1631, loss is 0.054068584\n",
      "epoch: 1 step: 1632, loss is 0.07302881\n",
      "epoch: 1 step: 1633, loss is 0.26436827\n",
      "epoch: 1 step: 1634, loss is 0.053883195\n",
      "epoch: 1 step: 1635, loss is 0.016681122\n",
      "epoch: 1 step: 1636, loss is 0.27409494\n",
      "epoch: 1 step: 1637, loss is 0.04734907\n",
      "epoch: 1 step: 1638, loss is 0.027033135\n",
      "epoch: 1 step: 1639, loss is 0.13960634\n",
      "epoch: 1 step: 1640, loss is 0.04555743\n",
      "epoch: 1 step: 1641, loss is 0.17703138\n",
      "epoch: 1 step: 1642, loss is 0.18636443\n",
      "epoch: 1 step: 1643, loss is 0.006722331\n",
      "epoch: 1 step: 1644, loss is 0.32759765\n",
      "epoch: 1 step: 1645, loss is 0.14893447\n",
      "epoch: 1 step: 1646, loss is 0.061191928\n",
      "epoch: 1 step: 1647, loss is 0.2399621\n",
      "epoch: 1 step: 1648, loss is 0.04727875\n",
      "epoch: 1 step: 1649, loss is 0.059184045\n",
      "epoch: 1 step: 1650, loss is 0.1053849\n",
      "epoch: 1 step: 1651, loss is 0.059289634\n",
      "epoch: 1 step: 1652, loss is 0.121347174\n",
      "epoch: 1 step: 1653, loss is 0.22168191\n",
      "epoch: 1 step: 1654, loss is 0.029022075\n",
      "epoch: 1 step: 1655, loss is 0.01343743\n",
      "epoch: 1 step: 1656, loss is 0.07532048\n",
      "epoch: 1 step: 1657, loss is 0.12682287\n",
      "epoch: 1 step: 1658, loss is 0.13183922\n",
      "epoch: 1 step: 1659, loss is 0.30451655\n",
      "epoch: 1 step: 1660, loss is 0.17477378\n",
      "epoch: 1 step: 1661, loss is 0.16705132\n",
      "epoch: 1 step: 1662, loss is 0.018746238\n",
      "epoch: 1 step: 1663, loss is 0.28499871\n",
      "epoch: 1 step: 1664, loss is 0.12521574\n",
      "epoch: 1 step: 1665, loss is 0.0951421\n",
      "epoch: 1 step: 1666, loss is 0.17755203\n",
      "epoch: 1 step: 1667, loss is 0.024901386\n",
      "epoch: 1 step: 1668, loss is 0.016737293\n",
      "epoch: 1 step: 1669, loss is 0.028700726\n",
      "epoch: 1 step: 1670, loss is 0.25620556\n",
      "epoch: 1 step: 1671, loss is 0.09906233\n",
      "epoch: 1 step: 1672, loss is 0.119347334\n",
      "epoch: 1 step: 1673, loss is 0.1526634\n",
      "epoch: 1 step: 1674, loss is 0.20434955\n",
      "epoch: 1 step: 1675, loss is 0.033184372\n",
      "epoch: 1 step: 1676, loss is 0.08633513\n",
      "epoch: 1 step: 1677, loss is 0.1956253\n",
      "epoch: 1 step: 1678, loss is 0.028049756\n",
      "epoch: 1 step: 1679, loss is 0.14387318\n",
      "epoch: 1 step: 1680, loss is 0.053388488\n",
      "epoch: 1 step: 1681, loss is 0.027290002\n",
      "epoch: 1 step: 1682, loss is 0.31803727\n",
      "epoch: 1 step: 1683, loss is 0.15151787\n",
      "epoch: 1 step: 1684, loss is 0.054278806\n",
      "epoch: 1 step: 1685, loss is 0.042600174\n",
      "epoch: 1 step: 1686, loss is 0.02414224\n",
      "epoch: 1 step: 1687, loss is 0.18347515\n",
      "epoch: 1 step: 1688, loss is 0.008202599\n",
      "epoch: 1 step: 1689, loss is 0.03685372\n",
      "epoch: 1 step: 1690, loss is 0.11425542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1691, loss is 0.31586635\n",
      "epoch: 1 step: 1692, loss is 0.016652398\n",
      "epoch: 1 step: 1693, loss is 0.30536532\n",
      "epoch: 1 step: 1694, loss is 0.1146685\n",
      "epoch: 1 step: 1695, loss is 0.18415359\n",
      "epoch: 1 step: 1696, loss is 0.104863755\n",
      "epoch: 1 step: 1697, loss is 0.06876555\n",
      "epoch: 1 step: 1698, loss is 0.2796171\n",
      "epoch: 1 step: 1699, loss is 0.060556445\n",
      "epoch: 1 step: 1700, loss is 0.27135572\n",
      "epoch: 1 step: 1701, loss is 0.17843182\n",
      "epoch: 1 step: 1702, loss is 0.02482147\n",
      "epoch: 1 step: 1703, loss is 0.021130752\n",
      "epoch: 1 step: 1704, loss is 0.11698529\n",
      "epoch: 1 step: 1705, loss is 0.13115078\n",
      "epoch: 1 step: 1706, loss is 0.020555038\n",
      "epoch: 1 step: 1707, loss is 0.14393555\n",
      "epoch: 1 step: 1708, loss is 0.14842501\n",
      "epoch: 1 step: 1709, loss is 0.46632153\n",
      "epoch: 1 step: 1710, loss is 0.18283802\n",
      "epoch: 1 step: 1711, loss is 0.1317218\n",
      "epoch: 1 step: 1712, loss is 0.25727025\n",
      "epoch: 1 step: 1713, loss is 0.06488792\n",
      "epoch: 1 step: 1714, loss is 0.18351129\n",
      "epoch: 1 step: 1715, loss is 0.0701038\n",
      "epoch: 1 step: 1716, loss is 0.07845127\n",
      "epoch: 1 step: 1717, loss is 0.49595422\n",
      "epoch: 1 step: 1718, loss is 0.15111835\n",
      "epoch: 1 step: 1719, loss is 0.40810373\n",
      "epoch: 1 step: 1720, loss is 0.03821364\n",
      "epoch: 1 step: 1721, loss is 0.060729194\n",
      "epoch: 1 step: 1722, loss is 0.03975378\n",
      "epoch: 1 step: 1723, loss is 0.18609482\n",
      "epoch: 1 step: 1724, loss is 0.043053485\n",
      "epoch: 1 step: 1725, loss is 0.11461286\n",
      "epoch: 1 step: 1726, loss is 0.115049995\n",
      "epoch: 1 step: 1727, loss is 0.14427969\n",
      "epoch: 1 step: 1728, loss is 0.089327686\n",
      "epoch: 1 step: 1729, loss is 0.1275086\n",
      "epoch: 1 step: 1730, loss is 0.013967913\n",
      "epoch: 1 step: 1731, loss is 0.22404045\n",
      "epoch: 1 step: 1732, loss is 0.105075434\n",
      "epoch: 1 step: 1733, loss is 0.039694794\n",
      "epoch: 1 step: 1734, loss is 0.081343554\n",
      "epoch: 1 step: 1735, loss is 0.21068461\n",
      "epoch: 1 step: 1736, loss is 0.043303587\n",
      "epoch: 1 step: 1737, loss is 0.13215047\n",
      "epoch: 1 step: 1738, loss is 0.013993139\n",
      "epoch: 1 step: 1739, loss is 0.015351852\n",
      "epoch: 1 step: 1740, loss is 0.2161441\n",
      "epoch: 1 step: 1741, loss is 0.0070830025\n",
      "epoch: 1 step: 1742, loss is 0.07486534\n",
      "epoch: 1 step: 1743, loss is 0.02932832\n",
      "epoch: 1 step: 1744, loss is 0.13578773\n",
      "epoch: 1 step: 1745, loss is 0.030561801\n",
      "epoch: 1 step: 1746, loss is 0.008741821\n",
      "epoch: 1 step: 1747, loss is 0.08857304\n",
      "epoch: 1 step: 1748, loss is 0.11589983\n",
      "epoch: 1 step: 1749, loss is 0.047920015\n",
      "epoch: 1 step: 1750, loss is 0.115788236\n",
      "epoch: 1 step: 1751, loss is 0.17805716\n",
      "epoch: 1 step: 1752, loss is 0.08054957\n",
      "epoch: 1 step: 1753, loss is 0.10071593\n",
      "epoch: 1 step: 1754, loss is 0.026864003\n",
      "epoch: 1 step: 1755, loss is 0.061612\n",
      "epoch: 1 step: 1756, loss is 0.022064285\n",
      "epoch: 1 step: 1757, loss is 0.068508215\n",
      "epoch: 1 step: 1758, loss is 0.082496084\n",
      "epoch: 1 step: 1759, loss is 0.089356594\n",
      "epoch: 1 step: 1760, loss is 0.12105812\n",
      "epoch: 1 step: 1761, loss is 0.16415116\n",
      "epoch: 1 step: 1762, loss is 0.14463258\n",
      "epoch: 1 step: 1763, loss is 0.16867046\n",
      "epoch: 1 step: 1764, loss is 0.097491555\n",
      "epoch: 1 step: 1765, loss is 0.13946488\n",
      "epoch: 1 step: 1766, loss is 0.14400193\n",
      "epoch: 1 step: 1767, loss is 0.011152694\n",
      "epoch: 1 step: 1768, loss is 0.21570401\n",
      "epoch: 1 step: 1769, loss is 0.117904745\n",
      "epoch: 1 step: 1770, loss is 0.01257468\n",
      "epoch: 1 step: 1771, loss is 0.17823035\n",
      "epoch: 1 step: 1772, loss is 0.018407362\n",
      "epoch: 1 step: 1773, loss is 0.07824042\n",
      "epoch: 1 step: 1774, loss is 0.017692821\n",
      "epoch: 1 step: 1775, loss is 0.27000493\n",
      "epoch: 1 step: 1776, loss is 0.26023793\n",
      "epoch: 1 step: 1777, loss is 0.17717414\n",
      "epoch: 1 step: 1778, loss is 0.059077777\n",
      "epoch: 1 step: 1779, loss is 0.17385578\n",
      "epoch: 1 step: 1780, loss is 0.18089835\n",
      "epoch: 1 step: 1781, loss is 0.09047447\n",
      "epoch: 1 step: 1782, loss is 0.08430125\n",
      "epoch: 1 step: 1783, loss is 0.0026204977\n",
      "epoch: 1 step: 1784, loss is 0.02531832\n",
      "epoch: 1 step: 1785, loss is 0.27614108\n",
      "epoch: 1 step: 1786, loss is 0.1162663\n",
      "epoch: 1 step: 1787, loss is 0.16978069\n",
      "epoch: 1 step: 1788, loss is 0.15761353\n",
      "epoch: 1 step: 1789, loss is 0.049235146\n",
      "epoch: 1 step: 1790, loss is 0.17887567\n",
      "epoch: 1 step: 1791, loss is 0.20283449\n",
      "epoch: 1 step: 1792, loss is 0.0061343494\n",
      "epoch: 1 step: 1793, loss is 0.011954833\n",
      "epoch: 1 step: 1794, loss is 0.19696522\n",
      "epoch: 1 step: 1795, loss is 0.1294593\n",
      "epoch: 1 step: 1796, loss is 0.07736407\n",
      "epoch: 1 step: 1797, loss is 0.19540425\n",
      "epoch: 1 step: 1798, loss is 0.14758815\n",
      "epoch: 1 step: 1799, loss is 0.055320013\n",
      "epoch: 1 step: 1800, loss is 0.31772676\n",
      "epoch: 1 step: 1801, loss is 0.25686538\n",
      "epoch: 1 step: 1802, loss is 0.060361147\n",
      "epoch: 1 step: 1803, loss is 0.29740694\n",
      "epoch: 1 step: 1804, loss is 0.20332676\n",
      "epoch: 1 step: 1805, loss is 0.11923816\n",
      "epoch: 1 step: 1806, loss is 0.115911946\n",
      "epoch: 1 step: 1807, loss is 0.052371446\n",
      "epoch: 1 step: 1808, loss is 0.12310862\n",
      "epoch: 1 step: 1809, loss is 0.5295109\n",
      "epoch: 1 step: 1810, loss is 0.0343727\n",
      "epoch: 1 step: 1811, loss is 0.078363545\n",
      "epoch: 1 step: 1812, loss is 0.03337823\n",
      "epoch: 1 step: 1813, loss is 0.0135552315\n",
      "epoch: 1 step: 1814, loss is 0.00981911\n",
      "epoch: 1 step: 1815, loss is 0.32793882\n",
      "epoch: 1 step: 1816, loss is 0.17899522\n",
      "epoch: 1 step: 1817, loss is 0.03321256\n",
      "epoch: 1 step: 1818, loss is 0.02370546\n",
      "epoch: 1 step: 1819, loss is 0.15172538\n",
      "epoch: 1 step: 1820, loss is 0.027277995\n",
      "epoch: 1 step: 1821, loss is 0.13931352\n",
      "epoch: 1 step: 1822, loss is 0.1793023\n",
      "epoch: 1 step: 1823, loss is 0.11972391\n",
      "epoch: 1 step: 1824, loss is 0.10639357\n",
      "epoch: 1 step: 1825, loss is 0.08397127\n",
      "epoch: 1 step: 1826, loss is 0.10990251\n",
      "epoch: 1 step: 1827, loss is 0.21178995\n",
      "epoch: 1 step: 1828, loss is 0.11215887\n",
      "epoch: 1 step: 1829, loss is 0.029213058\n",
      "epoch: 1 step: 1830, loss is 0.44162026\n",
      "epoch: 1 step: 1831, loss is 0.024592575\n",
      "epoch: 1 step: 1832, loss is 0.034840822\n",
      "epoch: 1 step: 1833, loss is 0.27802646\n",
      "epoch: 1 step: 1834, loss is 0.27189982\n",
      "epoch: 1 step: 1835, loss is 0.15891738\n",
      "epoch: 1 step: 1836, loss is 0.06323076\n",
      "epoch: 1 step: 1837, loss is 0.031240579\n",
      "epoch: 1 step: 1838, loss is 0.05181766\n",
      "epoch: 1 step: 1839, loss is 0.24571651\n",
      "epoch: 1 step: 1840, loss is 0.089639656\n",
      "epoch: 1 step: 1841, loss is 0.1810686\n",
      "epoch: 1 step: 1842, loss is 0.252783\n",
      "epoch: 1 step: 1843, loss is 0.4369812\n",
      "epoch: 1 step: 1844, loss is 0.31739122\n",
      "epoch: 1 step: 1845, loss is 0.17282064\n",
      "epoch: 1 step: 1846, loss is 0.08468941\n",
      "epoch: 1 step: 1847, loss is 0.13069986\n",
      "epoch: 1 step: 1848, loss is 0.314167\n",
      "epoch: 1 step: 1849, loss is 0.074816436\n",
      "epoch: 1 step: 1850, loss is 0.105258465\n",
      "epoch: 1 step: 1851, loss is 0.12038314\n",
      "epoch: 1 step: 1852, loss is 0.094276816\n",
      "epoch: 1 step: 1853, loss is 0.13264057\n",
      "epoch: 1 step: 1854, loss is 0.09248036\n",
      "epoch: 1 step: 1855, loss is 0.029399278\n",
      "epoch: 1 step: 1856, loss is 0.06050051\n",
      "epoch: 1 step: 1857, loss is 0.15092826\n",
      "epoch: 1 step: 1858, loss is 0.021551287\n",
      "epoch: 1 step: 1859, loss is 0.39599943\n",
      "epoch: 1 step: 1860, loss is 0.0184165\n",
      "epoch: 1 step: 1861, loss is 0.22493638\n",
      "epoch: 1 step: 1862, loss is 0.11667955\n",
      "epoch: 1 step: 1863, loss is 0.0673678\n",
      "epoch: 1 step: 1864, loss is 0.06534538\n",
      "epoch: 1 step: 1865, loss is 0.2050861\n",
      "epoch: 1 step: 1866, loss is 0.2616502\n",
      "epoch: 1 step: 1867, loss is 0.00683715\n",
      "epoch: 1 step: 1868, loss is 0.002503902\n",
      "epoch: 1 step: 1869, loss is 0.006746034\n",
      "epoch: 1 step: 1870, loss is 0.043680277\n",
      "epoch: 1 step: 1871, loss is 0.19812445\n",
      "epoch: 1 step: 1872, loss is 0.31692633\n",
      "epoch: 1 step: 1873, loss is 0.06562613\n",
      "epoch: 1 step: 1874, loss is 0.123896874\n",
      "epoch: 1 step: 1875, loss is 0.14199209\n",
      "************************Finished training*************************\n",
      "************************Start evaluation*************************\n",
      "============== Accuracy:{'Accuracy': 0.9671474358974359} ==============\n"
     ]
    }
   ],
   "source": [
    "# 创建mnist路径\n",
    "ckpt_folder = '/etc/tinyms/serving/lenet5'\n",
    "ckpt_path = '/etc/tinyms/serving/lenet5/lenet5.ckpt'\n",
    "if not os.path.exists(ckpt_folder):\n",
    "    !mkdir -p  /etc/tinyms/serving/lenet5\n",
    "else:\n",
    "    print('lenet5 ckpt folder already exists')\n",
    "\n",
    "# 设置环境参数\n",
    "device_target = \"CPU\"\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=device_target)  \n",
    "dataset_sink_mode = False\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = MnistDataset(os.path.join(mnist_path, \"train\"), shuffle=True)\n",
    "train_dataset = mnist_transform.apply_ds(train_dataset)\n",
    "eval_dataset = MnistDataset(os.path.join(mnist_path, \"test\"), shuffle=True)\n",
    "eval_dataset = mnist_transform.apply_ds(eval_dataset)\n",
    "\n",
    "# 设置训练参数\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "epoch_size = 1\n",
    "batch_size = 32\n",
    "\n",
    "# 定义loss函数\n",
    "net_loss = SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "# 定义optimizer\n",
    "net_opt = opt.Momentum(net.trainable_params(), lr, momentum)\n",
    "net_metrics={\"Accuracy\": Accuracy()}\n",
    "model.compile(loss_fn=net_loss, optimizer=net_opt, metrics=net_metrics)\n",
    "\n",
    "print('************************Start training*************************')\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10))\n",
    "model.train(epoch_size, train_dataset, callbacks=[ckpoint_cb, LossMonitor()],dataset_sink_mode=dataset_sink_mode)\n",
    "print('************************Finished training*************************')\n",
    "model.save_checkpoint(ckpt_path)\n",
    "\n",
    "\n",
    "model.load_checkpoint(ckpt_path)\n",
    "print('************************Start evaluation*************************')\n",
    "acc = model.eval(eval_dataset, dataset_sink_mode=dataset_sink_mode)\n",
    "print(\"============== Accuracy:{} ==============\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-characteristic",
   "metadata": {},
   "source": [
    "### 4. 定义servable.json\n",
    "\n",
    "定义lenet5 servable json文件，Servable json文件定义了servable名称，模型名称，模型格式和分类数量，以便后续推理使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "raising-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "servable_json = [{'name': 'lenet5', \n",
    "                  'description': 'This servable hosts a lenet5 model predicting numbers', \n",
    "                  'model': {\n",
    "                      \"name\": \"lenet5\", \n",
    "                      \"format\": \"ckpt\", \n",
    "                      \"class_num\": 10}}]\n",
    "os.chdir(\"/etc/tinyms/serving\")\n",
    "json_data = json.dumps(servable_json, indent=4)\n",
    "\n",
    "with open('servable.json', 'w') as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-matthew",
   "metadata": {},
   "source": [
    "### 5. 启动服务器\n",
    "\n",
    "#### 5.1 介绍\n",
    "TinyMS推理是C/S（Client/Server）架构。TinyMS使用[Flask](https://flask.palletsprojects.com/en/1.1.x/)这个轻量化的网页服务器架构作为C/S通讯的基础架构。为了能够对模型进行推理，用户必须首先启动服务器。如果成功启动，服务器会在子进程中运行并且会监听从地址127.0.0.1，端口号5000发送来的POST请求并且使用MindSpore作为后端来处理这些请求。后端会构建模型，运行推理并且返回结果给客户端\n",
    "\n",
    "#### 5.2 启动服务器\n",
    "\n",
    "运行下列代码以启动服务器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "convinced-theorem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server starts at host 127.0.0.1, port 5000\n"
     ]
    }
   ],
   "source": [
    "start_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-wallpaper",
   "metadata": {},
   "source": [
    "### 6. 推理\n",
    "\n",
    "#### 6.1 上传图片\n",
    "\n",
    "用户需要上传一张0~9之间的数字图片作为输入。如果使用命令行终端，可以使用'scp'或者'wget'获取图片，如果使用Jupyter，点击菜单右上方的'Upload'按钮并且选择上传的图片。本教程中使用的图片可以点击[这里](https://ascend-tutorials.obs.cn-north-4.myhuaweicloud.com/tinyms-test-pics/numbers/7.png)进行下载，将图片保存在根目录下，重命名为'7.png'（或者任何用户喜欢的名字）\n",
    "\n",
    "或者运行如下的代码下载图片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "activated-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-17 16:32:24--  https://ascend-tutorials.obs.cn-north-4.myhuaweicloud.com/tinyms-test-pics/numbers/7.png\n",
      "Resolving ascend-tutorials.obs.cn-north-4.myhuaweicloud.com (ascend-tutorials.obs.cn-north-4.myhuaweicloud.com)... 49.4.112.5, 49.4.112.113, 121.36.121.44, ...\n",
      "Connecting to ascend-tutorials.obs.cn-north-4.myhuaweicloud.com (ascend-tutorials.obs.cn-north-4.myhuaweicloud.com)|49.4.112.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34970 (34K) [image/png]\n",
      "Saving to: ‘/root/7.png’\n",
      "\n",
      "7.png               100%[===================>]  34.15K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-03-17 16:32:25 (323 KB/s) - ‘/root/7.png’ saved [34970/34970]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/root/7.png'):\n",
    "    !wget -P /root/ https://ascend-tutorials.obs.cn-north-4.myhuaweicloud.com/tinyms-test-pics/numbers/7.png\n",
    "else:\n",
    "    print('7.png already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-timer",
   "metadata": {},
   "source": [
    "#### 6.2 List servables\n",
    "\n",
    "使用`list_servables`函数检查当前后端的serving模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "satisfactory-windows",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'This servable hosts a lenet5 model predicting numbers',\n",
       "  'model': {'class_num': 10, 'format': 'ckpt', 'name': 'lenet5'},\n",
       "  'name': 'lenet5'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_servables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-transsexual",
   "metadata": {},
   "source": [
    "如果输出的`description`字段显示这是一个`lenet5`的模型，则可以继续到下一步发送推理请求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-consensus",
   "metadata": {},
   "source": [
    "#### 6.3 发送推理请求\n",
    "\n",
    "运行`predict`函数发送推理请求，第4个参数选择`TOP1_CLASS`或者`TOP5_CLASS`输出:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "breeding-economics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=P size=300x300 at 0x7F23306F5B10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP1: 7, score: 0.99136555194854736328\n"
     ]
    }
   ],
   "source": [
    "# 设置图片路径和输出策略（可以在TOP1和TOP5中选择）\n",
    "image_path = \"/root/7.png\"\n",
    "strategy = \"TOP1_CLASS\"\n",
    "\n",
    "# predict(image_path, servable_name, dataset='mnist', strategy='TOP1_CLASS')\n",
    "# predict方法的四个参数分别是图片路径、servable名称，数据集名称（默认MNIST）和输出策略（默认输出TOP1，可以选择TOP5）\n",
    "if server_started() is True:\n",
    "    display(Image.open(image_path).resize((300, 300), Image.ANTIALIAS))\n",
    "    print(predict(image_path,'lenet5', 'mnist', strategy))\n",
    "else:\n",
    "    print(\"Server not started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-greenhouse",
   "metadata": {},
   "source": [
    "如果用户能看到类似如下输出:  \n",
    "```\n",
    "TOP1: 7, score: 0.99934917688369750977\n",
    "```  \n",
    "那么意味着已经进行了一次成功的推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-chick",
   "metadata": {},
   "source": [
    "## 关闭服务器\n",
    "\n",
    "运行以下代码关闭服务器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "touched-africa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Server shutting down...'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
